<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?>
<!-- generated by https://github.com/cabo/kramdown-rfc version 1.7.29 (Ruby 3.2.3) -->
<rfc xmlns:xi="http://www.w3.org/2001/XInclude" ipr="trust200902" docName="draft-akhavain-moussa-ai-network-01" category="info" consensus="true" submissionType="IETF" tocInclude="true" sortRefs="true" symRefs="true" version="3">
  <!-- xml2rfc v2v3 conversion 3.31.0 -->
  <front>
    <title abbrev="AI-Internet">AI Network for Training, Inference, and Agentic Interactions</title>
    <seriesInfo name="Internet-Draft" value="draft-akhavain-moussa-ai-network-01"/>
    <author fullname="Arashmid Akhavain">
      <organization>Huawei Canada</organization>
      <address>
        <email>arashmid.akhavain@huawei.com</email>
      </address>
    </author>
    <author fullname="Hesham Moussa">
      <organization>Huawei Canada</organization>
      <address>
        <email>hesham.moussa@huawei.com</email>
      </address>
    </author>
    <date year="2025" month="November" day="02"/>
    <area>Internet</area>
    <keyword>AI Network</keyword>
    <keyword>Agentic Networks</keyword>
    <keyword>AI inference</keyword>
    <keyword>AI training</keyword>
    <abstract>
      <?line 48?>

<t>Artificial Intelligence (AI) is rapidly transforming industries and everyday life, fueled by advances in model architectures, training paradigms, and data infrastructure for generation and consumption. Today, machine learning models are embedded in many of our daily activities, ranging from simple classification systems to advanced architectures such as large language models (LLMs) like ChatGPT, Claude, Grok, and DeepSeek. These models highlight the transformative potential of AI across diverse applications—from productivity tools to complex decision-making systems.</t>
      <t>However, the effectiveness and reliability of AI depend on two foundational processes: training and inference. Each process introduces unique challenges related to data management, computation, connectivity, privacy, trust, security, and governance.</t>
      <t>In this draft, we introduce the Data and Agent Aware-Inference and Training Network (DA-ITN)—a unified, intelligent, multi-plane network architecture designed to address the full spectrum of requirements needed to enable AI networks. DA-ITN provides a scalable and adaptive infrastructure that connects AI clients, data providers, model providers, agent providers,  service facilitators, and computational resources to support end-to-end training, inference, and agentic interaction lifecycle operations. The architecture features dedicated control, data, and operations &amp; management (OAM) planes to orchestrate training, inference, and agentic services while ensuring reliability, transparency, and accountability. By outlining the key requirements of such an AI ecosystem and demonstrating how DA-ITN fulfills them, this document presents an architecture for the future of AI-native networking—an "AI internet"—optimized for AI learning, efficient inference, scalable deployment, and seamless agent-to-agent collaboration.</t>
    </abstract>
  </front>
  <middle>
    <?line 58?>

<section anchor="introduction">
      <name>Introduction</name>
      <t>AI has become a major focus in recent years, with its influence rapidly expanding from everyday tasks like scheduling to complex areas such as healthcare. This growth is largely driven by advances in model architectures, training paradigms, and data infrastructure for generation and consumption. For example, large language models (LLMs) like ChatGPT, Claude, Grok, and DeepSeek, which are now widely used for tasks such as text generation, translation, reasoning, coding, and data analysis, highlight AI’s transformative power to boost productivity and simplify real-life applications. As such, it is clear that AI and machine learning are not passing trends but lasting and evolving forces that will only continue to evolve. For clarity, in this draft, the term AI refers broadly to all types of models—from simple classification systems to advanced general intelligence models.</t>
      <t>However, it is crucial to recognize that the success of AI systems relies on two fundamental pillars: training and inference. Both of these pillars have a number of factors and moving parts that need to be carefully coordinated, designed, and managed to ensure accuracy, resilience, usability, continuous evolution, trustworthiness, and reliability. Moreover, once deployed, AI systems must be continuously monitored and governed to safeguard user safety and societal well-being.</t>
      <t>As such, aspects such as data management, computational resources, connectivity, security, privacy, trust, billing, and rigorous testing are all crucial when handling AI systems. Thus, it is important to clearly understand the requirements of the AI systems from the training, inference, and agentic interaction prospectives as all of these pillars constitute an entangled framework and cannot be tackled in isolation.</t>
      <t>In this document, we present a vision of an ecosystem, especially designed to facilitate what we call the AI network. We propose a unified, intelligent network architecture called the "Data and Agent aware-Inference and Training Network" (DA-ITN). This ecosystem is envisioned as a comprehensive, multi-plane network with dedicated control, data, and operations &amp; management (OAM) planes. It is designed to interconnect all relevant stakeholders, including clients, AI service providers, data providers, and third-party facilitators. Its core objective is to provide the infrastructure and coordination necessary to support an ecosystem for enabling AI of the future at scale.</t>
      <t>To that end, we begin by outlining the specific requirements of AI from both the training and inference standpoints. We then introduce the core components of the DA-ITN and illustrate how they collectively meet these requirements. Finally, this network is positioned as an ecosystem for agent-to-agent collaborations, interactions, and communications.</t>
    </section>
    <section anchor="training-requirements">
      <name>Training Requirements</name>
      <t>AI model training is the foundational process through which an AI system learns to perform tasks by analyzing data and adjusting its internal parameters to minimize performance errors. At its core, this process involves feeding input data into a model, and applying optimization algorithms to iteratively refine the model’s performance. As such, the training process involves creating rendezvous point where data, compute, and AI models can interact.</t>
      <section anchor="centralized-versus-decentralized-training">
        <name>Centralized versus Decentralized Training</name>
        <t>It is clear from the above that no matter how advanced the model architecture may be, the success of any training process ultimately hinges on two fundamental components: the model and the data. While the model itself is often developed and hosted in a centralized location—typically within the secure infrastructure of the model owner or designer—data is inherently distributed. The training data might originate from sensors, devices, logs, events, documents, and other diverse sources spread across different geographies and domains. To be exact, whether due to geographic dispersion, organizational silos, privacy constraints, or edge-device generation, data rarely exists in a single, clean repository.</t>
        <t>Today, model training can happen in one of two ways or a combination thereof: centralized or decentralized. In centralized training, thanks to the development of robust data collection techniques and high-throughput connectivity networks, it is now feasible to collect data and bring it to where the model training would occur. On the other hand, a more recent paradigm known as model-follow-data has emerged, advocating for the reverse: rather than transporting large volumes of potentially sensitive data to a central location, the model is dispatched to where the data resides—enabling distributed or federated training.</t>
        <t>Accordingly, to facilitate the training process, rendezvous points scheduling, whether centralized (data is collected and shipped to where the model is) or decentralize (model is shipped to where the data is),  between distributed data, compute and storage resources, and AI models awaiting training needs to be arranged and managed, which is fundamental for successful model training. However, this scheduling process introduces a number of challenges spanning privacy, trust, utility, and computational and connectivity resources management. Moreover, as AI adoption accelerates, both centralized and decentralized approaches will drive increasing pressure on underlying connectivity infrastructure. Therefore, to ensure scalable, efficient, and cost-effective AI training, it is vital to implement intelligent mechanisms for managing data and model movement, selecting relevant subsets for training, and minimizing unnecessary transfers.</t>
        <t>In the sections that follow, we explore the architectural and operational requirements needed to support this vision and lay the foundation for a high-performance, AI-native training ecosystem.</t>
      </section>
      <section anchor="requirements-breakdown">
        <name>Requirements Breakdown</name>
        <t>Consider a number of AI model training clients awaiting training service. An AI model training client is a user with a new or a pre-trained model who wishes to train or continue training their AI model using data that can be found in the data corpus. The data corpus (the global dataset), as has been previously established, consists of a group of datasets that are distributed across various geographical locations. The AI model training client requires access to this data either in a centralized or distributed manner.</t>
        <section anchor="data-collectionmodel-dispatching">
          <name>Data Collection/Model Dispatching</name>
          <t>As previously discussed, data is inherently distributed. In centralized training paradigms, this data must be transferred from its sources to centralized locations where model training occurs. Consider a scenario involving multiple AI model training clients, each awaiting centralized training of their AI model. Each client is interested in a particular data set that is sufficient for the intended training objective. Aggregating large volumes of data from geographically dispersed sources to the  centralized server of each client introduces several significant challenges:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: The sheer volume of data to be transmitted can place substantial strain on the underlying transport networks, resulting in increased latency and bandwidth consumption.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Despite originating from different sources, data sets may carry overlapping or identical knowledge content. Transmitting such redundant content leads to unnecessary duplication, wasting resources without providing additional training value.</t>
            </li>
            <li>
              <t>Timely Delivery: In certain applications, the freshness of data is critical. Delays in transmission can degrade the value of the information, as these applications are sensitive to the Age of Information (AoI)—the time elapsed since data was last updated at the destination.</t>
            </li>
            <li>
              <t>Multi-Modal Data Handling: Data often exists in various formats—such as text, images, audio, video, etc—each with distinct transmission requirements. Ensuring accurate and reliable delivery of these diverse data types necessitates differentiated Quality of Service (QoS) levels tailored to the characteristics and sensitivity of each modality.</t>
            </li>
            <li>
              <t>Heterogeneous Access Media: Data may reside across diverse communication infrastructures—for example, some data may be accessible only via 3GPP mobile networks, while other data may be confined to wireline networks. Coordinating data collection across these heterogeneous domains, while maintaining synchronization and consistency, presents a significant operational challenge.</t>
            </li>
          </ul>
          <t>Importantly, many of these challenges are alleviated in decentralized training frameworks, where data remains local to its source and is not transferred over the network. Instead, the model itself is distributed to the various data locations. However, this alternate paradigm introduces its own set of unique challenges.</t>
          <t>As previously noted, modern AI models are growing increasingly large in size. In decentralized training, it is often necessary to replicate the AI model that require training and transmit the copies to multiple geographically dispersed data sites. This results in a different but equally significant set of logistical and technical hurdles:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: While data transfer is avoided, dispatching large model files across the network to multiple destinations can still impose substantial load on communication infrastructure, particularly in bandwidth-constrained environments.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Data residing at different locations may share overlapping knowledge content. Sending models to multiple sites with redundant knowledge content leads to inefficient use of network resources. In some cases, even when knowledge content is only partially redundant, it may be more efficient—considering communication cost—to forego marginal training benefits in favor of reduced overhead.</t>
            </li>
            <li>
              <t>Timeliness and Data Freshness: In certain applications, the Age of Information (AoI) remains critical. Prioritizing model dispatch to data sources with soon-to-expire or time-sensitive information is essential to maximize the utility of training and to maintain up-to-date model performance.</t>
            </li>
          </ul>
        </section>
        <section anchor="dataset-advertisement-and-discovery">
          <name>Dataset Advertisement and Discovery</name>
          <t>Given the distributed nature of data, there must be a mechanism through which data owners can advertise information about their datasets to AI model training clients. This requires the ability to describe the characteristics of the data—such as its knowledge content, quality, size, and Age of Information (AoI)—in a way that allows AI clients to discover and evaluate whether the data aligns with their training objectives. Training objectives can be one or more of: target performance, convergence time, training cost, etc.</t>
          <t>Crucially, the dataset discovery process may need to operate across multiple network domains and heterogeneous communication infrastructures. For example, an AI training client operating over a wireline connection may be interested in data residing on a 3GPP mobile network. This raises an important question: How can data owners effectively advertise their datasets in a way that is discoverable across diverse domains?
To enable such cross-domain data visibility and discovery, the following key requirements shall be considered:</t>
          <ul spacing="normal">
            <li>
              <t>Dataset Descriptors: These are metadata objects used by dataset owners to reveal essential information about their datasets to AI clients. Effective dataset descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by training clients. They should allow dataset owners to selectively disclose details about their data—such as type, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information (privacy preservation). Data descriptors also need to be easily modified as dataset can be dynamic, and the change in dataset needs to be effectively reflected into the dataset descriptions. To ensure interoperability, dataset descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>Dataset Discovery Mechanisms: The dataset discovery refers to the processes by which AI training clients locate and identify datasets across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant datasets regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility. It should also be highlighted that, discovery mechanisms should be considerably up-to-date with the changes that would occur as the underlying data changes dynamically.</t>
            </li>
            <li>
              <t>Dataset Relationship Maps: Training often requires identifying groups of datasets that collectively meet specific requirements. Evaluating each dataset in isolation may be insufficient. Instead, a mechanism is needed to establish relationships among datasets, enabling AI training clients to assemble the appropriate combination of data for their tasks. These relationships can be envisioned to look like maps or topologies. This is a crucial step as, if an AI model client was not able to find the right dataset that satisfies its requirements, the client might choose not to submit the model for training at this time which may reduce resource wastage from the get-go.</t>
            </li>
            <li>
              <t>Timely reporting: Given the dynamic nature of data availability, characteristics, and accessibility, it is essential to have advertisement mechanisms that can promptly reflect any changes. Real-time or near-real-time updates ensure that the AI training process remains aligned with the most current data conditions, thereby maximizing both effectiveness and accuracy. Timely reporting helps prevent training on outdated or irrelevant data and supports optimal decision-making in model selection and training pipeline configuration.</t>
            </li>
          </ul>
          <t>Additionally, it should be highlighted that in AI training, discovering dataset alone is not enough. For instance, third-party resources like compute and storage are essential, and the providers of those resources must be able to advertise their capabilities so AI clients can locate and utilize them effectively. Just like with data, resource discovery requires descriptors, multi-domain accessibility, and timely updates to support seamless coordination between models, data, and infrastructure.
It should be highlighted that data and resource discovery is essential in both centralized and decentralized training, as both can be done on third party infrastructure.</t>
        </section>
        <section anchor="handling-mobility-and-service-continuity">
          <name>Handling Mobility and Service Continuity</name>
          <t>In some decentralized training applications, AI models are designed to traverse a predefined route, training on multiple datasets in a sequential or federated manner. This introduces the need to manage model mobility. However, the underlying data landscape is often dynamic—new data is continuously generated, existing data may be deleted, or datasets may be relocated to different nodes or domains.</t>
          <t>As a result, enabling reliable model mobility in such a fluid environment requires robust mobility management mechanisms. For instance, while a model is en-route to a specific data location for training, that dataset may be moved elsewhere. In such cases, the model must either be re-routed to the new location or redirected to an alternative dataset that satisfies similar training objectives.</t>
          <t>Additionally, since training occurs on remote compute infrastructure and can be time-intensive, unexpected resource shutdowns or failures may interrupt the process. These interruptions can lead to service discontinuity, which must be addressed through mechanisms such as checkpointing, fallback resource selection, or dynamic rerouting of model or data to maintain training progress and system reliability.</t>
          <t>Additionally, model mobility may involve training on datasets that are distributed across heterogeneous communication infrastructures. Some infrastructures, such as emerging 6G networks, offer built-in mobility support—for example, when data resides on mobile user equipment (UE), its location can be tracked using native features of the network. However, such mobility handling capabilities may not exist in other infrastructures, such as traditional wireline networks or legacy systems, making seamless model movement and data access more challenging in those environments.</t>
        </section>
        <section anchor="privacy-trust-and-data-ownership-and-utility">
          <name>Privacy, Trust, and Data Ownership and Utility</name>
          <t>Privacy and trust are mutual responsibilities between data owners and model owners and shall be protected. Granting clients access to data for training and knowledge building should be a regulated process, with mechanisms to track data ownership and future use. Initial discussions on this topic have taken place in forums such as the AI-Control Working Group.</t>
          <t>Equally important is ensuring that model owners are protected from data poisoning. They must have confidence that the datasets they use are accurately described and not misrepresented. If data owners provide false metadata—intentionally or otherwise—model owners may unknowingly train on unsuitable or harmful datasets, leading to degraded model performance. To safeguard both parties, innovative verification and enforcement mechanisms are needed. Technologies like blockchain could offer potential solutions for establishing trust and accountability, but further research and exploration are necessary to develop practical frameworks.</t>
        </section>
        <section anchor="testing-and-performance-management">
          <name>Testing and Performance Management</name>
          <t>Another critical aspect of training is testing and performance evaluation, typically carried out using a separate subset of the data known as the testing dataset. This dataset is not used to update the model’s weights but to assess its performance on unseen samples. In centralized training, this process is straightforward because all data resides in a single, accessible location, making it easy to partition the dataset into training and testing subsets. However, in distributed training environments, where data is spread across multiple locations or devices, creating a representative and unbiased testing dataset without aggregating the data centrally becomes a major challenge. Developing effective, privacy-preserving methods for testing in such settings requires innovative solutions</t>
        </section>
        <section anchor="training-service-qos-guarantee">
          <name>Training Service QoS Guarantee</name>
          <t>Beyond ensuring traditional Quality of Service (QoS) for data transmission, a new dimension of QoS must be considered—the QoS of training itself. In AI training workflows, it is crucial to guarantee that key performance indicators (KPIs) related to training, such as accuracy convergence, training time, and resource utilization, are met consistently. This raises several important questions:
* How can these training KPIs be guaranteed in dynamic or distributed environments?</t>
          <ul spacing="normal">
            <li>
              <t>What mechanisms can be used to monitor and track training performance in real time?</t>
            </li>
            <li>
              <t>Should AI training be treated like best-effort traffic, where no guarantees are made and resources are allocated as available?</t>
            </li>
            <li>
              <t>Should training tasks receive prioritized or differentiated service levels, similar to high-priority traffic in traditional networks?</t>
            </li>
          </ul>
          <t>Addressing these questions is essential to ensure predictable and reliable AI model development, especially as training workloads grow in complexity and scale. It may require introducing new QoS frameworks tailored specifically to the needs of AI training systems.</t>
        </section>
        <section anchor="charging-and-billing">
          <name>Charging and Billing</name>
          <t>The AI training process involves a diverse ecosystem of stakeholders, including data owners, model owners, and resource providers. Each of these parties plays one or more vital roles in enabling successful training workflows.</t>
          <t>For example, communication providers contribute not only by transporting dataset and models across the network but also they themselves may also serve as data providers. This is particularly evident in the emerging design of 6G networks, which integrate sensing capabilities with communication infrastructure. As a result, 6G operators are uniquely positioned to offer both connectivity and data, making them central players in the training pipeline.</t>
          <t>Despite their different roles, all parties contribute to enabling AI training as a service, a complex and resource-intensive process that is far from free. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should training services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Should there be tiered service offerings, such as gold, silver, and platinum, each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to facilitating collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="inference">
      <name>Inference</name>
      <t>Inference is critical because it represents the phase where the model begins to deliver practical value. Unlike training, which is typically, a one-time or periodic, resource-intensive process, inference often needs to operate continuously and efficiently, sometimes in real-time. Although inference is a less resource-intensive process, it has strict requirements that govern its success. While a single AI inference might be lightweight and fast, serving many users, with many inference requests, demands significant hardware resources andposes serious scalability challenges. In what follows, we explore these requirements that shall enable a successful AI inference ecosystem.</t>
      <section anchor="requirement-breakdown">
        <name>Requirement Breakdown</name>
        <t>We envision an inference ecosystem composed of a large number of pre-trained AI models (or agents) distributed across a geographical location. These models are capable of performing a wide range of tasks, such as image classification, language translation, or speech recognition. Some models may specialize in the same task but vary in performance, accuracy, latency, or resource demands. This diverse pool of models is accessed by numerous inference clients (users or applications) who submit inputs, referred to as queries, and receive task-specific outputs.</t>
        <t>These queries can vary greatly in complexity, structure, and modality, with some requiring the cooperation of multiple models to fulfill a single request. The overarching goal of the ecosystem is to efficiently match incoming queries with the most suitable models, ensuring accurate, timely, and resource-aware responses. Achieving this requires intelligent orchestration, load balancing, and potentially dynamic model selection based on factors such as performance, availability, cost, and user-specific requirements. In what follows, we discuss the various aspects of this ecosystem and discuss the different requirements needed for its success.</t>
        <section anchor="model-deployment-and-mobility">
          <name>Model Deployment and Mobility</name>
          <t>The first step toward building a successful AI inference ecosystem is the optimal deployment of trained AI models (or AI agents). In this context, optimality refers to both the physical or network location of the model and the manner in which it is deployed. AI models vary significantly in size and resource requirements—ranging from lightweight models that are only a few kilobytes to large-scale models with billions of parameters. This wide range makes deployment decisions critical to achieving both efficient performance and effective resource utilization. Also, a unique factor to AI models/agents is the fact that they are software components that are not bounded to a certain hardware. They can be deleted, copied, moved, or split across multiple compute locations. All these unique aspects provide flexibility in design if the real-time status of the underlying network dynamics and resources is made accessible. As such, the following aspects must be taken into account when handling model deployment and mobility:</t>
          <ul spacing="normal">
            <li>
              <t>Choosing the right facility to host a model: whether it's a lightweight edge device, a local server, or a high-performance cloud data center, deployment will depend on the model's size, computational requirements, and expected query volume. For example, smaller models might be best suited for deployment on edge devices closer to users, enabling low-latency responses. In contrast, larger models may require centralized or specialized infrastructure with high compute and memory capacity.</t>
            </li>
            <li>
              <t>Load balancing: Once models are deployed, inference traffic begins to flow, with users or applications sending queries to the appropriate agents. If not managed properly, this traffic can lead to congestion, creating bottlenecks that degrade inference performance through increased latency or dropped requests. To avoid such scenarios, models should be deployed strategically to distribute the load, ensuring smooth operation. Traditional load balancing techniques can be employed to redirect traffic away from overburdened nodes and towards underutilized ones. However, more sophisticated strategies may involve replicating models and placing these replicas closer to regions with high query demand, thereby minimizing latency and easing network traffic engineering challenges.</t>
            </li>
            <li>
              <t>Mobility-aware deployment: the dynamic nature of inference traffic necessitates mobility-aware deployment. For instance, consider a large data center acting as a centralized inference hub, hosting numerous models and handling a significant volume of queries. Over time, this hub may experience traffic overload. In such cases, migrating certain models to alternative locations can help alleviate pressure. However, model migration is not without its challenges—particularly if a model is actively serving queries at the time of migration. In such situations, mobility handling mechanisms must be in place to ensure seamless service continuity. These mechanisms could involve session handovers, temporary state preservation, or model version synchronization, all designed to maintain uninterrupted service during the migration process.</t>
            </li>
          </ul>
          <t>In summary, optimal model deployment requires careful consideration of model size, resource needs, query distribution, and real-time adaptability. Achieving this lays the foundation for a responsive, scalable, and resilient AI inference ecosystem.</t>
        </section>
        <section anchor="ai-model-ai-agent-discovery-and-description">
          <name>AI Model (AI Agent) Discovery and Description</name>
          <t>Just as data descriptors and discovery mechanisms are essential during the training phase, AI model inference clients also require a robust discovery mechanism during the inference stage. In an ecosystem populated by a large and diverse pool of models—each with unique capabilities and specializations—clients are presented with significant flexibility and choice in selecting the most suitable models for their queries. However, to make informed decisions, clients must have access to information that enables them to distinguish between models based on criteria such as performance, specialization, availability, and resource requirements.</t>
          <t>The AI model discovery process becomes even more complex when it needs to function across multiple network domains and heterogeneous communication infrastructures. For instance, a client connected via a wireline network might need to interact with a model deployed on a mobile 3GPP network. Such scenarios raise a critical question: How can model owners advertise their models in a way that ensures discoverability and interoperability across diverse domains?</t>
          <t>Addressing this challenge requires the development of standardized model advertisement and discovery protocols that can operate seamlessly across infrastructure boundaries. These protocols must accommodate differences in network technology, latency constraints, and security requirements while providing consistent and reliable access to model information. Ensuring cross-domain discoverability is crucial to unlocking the full potential of a globally distributed inference ecosystem.</t>
          <t>To enable such cross-domain AI model visibility and discovery, the following key requirements must be considered:</t>
          <ul spacing="normal">
            <li>
              <t>AI Model Descriptors: These are metadata objects used by model owners to reveal essential aspects about their datasets to AI inference clients. Effective data descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by inference clients. They should allow model owners to selectively disclose details about their model—such as skills, performance reviews, trust level, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information. To ensure interoperability, model descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>AI Model Discovery Mechanisms: These refer to the processes by which AI inference clients locate and identify models/agents across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant model/agents regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility.</t>
            </li>
            <li>
              <t>AI Model relationship maps: As queries may requiring the collaboration between multiple models/agents, relationships between models/agents with respect to different task might present useful tools as to help clients choose the appropriate subset of models/agents that would handle their queries.</t>
            </li>
            <li>
              <t>Timely Reporting: Similar to AI datasets, the status of an AI model can change over time—for example, due to shifts in workload or resource availability. It is important that such changes are reported promptly and accurately, allowing clients to make informed decisions based on the model’s current state. This is essential for ensuring efficient model selection and maintaining high-quality, reliable inference outcomes.</t>
            </li>
          </ul>
          <t>It is important to emphasize that AI model discovery differs fundamentally from data discovery. While data are passive objects that require external querying or manipulation, AI models are intelligent, autonomous entities capable of making decisions based on their own capabilities, status, and context. This distinction opens up new and more dynamic possibilities for how models are discovered and engaged in an inference ecosystem.</t>
          <t>In traditional data discovery, clients search for and retrieve relevant datasets based on metadata or predefined criteria. However, in the case of model discovery, the process can be much more interactive and flexible. One approach involves the client actively discovering models by querying a directory or registry using model descriptors. Based on these descriptors, the client selects one or more models to handle a specific inference task. However, given that models can reason and act independently, model discovery does not have to be limited to client-driven selection. An alternative approach is to reverse the flow of interaction. Instead of clients seeking out models, they can publish their tasks to a shared task pool, accessible to all available models. These tasks include descriptors that define the type of work to be done, expected outputs, and quality-of-service requirements. Models can then autonomously scan this pool, evaluate whether they are well-suited for specific tasks, and choose to express interest in executing them. This self-selection process allows models to play an active role in task matching, improving system scalability and efficiency.</t>
          <t>The final assignment of a task can be handled in different ways. Clients may retain full control and approve or reject interested models based on their preferences or priorities. Alternatively, the system may operate in a fully autonomous mode, where tasks are assigned automatically to the first or best-matching model, without requiring client intervention—depending on the client's chosen policy.</t>
          <t>This agent-driven paradigm reflects the shift toward more decentralized and intelligent AI ecosystems, where models are not merely passive computation endpoints but active participants in task negotiation and resource allocation. Such a system not only enhances scalability and flexibility but also allows for more efficient utilization of the available model pool, especially in heterogeneous and dynamic environments.</t>
        </section>
        <section anchor="query-and-inference-result-routing">
          <name>Query and Inference Result Routing</name>
          <t>A significant challenge in AI inference networks lies in efficiently routing client queries to the appropriate inference models and ensuring the corresponding results are reliably delivered back to the client. This becomes particularly complex in scenarios involving mobility and multi-domain environments, where both the client and the model may exist across different types of network infrastructures. The key challenges and considerations include:</t>
          <ul spacing="normal">
            <li>
              <t>Query Routing Across Heterogeneous Networks: When a client accesses the inference ecosystem through a mobile network such as 3GPP 6G, and the target model is hosted in a wireline or cloud-based infrastructure, routing the query across these distinct domains is non-trivial. Differences in network architecture, protocols, and service guarantees complicate the end-to-end flow.</t>
            </li>
            <li>
              <t>Mobility Management During Inference Execution: While mobile networks like 6G are designed to handle user mobility, inference tasks may take time to process—particularly when using large models or performing complex computations. During this time, the client may change physical location, switch devices, or even go offline. Ensuring that inference results can still reach the client under these dynamic conditions poses a significant challenge.</t>
            </li>
            <li>
              <t>Handling Client State Changes: If a client becomes idle or disconnects entirely during inference, the system must decide what to do with the completed result. Should it be queued, buffered, forwarded to another linked device, or simply discarded? A robust mechanism is needed to track client state, maintain context, and guarantee result delivery or at least graceful degradation.</t>
            </li>
            <li>
              <t>Support for Live and Streaming Inference: Some use cases, such as real-time audio transcription, involve live streaming of data from the client to the model and vice versa. These sessions require sustained, low-latency connections and are particularly sensitive to interruptions caused by mobility or handoffs between networks. Ensuring session continuity and maintaining streaming quality across network boundaries is a complex but critical aspect of real-world inference deployments.</t>
            </li>
            <li>
              <t>Cross-Domain Connectivity and Session Management: The involvement of multiple network operators and domains introduces questions around interoperability, session tracking, and handover coordination. There is a need for intelligent infrastructure capable of end-to-end session management, including maintaining metadata, context, and service quality as the session traverses’ different networks.</t>
            </li>
          </ul>
        </section>
        <section anchor="inference-chainingcollaborative-inference">
          <name>Inference Chaining/Collaborative Inference</name>
          <t>Another critical aspect of an AI inference ecosystem is the need for model collaboration to fulfill complex or multi-faceted tasks. Not all inference requests can be handled by a single model; in many cases, collaboration between multiple models is necessary. Effectively managing this task-based collaboration is essential to ensure accurate, efficient, and scalable inference services. Model collaboration can take several distinct forms:</t>
          <ul spacing="normal">
            <li>
              <t>Inference Chaining: In this model, the output of one model serves as the input to the next in a sequential pipeline. Each model performs a specific stage of the task, and the final result—produced by the last model in the chain—is returned to the client. This is common in multi-stage tasks such as image processing followed by object detection and then classification.</t>
            </li>
            <li>
              <t>Parallel Inference: Here, a complex task is decomposed into multiple subtasks, each of which is assigned to a specialized model. These models operate concurrently, and their outputs are aggregated to form a unified inference result. This approach is particularly useful when dealing with large data sets or when a task spans different domains of expertise.</t>
            </li>
            <li>
              <t>Hierarchical inference: A model is assigned as a task manager and is responsible for delegating tasks to service models</t>
            </li>
            <li>
              <t>Collaborative Inference: In this more dynamic and decentralized form, the task is assigned to a group of models that are capable of discovering one another, assessing their respective capabilities, and coordinating among themselves to devise a shared strategy for completing the task. This model requires more sophisticated communication, negotiation, and orchestration mechanisms.</t>
            </li>
          </ul>
          <t>Regardless of the collaboration format, the success of such multi-model interactions depends on the availability of a robust management infrastructure. This infrastructure must enable seamless coordination between models, even when:</t>
          <ul spacing="normal">
            <li>
              <t>The models are hosted by different providers,</t>
            </li>
            <li>
              <t>They are deployed across heterogeneous communication networks,</t>
            </li>
            <li>
              <t>They use varying protocols, or</t>
            </li>
            <li>
              <t>They have differing performance characteristics.</t>
            </li>
          </ul>
          <t>Such a management system must abstract away the underlying complexities and provide standardized interfaces, discovery mechanisms, communication protocols, and coordination frameworks that allow models to interact effectively. Without this, collaborative inference would be brittle, inefficient, or impossible to scale. In essence, the ability to orchestrate model collaboration across diverse environments is a cornerstone of a flexible, intelligent, and robust AI inference ecosystem.</t>
        </section>
        <section anchor="compute-and-resource-management">
          <name>Compute and Resource Management</name>
          <t>In many scenarios, the compute infrastructure used to host and run inference models is managed by third-party providers, not the model owners themselves. These compute providers are responsible for meeting the Quality of Service (QoS) levels agreed upon with the model owners—such as latency, uptime, throughput, and reliability.</t>
          <ul spacing="normal">
            <li>
              <t>Ensuring these service levels are consistently met raises the question of accountability. If performance degrades due to compute resource issues—such as overloaded hardware or network outages—who is responsible for the failed inference tasks?</t>
            </li>
            <li>
              <t>There must be clear, enforceable service-level agreements (SLAs) that define roles, responsibilities, and penalties for non-compliance.</t>
            </li>
            <li>
              <t>Mechanisms for performance monitoring, auditing, and dispute resolution need to be integrated into the ecosystem to make such arrangements viable and trustworthy.</t>
            </li>
          </ul>
        </section>
        <section anchor="privacy-preservation-and-security">
          <name>Privacy Preservation and Security</name>
          <t>While models are the intellectual property of their owners, they may operate on infrastructure owned by others. This raises significant concerns around privacy and intellectual property protection.</t>
          <ul spacing="normal">
            <li>
              <t>Sensitive model details such as architecture, weights, and optimization strategies must be protected from exposure or reverse engineering by untrusted compute hosts.</t>
            </li>
            <li>
              <t>Techniques such as secure computing, encrypted model execution, and remote attestation protocols may be necessary to ensure that models run securely without revealing proprietary details.</t>
            </li>
            <li>
              <t>Model owners must also be assured that inference inputs and outputs remain confidential, particularly in applications involving personal or sensitive data.</t>
            </li>
          </ul>
        </section>
        <section anchor="utility-handling-and-qos-requirements">
          <name>Utility Handling and QoS Requirements</name>
          <t>Utility handling refers to the regulation, protection, and fair governance of how models are used, accessed, and monitored throughout the ecosystem. This encompasses several critical questions:</t>
          <ul spacing="normal">
            <li>
              <t>How can we guarantee that a model deployed on remote infrastructure is not being tampered with, copied, or intentionally repurposed?</t>
            </li>
            <li>
              <t>How do we ensure that workload distribution is fair across available models, preventing monopolization by a few and giving equal visibility and opportunity to all participating models?</t>
            </li>
            <li>
              <t>What protections are in place to ensure that models are not being poisoned, exploited, or involved in illegal activities, either through malicious inputs or untrusted outputs?</t>
            </li>
            <li>
              <t>How do we ensure the integrity of inference results, so that outputs are delivered to clients without alteration, manipulation, or censorship?
Addressing these concerns may require digital rights management (DRM) for AI models, usage monitoring tools, and potentially blockchain-based logging or audit trails to ensure transparency and traceability.</t>
            </li>
          </ul>
          <t>On the other hand, the definition of Quality of Service (QoS), when it comes to inference tasks, is very broad and can take many forms. For instance, QoS could be to guarantee a certain accuracy of a response, or time of the response, or expertise level needed. We believe that the topic of QoS guarantee requires extensive studying and analysis.</t>
        </section>
        <section anchor="model-upgrade-streamlining">
          <name>Model Upgrade Streamlining</name>
          <t>AI models are not static; they undergo continuous upgrades, improvements, and fine-tuning to maintain accuracy, adapt to new data, or support evolving tasks.</t>
          <ul spacing="normal">
            <li>
              <t>The ecosystem must support seamless model versioning, including adding, removing, or modifying model agents without disrupting ongoing services.</t>
            </li>
            <li>
              <t>Updated model profiles must be instantly reflected in the discovery layer, ensuring clients always have access to the most current and accurate model descriptions.</t>
            </li>
            <li>
              <t>For large models, upgrade procedures must be efficient and bandwidth-conscious, potentially using incremental update techniques to avoid full redeployment.</t>
            </li>
            <li>
              <t>Moreover, strategies must be in place to handle hot-swapping of models, where an old model is gracefully decommissioned and replaced by a new one—without causing inference failures or data loss during the transition.</t>
            </li>
          </ul>
        </section>
        <section anchor="charging-and-billing-1">
          <name>Charging and Billing</name>
          <t>The AI inference process involves a diverse ecosystem of stakeholders, including model owners, compute providers, and communication providers. Each of these parties plays one or more vital roles in enabling successful inference workflows. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should inference services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Will there be tiered service offerings—such as gold, silver, and platinum—each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
            <li>
              <t>What about discovery framework providers? Would they be offering a free service like google search or would it be more structured?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to fostering collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="framework-for-da-itn-data-and-agent-aware-inference-and-training-network">
      <name>Framework for DA-ITN (Data and Agent Aware Inference and Training Network)</name>
      <t>The DA-ITN is envisioned as a multi-domain, multi-technology network operating at the AI layer, designed to address the various layers of complexity inherent in modern AI ecosystems. As mentioned earlier, the network aims to support a wide range of requirements, some of which are outlined above, across AI training, inference, and agent-to-agent interaction.</t>
      <t>The network consists of set of nodes and equipment connected via one or more traditional underlay networks as depicted below.</t>
      <figure anchor="fig1">
        <name>Figure 1: DA-ITN nodal view</name>
        <artwork align="center"><![CDATA[
+---------------------------------------------+
| DA-ITN nodal view                           |
|                                             |
|  +----------------+     +----------------+  |    DA-ITN node types
|  | DA-ITN Node (A)|<--->| DA-ITN Node (B)|  |      A- Data node
|  +----------------+  |  +----------------+  |      B- Compute node
|                      |                      |      C- Storage node
|                      |                      |      D- Model node
|  +----------------+  |  +----------------+  |      E- Evaluation node
|  | DA-ITN Node (E)|<--->| DA-ITN Node (G)|  |      F- Agent node
|  +----------------+  |  +----------------+  |      G- Multi-purpose node
|                      |                      |
|                      |                      |
|  +----------------+  |  +----------------+  |
|  | DA-ITN Node (F)|<--->|DA-ITN Node(C+D)|  |
|  +----------------+     +----------------+  |
|                                             |
+---------------------------------------------+
]]></artwork>
      </figure>
      <t>Nodes with DA-ITN along with its core functionality interact together to provide different training, inference, and agentic services. In this manner, DA-ITN can be divided into four interacting major building blocks as shown bellow.</t>
      <figure anchor="fig2">
        <name>Figure 2: DA-ITN high level architecture and building blocks</name>
        <artwork align="center"><![CDATA[
+--------------------+         +--------------------+ 
|   DA-ITN Service   |         |   DA-ITN Client    |
| Provider Community |         |     Community      |
+--------------------+         +--------------------+
    ↑     ↑                               ↑     ↑
    |     |                               |     |
    |     |                               |     |
    |     +-------------------------------+     |
    |                     |                     |
    |                     |                     |
    |                     ↓                     |
    |           +--------------------+          |
    |           |     DA-ITN Core    |          |
    |           |                    |          |
    |           +--------------------+          |
    |                     ↑                     |
    |                     |                     |
    |                     |                     |
    ↓                     ↓                     ↓
+---------------------------------------------------+
|                    DA-ITN Enablers                |
+---------------------------------------------------+
]]></artwork>
      </figure>
      <section anchor="da-itn-core">
        <name>DA-ITN Core</name>
        <t>This block contains DA-ITN main internal modules, functions, and services. Dedicated logical
planes in this block handle interactions between its different modules and functions. Interactions between different modules and functions in this block are not visible or accessible to entities in other blocks. DA-ITN core offers its services to external entities via clear and well defined interfaces and protocols. The following illustrates different modules and functions of DA-ITN core block.</t>
        <figure anchor="fig3">
          <name>Figure 3: DA-ITN core and its different modules and function</name>
          <artwork align="center"><![CDATA[
+-----------------------------------+
|            DA-ITN Core            |
|                                   |
|   +----------+ +--------------+   |
|   | X-RCE    | |Registration &|   |     X-RCE:  Training, model, query, etc.
|   |          | |Authentication|   |             route compute engine
|   +----------+ +--------------+   |     XOD:    Model, agent deployment  
|   +----------+ +--------------+   |             optimizer
|   | X-DO     | |Discovery &   |   |     S-FAM:  Different Service feasibility
|   |          | |Advertisement |   |             assessment module
|   +----------+ +--------------+   |     TAG:    Training algorithm generator
|   +----------+ +--------------+   |     PVM:    Performance verification
|   | S-FAM    | |Billing &     |   |             Module 
|   |          | |Accounting    |   |     DDRT:   Data dynamics and resource
|   +----------+ +--------------+   |             topology
|   +----------+ +--------------+   |
|   | TAG      | |Reputation &  |   |
|   |          | |Trust Mgmt.   |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   | PVM      | | Upgrade Mgmt.|   |
|   |          | |              |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   | Resource | |Mobility Mgmt.|   |
|   | Mgmt.    | |              |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   |   DDRT   | | Tools Mgmt.  |   |
|   |          | |     ???      |   |
|   +----------+ +--------------+   |
|            +---------+            |
|            |   OAM   |            |
|            +---------+            |
+-----------------------------------+
]]></artwork>
        </figure>
      </section>
      <section anchor="da-itn-service-provider-community">
        <name>DA-ITN Service Provider Community</name>
        <t>Providers for different services such as data, model, agent, and resource providers reside within the Service Provider Community block of the DA-ITN. Service providers join the network via a registration and authentication process offered by DA-ITN core. The service providers use DA-ITN to advertise their services, capabilities, etc. across the overall network. They can also register for notifications to get updates e.g. arrival of new models, training data, agents, etc. DA-ITN dispenses revenue to providers for the services rendered via its billing and accounting module.</t>
        <t>The following figure shows different modules of DA-ITN service provider community.</t>
        <figure anchor="fig4">
          <name>Figure 4: DA-ITN Service Provider Community</name>
          <artwork align="center"><![CDATA[
+-------------------------------+
|       DA-ITN Service          |
|     Provider Community        |
|                               |
|  +----------+ +----------+    |
|  | Data     | | Model    |    |
|  | providers| | providers|    |
|  +----------+ +----------+    |
|  +----------+ +----------+    |
|  | Agent    | | Resource |    |
|  | providers| | providers|    |
|  +----------+ +----------+    |
|  +--------------+             |
|  | Tools        |             |
|  | providers ???|             |
|  +--------------+             |
+-------------------------------+
]]></artwork>
        </figure>
        <t>The tool module within the provider block requires further investigation and analysis. Agentic protocols such as Model Context Protocol(MCP) provide access to MCP tools from the agent interaction point of view. Whether DA-ITN needs to support additional capabilities w.r.t agents or whether it needs to support distinct tools w.r.t training and inference is an open question for now. Will there be a need for unified tools' protocols that fits all utilities, or a protocol per utility?</t>
      </section>
      <section anchor="da-itn-client-community">
        <name>DA-ITN Client Community</name>
        <t>This block represents the client side of DA-ITN. The clients are network participants requiring training, inference, agent-to-agent interactions, and those who need access to resources such as storage, compute, etc. offered by resource providers in DA-ITN.</t>
        <t>DA-ITN enables clients to discover potential providers by tuning into DA-ITN discovery, and advertisement module, allowing them to select the best match based on their requirements. Alternatively, clients may delegate the matching process to DA-ITN, requesting DA-ITN to identify the most suitable provider based on their criteria. For example, a client using the model training service may opt to fully control the training process and make all decisions independently. Alternatively, the client can delegate the training responsibilities to the DA-ITN core. In the case of delegation, modules such as X-RCE, DDRT, PVM, S-FAM, and TAG can work collaboratively to train the model on the client’s behalf and deliver the finalized, trained model back to them.</t>
        <figure anchor="fig5">
          <name>Figure 5: DA-ITN Client Community</name>
          <artwork align="center"><![CDATA[
+-------------------------------+
|       DA-ITN Client           |
|         Community             |
|                               |
|  +----------+ +----------+    |
|  | Data     | | Model    |    |
|  | clients  | | clients  |    |
|  +----------+ +----------+    |
|  +----------+ +----------+    |
|  | Agent    | | Resource |    |
|  | clients  | | clients  |    |
|  +----------+ +----------+    |
|  +--------------+             |
|  | Tools        |             |
|  | Clients   ???|             |
|  +--------------+             |
+-------------------------------+
]]></artwork>
        </figure>
        <t>It must be noted that a node/entity in DA-ITN can act both as provider and/or a client. For example, a node providing data as its service, might need access to a resource provider service. Or a model provider enabling inference might employ the services of data providers for Retrieval-Augmented Generation (RAG).</t>
        <t>Similar to the provider community block in DA-ITN, the tools module withing the client community requires further study.</t>
      </section>
      <section anchor="da-itn-enablers">
        <name>DA-ITN Enablers</name>
        <t>This layer represents external and underlying services that DA-ITN itself employs to accomplish its different tasks. Various networking layers, access technologies, location, and sensing functions are examples of such services.</t>
        <figure anchor="fig6">
          <name>Figure 6: DA-ITN Enablers</name>
          <artwork align="center"><![CDATA[
+-------------------------------------------------------------------------+
|                             DA-ITN Enablers                             |
|                                                                         |
|  +---------------------------+  +-----------+  +-----------+            |
|  | Communications/Networking |  | Location  |  |  Sensing  |            |
|  |                           |  |           |  |           |            |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | | Mobile  |  | Internet | |  | | GPS   | |  | | IoT   | |            |
|  | | network |  +----------+ |  | +-------+ |  | +-------+ |            |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | | NTN     |  | WiFi     | |  | |Sensors| |  | | ISAC  | |  Others??? |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | +-----------------------+ |  | +-------+ |  | +-------+ |            |
|  | |        Others?        | |  | |Mobile | |  | |Others?| |            |
|  | +-----------------------+ |  | |network| |  | +-------+ |            |
|  |                           |  | +-------+ |  |           |            |
|  |                           |  | +-------+ |  |           |            |
|  |                           |  | |Others?| |  |           |            |
|  |                           |  | +-------+ |  |           |            |
|  +---------------------------+  +-----------+  +-----------+            |
+-------------------------------------------------------------------------+
]]></artwork>
        </figure>
      </section>
    </section>
    <section anchor="da-itn-high-level-architecture">
      <name>DA-ITN high level architecture</name>
      <t>To manage these complexities and cater for the requirements, we propose structuring the DA-ITN around four core components: a Control Plane (CP), a Data Plane (DP), an Operations and Management (OAM) Plane, and an Intelligence Layer. It is important to note that the DA-ITN is agnostic to the underlying communication infrastructure, allowing it to operate seamlessly over heterogeneous networks, whether mobile, wire-line, or satellite-based. he DA-ITN integrates with these underlying infrastructures through any available means, embedding its control and intelligence capabilities to coordinate and manage AI-specific services in a flexible and scalable manner.</t>
      <section anchor="control-plane-and-intelligence-layer">
        <name>Control plane and Intelligence Layer</name>
        <t>The Control Plane and Intelligence Layer work together to enable an efficient, reliable, and timely information collection infrastructure. They continuously gather up-to-date information on data availability, model status, agent conditions, resource utilization, and reachability across all participating entities. The collected information comes in the form of dynamic descriptors for data, models, and resources, essential components for enabling intelligent, context-aware decision-making within the AI ecosystem as has previously been highlighted. Also, with the help of data, resource, and reachability topology engine (DRRT) housed within the intelligence layer, the gathered information and descriptors can be used to construct meaningful relationships across the ecosystem. These are captured in the form of dynamic topologies or map-like structures, which help optimize decision-making processes across training, inference, and agent-to-agent collaboration tasks. This design provides a continuous awareness that is very essential for the success, reliability, accuracy, and responsiveness of the AI functionalities and services enabled by the DA-ITN within the AI ecosystem.</t>
        <t>The DA-ITN control plane also lays a foundation for an advanced discovery infrastructure where the generated descriptors can be made easily accessible to all authorized participants to facilitate their required AI service For example, AI clients subscribed to training services can access up-to-date data descriptors and resource topologies, enabling them to select appropriate datasets and compute resources that align with their performance and accuracy goals. Similarly, inference clients or agents seeking collaboration can discover models based on capabilities, or submit task descriptors that enable models to respond intelligently and autonomously.</t>
        <t>Aside from descriptor collection, topology creation, and discovery, the DA-ITN control plane also supports a secure and trusted environment where clients, data providers, model providers, and resource providers can engage in AI processes without compromising integrity or accountability. It also plays a key role in managing charging, billing, and rights enforcement, ensuring that all contributors to the AI service chain are fairly compensated and protected.</t>
        <t>It is worth noting that the DA-ITN’s Control Plane is not constrained by specific protocol stacks. Instead, it provides a flexible connectivity and coordination infrastructure upon which various AI-related protocols—such as Agent-to-Agent (A2A), Model Control Protocol (MCP), or AI Coordination Protocol (ACP)—can operate. Regardless of the protocol used, implementations must meet the core DA-ITN requirements, including timely information exchange, flexible descriptor encapsulation, support for multi-model and multi-domain environments, and robust security and privacy protections. The DA-ITN is also designed to support both centralized and decentralized modes of operation, offering high adaptability across different deployment contexts.</t>
        <t>It’s also important to clarify that the Intelligence Layer encompasses all previously mentioned DA-ITN core functions, along with any additional intelligence required to support the full range of DA-ITN services. The term “Intelligence Layer” is intentionally broad to allow flexibility in its design and contents. Nonetheless, its role is clearly defined: it serves as a functional layer that interfaces with other DA-ITN components through the control plane, data plane, and OAM plane to fulfill its responsibilities.</t>
      </section>
      <section anchor="data-plane">
        <name>Data Plane</name>
        <t>On the other hand, the Data Plane of the DA-ITN provides support for mobility management and intelligent scheduling, enabling the dynamic creation of rendezvous points where data, queries, models, agents, and compute infrastructure can be brought together with minimal latency and overhead. Thanks to its infrastructure-agnostic nature, the DA-ITN leverages existing communication networks—such as those offered by 6G or edge service providers—as tools to enable model mobility, data mobility, and agent-to-agent coordination. This capability is essential for supporting scenarios where mobility or geographical dispersion of resources would otherwise lead to performance degradation or inefficiency.</t>
        <t>The construction of the Data Plane may fall under the responsibility of the DA-ITN core or Intelligence Layer, which would orchestrate the necessary resources from the DA-ITN Enabler block to build the required structure. Alternatively, the Enabler block itself may possess sufficient intelligence to autonomously construct the Data Plane as needed.</t>
      </section>
      <section anchor="operation-and-management-plane-oam">
        <name>Operation and Management Plane (OAM)</name>
        <t>Finally, the Operations and Management (OAM) layer plays a critical role in supporting the day-to-day operational needs of the AI ecosystem. This layer is responsible for a wide range of essential functions, including monitoring, registration, configuration, fault management, and lifecycle maintenance of models, data, and services. It serves as the management backbone of the DA-ITN, ensuring transparency, accountability, and operational control throughout the system.</t>
        <t>Consider the scenario of an AI model training client deploying a model into the ecosystem for training. Through the capabilities of the OAM layer, the client can continuously monitor the training performance of their model in real time—tracking key performance indicators such as convergence speed, loss metrics, resource usage, and network traversal. The model’s location within the ecosystem can be dynamically tracked, allowing clients to know exactly where their model resides or which data centers or devices it is interacting with.</t>
        <t>Moreover, the OAM layer enables interactive control. Clients can use it to adjust training parameters on the fly, such as learning rates, data sampling strategies, or the choice of collaborative partners. They can even pause, resume, or terminate the training process at will, giving them full agency over the lifecycle of their models. This flexibility is crucial in adaptive AI systems where responsiveness and real-time decision-making are valued.</t>
        <t>In this way, the OAM layer effectively functions as the control dashboard or command-line terminal of the DA-ITN-enabled AI ecosystem. Whether through a graphical user interface (GUI), APIs, or automated orchestration scripts, the OAM provides the necessary tools for fine-grained management, status visualization, and policy enforcement.</t>
        <t>Beyond individual model control, the OAM layer also facilitates system-wide coordination and policy administration. OAM in coordination with a potential policy enforcement module man help ensuring compliance with service-level agreements (SLAs), enforcing data governance policies, and managing access rights across domains. It plays a foundational role in building trustworthy, maintainable, and operationally efficient AI services across diverse infrastructure providers and stakeholders.</t>
      </section>
      <section anchor="summary-of-the-da-itn-general-framework">
        <name>Summary of the DA-ITN General Framework</name>
        <t>Accordingly, the DA-ITN is well positioned and designed to provide a range of intelligent services that can be leveraged by both AI clients and service providers. It forms the foundation for a scalable, decentralized AI internet, driving the emergence of a vibrant and cooperative agent-based ecosystem. By enabling the formation of adaptive and intelligence-driven topologies and being agnostic to the infrastructure, the DA-ITN facilitates more effective decisions in AI training, inference, and agent-to-agent interactions—ultimately supporting a more responsive, resilient, and capable AI infrastructure that can scale with future demands.</t>
        <t>In the following sections, we provide more detailed insights into the specific DA-ITN components that support training and inference services.</t>
      </section>
    </section>
    <section anchor="da-itn-for-training">
      <name>DA-ITN for Training</name>
      <t>The training architecture of the DA-ITN consists of five layers: i) the terminal layer (DA-ITN provider and client communities); ii) the network layer (Enablers); iii) the data, resource, and reachability topology layer (DRRT); iv) the DA-ITN intelligence layer (DA-ITN core); and v) the OAM layer. The layers interact together using control and data planes (CP and DP respectively) as is discussed in the following.</t>
      <t>First, the network layer, which is at the heart of the DA-ITN training system, is responsible for providing connectivity services to the four other layers. It provides both control and data plane connectivity to enable various services. The network layer connects to the terminal and DRRT layers via CP and DP links, and connects to the intelligence layer via a CP link only. The network layer also enables the overarching OMA layer by enabling a multi-layer connectivity structure.</t>
      <t>Second, the terminal layer from the point of view of training, is the lowest layer in the architecture, and it contains the terminal components of the system. These include nodes that host the training data, facilities that provide computing resources where the model can be trained, and newly proposed components that we refer to as the model performance verification modules (MPVMs), where the model testing phase takes place. It should be noted that facilities providing computing resources come in various forms including private property such as personal devices, in a distributed form such as in the case of mobile edge computing in 6G networks, on the cloud such as on the AWS cloud, or anywhere that is accessible by both the data and the model and holds sufficient compute for training. As for the MPVU, this unit is important when conducting distributed training as it takes the role of a trusted proxy node that holds a globally constructed testing dataset - the dataset is constructed via collecting sample datasets from each participating node - and provides safe and secure access to it. Last, the terminal layer also hosts the AI training clients.</t>
      <t>The terminal layer relies on the network layer to build an overarching knowledge-sharing network. To be exact, the network layer provides three main services to the terminal layer, namely: i) moving models and data between the identified rendezvous compute points where training can happen; ii) moving the models towards the MPVU units where performance evaluation can be conducted to keep track of the training progress; and iii) enabling AI training clients to submit their models, monitor the training progress, modify training requirements, and collect the trained models. Control and data traffic exist for each one of these services. For instance, moving a model toward a compute facility requires authorization for the utility of the resources; hence, authorization control data is required to be exchanged over the Terminal-NET CP links. The service also requires the physical transmission of the model to the computing facility which is handled over the Terminal-NET DP link. Similar situations can be extrapolated for the other provided services. It is worth noting that the network layer can be built on top of any access network technology including 3GPP cellular networks, WiFi, wireline, peer-to-peer, satellites, and non-terrestrial networks (NTN), or a combination of the above. These networks can be used to build dedicated CP and DP links strictly designed to enable the DA-ITN training system and its services.</t>
      <t>Third, the DRRT layer holds all the information required to make accurate decisions and sits between the intelligence layer and the terminal layer. It consists of a DRRT-manager (DRRT-M) unit which is the brain of this layer and interfaces with the other layers over CP links. The DRRT layer provides the intelligence layer with visibility and accessibility services to specific information about the underlying terminal layer's data, resource, and reachability status. To be exact, the DRRT layer holds information regarding the type, quality, amount, age, dynamics, and any other essential information about the data available for training. It also provides reachability information of the participating nodes to avoid unnecessary communication overhead and packet droppage.  Lastly, the DRRT also contains information about computing resources and MPVUs such as resource availability, location, trustworthiness, and nature of the testing datasets hosted at the different MPVF units.</t>
      <t>The DRRT relies on the network layer to collect the necessary information to build the Global-DRRT topology (G-DRRT). The G-DRRT is a none model specific topology, it is rather a large canvas that holds the high-level view of the data, resource, and reachability information. The DRRT-M unit in the DRRT layer communicates with the network layer over CP links to manage the collection process of the required information. For instance, the DRRT-M may instruct the 3GPP component of the network layer to convey connectivity information about the data nodes, or it might instruct it to wake up an ideal data provider device. It might also instruct satellites to share GPS locations of mobile data nodes. The collected data by the network layer are then shipped toward the G-DRRT component of the DRRT layer over DP links. The G-DRRT hosts intelligence that allows it to convert the collected information into useful global topology ready to provide services to the AI training clients.</t>
      <t>Fourth, The Intelligence Layer is responsible for hosting the decision-making logic required to fulfill the specific training requirements submitted by clients. It contains several key components that collaboratively determine how, where, and whether training should proceed. Among these is the Model Training Route Compute Engine (MTRCE), which identifies suitable rendezvous points between models and data. Another critical component is the Training Feasibility Assessment Module (T-FAM), which functions as an admission controller—evaluating whether a submitted model, given its requirements and constraints, can be effectively trained within the available ecosystem.</t>
      <t>Additional intelligent modules include the Training Algorithm Generator (TAG) and the Hyperparameter Optimizer (HPO). These components are responsible for selecting the appropriate training paradigm—such as reinforcement learning (RL), federated learning (FL), or supervised learning (SL)—as well as determining other configuration details like the number of training epochs, batch size, and optimization strategy. The Intelligence Layer also interfaces with both the Network Layer and the DRRT Layer to acquire the context needed for effective decision-making. From the Network Layer, it receives control data over CP links—this includes model structure, target accuracy, convergence time, monitoring instructions, and client-specified training preferences. It also receives feedback data that allows the TAG and HPO modules to refine their recommendations dynamically.</t>
      <t>Meanwhile, the Intelligence Layer connects to the DRRT Layer via both CP and DP links to access up-to-date visibility into training data, compute resources, and node reachability. This information is essential for components like MTRCE and T-FAM to make routing and admission decisions. To further enhance decision efficiency, the Intelligence Layer may also host a DRRT-Adaptability Unit (DRRT-A). This optional module works in coordination with MTRCE, T-FAM, and the DRRT Manager (DRRT-M) to generate model-specific DRR topologies—lightweight, targeted representations carved out from the global DRR topology. These customized topologies are optimized to reduce computational overhead and accelerate decision-making for individual training requests.</t>
      <t>Last, the OAM layer, which spans all the layers, is mainly intended as a management layer to configure the training components, the connectivity of the network layer, and enable feedback functions essential for progress monitoring and model localization and tracking. It is also intended to provide feedback to the clients about their submitted models every step of the way.</t>
    </section>
    <section anchor="da-itn-for-inference">
      <name>DA-ITN for Inference</name>
      <t>The Inference architecture of the DA-ITN provides automated AI inference services using a similar structure to the training architecture with a few differences.</t>
      <t>First, unlike training, where the moving components are models and training data, and the rendezvous points are computing facilities, in inference, models/agents and queries/tasks are the moving components that require networking, and the rendezvous points are model hosting facilities.</t>
      <t>Second, in inference, the clients are both the task/query owners as well as the model/agent owners. Query owners are the inference service users who send their queries into the system and collect the resulting inference. On the other hand, model owners are divided into two types. The first type consists of model hosts - the model used for inference does not have to be owned by them, but it is hosted on their computing facilities.  The second type consists of model/agent providers - they develop models/agents and deploy them either at their own facilities or at model hosts. Model owners are represented in the terminal layer as model deployment facility providers (MDFP) which are distributed across the global network.</t>
      <t>Third,  the network layer provides the following services to the terminal layer using its control and data planes: i) model mobility from model generators to model hosts; ii) query routing towards models deployed on MDFPs; iii) model mobility from one location to the other in case of load balancing situations; iv) model mobility towards re-training and calibration facilities which may be hosted on MVPF units; v) query response and inference result routing towards the query owners or any indicated destination around the globe; and vi) feedback and monitoring information to model and query owners.</t>
      <t>Fourth, the DRRT layer is replaced by a query, resource, and reachability topology (QRRT) layer. It provides the same type of services to the other layers; however, from the point of view of queries and models. That is, it provides information about both models and queries such as i) for models: model locations, model capabilities, current loading conditions, inference speed, inference accuracy, model reachability and accessibility (i.e., reachability and accessibility of the MDFP), and ii) for query: query patterns and dynamics (could be associated with a geographical location), query types, and reachability status of query owners for response communication purposes. The information collected by the QRRT is used to make appropriate decisions about model deployment and distribution strategies, query-to-model routing decisions, and response routing decisions. The QRRT has a management function that coordinates with the Network layer to collect the required information from the terminal layer to build the Global-QRRT (G-QRRT). It also optionally communicates with the QRRT-adaptation (QRRT-A) function in the inference intelligence layer to build query- or model-specific QRRTs.</t>
      <t>Last, the inference intelligence layer hosts different intelligent decision-making components including the Query Feasibility Assessment Module (Q-FAM), the Query Inference Route Compute Engine (QIRCE), and the Model Deployment Optimizer module (MDO). Just like with the training, these components make decisions based on the QRRT. For instance, the Q-FAM hosts intelligence that acts as an admission control unit that evaluates if a submitted query could be serviced given the current network inference capabilities. The QIRCE handles query routing towards the correct models while observing loading conditions. Furthermore, the MDO module acts as an admission controller for newly submitted models where it evaluates deployment feasibility based on the submitted model's architecture, compute requirements, and storage requirements. It matches these requirements to the currently available resources indicated in the QRRT and makes an admittance decision. It also handles deployment location optimization, aiming to minimize query response time and cost for inference.</t>
    </section>
    <section anchor="da-itn-facilitation-agentic-networks">
      <name>DA-ITN-Facilitation Agentic Networks</name>
      <t>While agent-to-agent interaction is commonly associated with task-oriented collaboration—often relying on inference chaining as discussed in the inference section—we propose that this only reflects one side of the coin. We believe there is a transformative alternative: collaborative agent training, where agents not only work together to complete tasks, but also contribute to each other's learning and evolution. This paradigm marks a significant shift from traditional models and positions the DA-ITN as an ideal enabler of a truly agentic future, where intelligent agents can grow, adapt, and improve continuously through structured cooperation.</t>
      <t>It is important to distinguish clearly between collaborative training and task-based collaboration. In task-based collaboration, agents exchange data or partial inferences related to the execution of a specific, external objective—such as processing a query or generating an output. Their internal models remain unchanged; they simply contribute to a shared computational goal. In contrast, collaborative training focuses on internal evolution: the goal is not to solve an external task, but to enhance the capabilities of the participating agents themselves.</t>
      <t>In a collaborative training setup, agents may exchange model parameters, training datasets, or knowledge representations. They may engage in distributed training paradigms such as federated learning, where learning happens locally and updates are shared globally, or continual learning, where agents adapt over time based on new experiences. They may also employ knowledge distillation or transfer learning, where more advanced "teacher agents" guide "student agents" through structured training programs.
One can even envision a highly dynamic and autonomous system where agents attend “agent schools”—virtual environments where they gather to learn, be tested, and graduate. In this imagined scenario, teacher agents would be responsible for training student agents, evaluating their performance, and possibly issuing certifications or verifiable credentials that guarantee the agent’s competencies and readiness for deployment. These credentials serve trust foundations in the broader agent ecosystem, ensuring that certified agents can be reliably selected and trusted by inference clients or other agents.</t>
      <t>To support such a vision, a wide range of new functional and technical requirements must be addressed. These include secure model sharing, certification and validation infrastructure, identity management, trust negotiation, resource discovery for training, and scheduling of learning sessions. Fortunately, many of these requirements align naturally with the capabilities and components of the DA-ITN architecture—including its support for mobility, discovery, descriptor sharing, trust enforcement, dynamic rendezvous, and topology management.</t>
    </section>
    <section anchor="security-considerations">
      <name>Security Considerations</name>
      <t>Security considerations are as outlined within the document under the privacy and security requirements</t>
    </section>
    <section anchor="iana-considerations">
      <name>IANA Considerations</name>
      <t>This document has no IANA actions.</t>
    </section>
    <section anchor="conclusions">
      <name>Conclusions</name>
      <t>As AI continues to evolve and integrate into every facet of modern life, it becomes increasingly clear that the supporting infrastructure must evolve with it. The training and inference processes—central to the success of AI—are no longer simple, isolated tasks; they are complex, distributed, and require intelligent coordination across data, compute, and communication domains.</t>
      <t>The DA-ITN architecture offers a forward-looking response to this complexity by providing a cohesive, scalable, and intelligent network ecosystem. With its dedicated control, data, and operations &amp; management planes, DA-ITN not only supports the technical requirements of training and inference but also addresses critical concerns such as mobility, privacy, trust, and agent collaboration.</t>
      <t>Ultimately, DA-ITN lays the foundation for a new generation of AI-native networks—capable of enabling persistent learning, dynamic agent interaction, and decentralized intelligence at scale. As we move toward an AI-driven future, such architectures will be essential for building reliable, trustworthy, and efficient AI ecosystems.</t>
    </section>
  </middle>
  <back>
    <section anchor="contributors" numbered="false" toc="include" removeInRFC="false">
      <name>Contributors</name>
      <contact fullname="Tong Wen">
        <organization>Huawei</organization>
        <address>
          <email>tongwen@huawei.com</email>
        </address>
      </contact>
      <contact fullname="Reza Rokui">
        <organization>Ciena</organization>
        <address>
          <email>rrokui@ciena.com</email>
        </address>
      </contact>
    </section>
  </back>
  <!-- ##markdown-source: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-->

</rfc>
