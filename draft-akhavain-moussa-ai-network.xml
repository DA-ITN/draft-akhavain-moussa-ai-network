<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?>
<!-- generated by https://github.com/cabo/kramdown-rfc version 1.7.29 (Ruby 3.2.3) -->
<rfc xmlns:xi="http://www.w3.org/2001/XInclude" ipr="trust200902" docName="draft-akhavain-moussa-ai-network-00" category="info" consensus="true" submissionType="IETF" tocInclude="true" sortRefs="true" symRefs="true" version="3">
  <!-- xml2rfc v2v3 conversion 3.30.0 -->
  <front>
    <title abbrev="AI-Internet">AI Network for Training, Inference, and Agentic Interactions</title>
    <seriesInfo name="Internet-Draft" value="draft-akhavain-moussa-ai-network-00"/>
    <author fullname="Arashmid Akhavain">
      <organization>Huawei Canada</organization>
      <address>
        <email>arashmid.akhavain@huawei.com</email>
      </address>
    </author>
    <author fullname="Hesham Moussa">
      <organization>Huawei Canada</organization>
      <address>
        <email>hesham.moussa@huawei.com</email>
      </address>
    </author>
    <date year="2025" month="July" day="22"/>
    <area>AREA</area>
    <workgroup>Network Working Group</workgroup>
    <keyword>AI Network</keyword>
    <keyword>Agentic Networks</keyword>
    <keyword>AI inference</keyword>
    <keyword>AI training</keyword>
    <abstract>
      <?line 50?>

<t>Artificial Intelligence (AI) is rapidly reshaping industries and daily life, driven by advances in large language models (LLMs) such as ChatGPT, Claude, Grok, and DeepSeek. These models have demonstrated the transformative potential of AI across diverse applications, from productivity tools to complex decision-making systems. However, the effectiveness and reliability of AI hinge on two foundational processes: training and inference. Each presents unique challenges related to data management, computation, connectivity, privacy, trust, security, and governance.
This document introduces the Data Aware-Inference and Training Network (DA-ITN)—a unified, intelligent, multi-plane network architecture designed to address the full spectrum of AI system requirements. DA-ITN provides a scalable and adaptive infrastructure that connects AI clients, data providers, service facilitators, and computational resources to support end-to-end AI lifecycle operations. The architecture features dedicated control, data, and operations &amp; management (OAM) planes to orchestrate training and inference while ensuring reliability, transparency, and accountability.
By outlining the key requirements of AI systems and demonstrating how DA-ITN fulfills them, this document presents a vision for the future of AI-native networking—an "AI internet"—optimized for continuous learning, scalable deployment, and seamless agent-to-agent collaboration.</t>
    </abstract>
  </front>
  <middle>
    <?line 58?>

<section anchor="introduction">
      <name>Introduction</name>
      <t>AI has become a major focus in recent years, with its influence rapidly expanding from everyday tasks like scheduling to complex areas such as healthcare. This growth is largely driven by advances in large language models (LLMs) like ChatGPT, Claude, Grok, and DeepSeek, which are now widely used for tasks such as brainstorming, editing, coding, and data analysis. These real-world applications highlight AI’s transformative power to boost productivity and simplify life. It’s clear that AI is not a passing trend but a lasting and evolving force.</t>
      <t>However, it is crucial to recognize that the success of AI systems relies on two fundamental pillars: training and inference. Both of these pillars have a number of factors and moving parts that need to be carefully coordinated, designed, and managed to ensure accuracy, resilience, usability, continuous evolution, trustworthiness, and reliability. Moreover, once deployed, AI systems must be continuously monitored and governed to safeguard user safety and societal well-being.</t>
      <t>As such, aspects such as data management, computational resources, connectivity, security, privacy, trust, billing, and rigorous testing are all crucial when handling AI systems. Thus, it is important to clearly understand the requirements of the AI systems from both the training and inference prospective as both of these pillars constitute an entangled framework and cannot be tackled in isolation.</t>
      <t>In this document, we present a vision of an ecosystem, especially designed to satisfy the requirements of AI from training and inference points of view. We propose a unified, intelligent network architecture—the Data Aware-Inference and Training Network (DA-ITN). This ecosystem is envisioned as a comprehensive, multi-plane network with dedicated control, data, and operations &amp; management (OAM) planes. It is designed to interconnect all relevant stakeholders, including clients, AI service providers, data providers, and third-party facilitators. Its core objective is to provide the infrastructure and coordination necessary to support an ecosystem for enabling AI of the future at scale.</t>
      <t>This document aims to introduce the DA-ITN vision and establish a compelling case for its central role in enabling a new generation of AI-native networks, i.e., AI internet. These networks will be optimized not only for learning and inference but also for seamless collaboration, interaction, and communication among AI agents.
To that end, we begin by outlining the specific requirements of AI from both the training and inference standpoints. We then introduce the core components of the DA-ITN and illustrate how they collectively meet these requirements. Finally, this network is positioned as an ecosystem for agent-to-agent collaborations, interactions, and communications.</t>
    </section>
    <section anchor="training-requirements">
      <name>Training Requirements</name>
      <t>AI model training is the foundational process through which an artificial intelligence system learns to perform tasks by analyzing data and adjusting its internal parameters—typically the weights in neural networks—to minimize prediction errors. At its core, this process involves feeding input data into a model, and applying optimization algorithms to iteratively refine the model’s performance. Among the most influential outcomes of this process are foundation models, such as ChatGPT and its peers, which are capable of performing a wide range of tasks across domains. Training these models now occurs at an unprecedented scale, requiring massive compute infrastructure, enormous amounts of training data, high-speed interconnects, and parallelized training frameworks (e.g., data, model, and pipeline parallelism).</t>
      <section anchor="centralized-versus-decentralized-training">
        <name>Centralized versus Decentralized Training</name>
        <t>It is clear from the above that no matter how advanced the model architecture may be, the success of any training process ultimately hinges on two fundamental components: the model and the data. While the model itself is often developed and hosted in a centralized location—typically within the secure infrastructure of the model owner or designer—data is inherently distributed. It originates from sensors, devices, logs, events, documents, and other diverse sources spread across different geographies and domains. To be exact, whether due to geographic dispersion, organizational silos, privacy constraints, or edge-device generation, data rarely exists in a single, clean repository.</t>
        <t>Today, model training can happen in one of two ways or a combination thereof: centralized or decentralized. In centralized training, thanks to the development of robust data collection techniques and high-throughput connectivity networks, it is now feasible to collect data and bring it to where the model training would occur. This traditional approach is often referred to as model-centralized training. On the other hand, a more recent paradigm known as model-follow-data has emerged, advocating for the reverse: rather than transporting large volumes of potentially sensitive data to a central location, the model is dispatched to where the data resides—enabling distributed or federated training.</t>
        <t>Accordingly, to facilitate the training process, rendezvous points scheduling between distributed data, compute and storage resources, and an AI model awaiting training needs to be arranged and managed, which is fundamental for successful model training. However, this scheduling process introduces a number of challenges spanning privacy, trust, utility, and computational and connectivity resources management. Moreover, as AI adoption accelerates, both centralized and decentralized approaches will drive increasing pressure on underlying connectivity infrastructure. Therefore, to ensure scalable, efficient, and cost-effective AI training, it is vital to implement intelligent mechanisms for managing data and model movement, selecting relevant subsets for training, and minimizing unnecessary transfers.</t>
        <t>In the sections that follow, we explore the architectural and operational requirements needed to support this vision and lay the foundation for a high-performance, AI-native training ecosystem.</t>
      </section>
      <section anchor="requirements-breakdown">
        <name>Requirements Breakdown</name>
        <t>Consider a number of AI model training clients awaiting training service. An AI model training client is a user with a raw or a pre-trained model who wishes to train or continue training their AI model using data that can be found in the data corpus. The data corpus (the global dataset), as has been previously established, consists of a group of datasets that are distributed across various geographical locations. AI clients require access to this data either in a centralized or distributed manner.</t>
        <section anchor="data-collectionmodel-dispatching">
          <name>Data Collection/Model Dispatching</name>
          <t>As previously discussed, data is inherently distributed. In centralized training paradigms, this data must be transferred from its sources to centralized locations where model training occurs. Consider a scenario involving multiple clients, each awaiting centralized training of AI models using distinct data sets of interest. Aggregating large volumes of data from geographically dispersed sources to centralized servers introduces several significant challenges:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: The sheer volume of data to be transmitted can place substantial strain on the underlying transport networks, resulting in increased latency and bandwidth consumption.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Despite originating from different sources, data sets may carry overlapping or identical knowledge content. Transmitting such redundant content leads to unnecessary duplication, wasting resources without providing additional training value.</t>
            </li>
            <li>
              <t>Timely Delivery: In certain applications, the freshness of data is critical. Delays in transmission can degrade the value of the information, as these applications are sensitive to the Age of Information (AoI)—the time elapsed since data was last updated at the destination.</t>
            </li>
            <li>
              <t>Multi-Modal Data Handling: Data often exists in various formats—such as text, images, audio, video, etc—each with distinct transmission requirements. Ensuring accurate and reliable delivery of these diverse data types necessitates differentiated Quality of Service (QoS) levels tailored to the characteristics and sensitivity of each modality.</t>
            </li>
            <li>
              <t>Heterogeneous Access Media: Data may reside across diverse communication infrastructures—for example, some data may be accessible only via 3GPP mobile networks, while other data may be confined to wireline networks. Coordinating data collection across these heterogeneous domains, while maintaining synchronization and consistency, presents a significant operational challenge.</t>
            </li>
          </ul>
          <t>Importantly, many of these challenges are alleviated in decentralized training frameworks, where data remains local to its source and is not transferred over the network. Instead, the model itself is distributed to the various data locations. However, this alternate paradigm introduces its own set of unique challenges.</t>
          <t>As previously noted, modern AI models are growing increasingly large in size. In decentralized training, it is often necessary to replicate the model and transmit it to multiple, geographically dispersed data sites. This results in a different but equally significant set of logistical and technical hurdles:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: While data transfer is avoided, dispatching large model files across the network to multiple destinations can still impose substantial load on communication infrastructure, particularly in bandwidth-constrained environments.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Data residing at different locations may share overlapping knowledge content. Sending models to multiple sites with redundant knowledge content leads to inefficient use of network resources. In some cases, even when knowledge content is only partially redundant, it may be more efficient—considering communication cost—to forego marginal training benefits in favor of reduced overhead.</t>
            </li>
            <li>
              <t>Timeliness and Data Freshness: In certain applications, the Age of Information (AoI) remains critical. Prioritizing model dispatch to data sources with soon-to-expire or time-sensitive information is essential to maximize the utility of training and to maintain up-to-date model performance.</t>
            </li>
          </ul>
        </section>
        <section anchor="data-and-resource-discovery">
          <name>Data and Resource Discovery</name>
          <t>Given the distributed nature of data, there must be a mechanism through which data owners can advertise information about their datasets to AI model training clients. This requires the ability to describe the characteristics of the data—such as its knowledge content, quality, size, and Age of Information (AoI)—in a way that allows AI clients to discover and evaluate whether the data aligns with their training objectives. Training objectives can be one or more of: target performance, convergence time, training cost, etc.</t>
          <t>Crucially, this discovery process may need to operate across multiple network domains and heterogeneous communication infrastructures. For example, an AI training client operating over a wireline connection may be interested in data residing on a 3GPP mobile network. This raises an important question: How can data owners effectively advertise their datasets in a way that is discoverable across diverse domains?
To enable such cross-domain data visibility and discovery, the following key requirements must be considered:</t>
          <ul spacing="normal">
            <li>
              <t>Data Descriptors: These are metadata objects used by data owners to reveal essential information about their datasets to AI clients. Effective data descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by training clients. They should allow data owners to selectively disclose details about their data—such as type, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information (privacy preservation). Data descriptors also need to be easily modified as data can be dynamic, and the change in data needs to be effectively reflected into the data descriptions. To ensure interoperability, data descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>Data Discovery Mechanisms: These refer to the processes by which AI training clients locate and identify datasets across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant datasets regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility. It should also be highlighted that, discovery mechanisms should be considerably up-to-date with the changes that would occur as the underlying data changes dynamically.</t>
            </li>
            <li>
              <t>Data Relationship Maps: Training often requires identifying groups of datasets that collectively meet specific requirements. Evaluating each dataset in isolation may be insufficient. Instead, a mechanism is needed to establish relationships among datasets, enabling AI training clients to assemble the appropriate combination of data for their tasks. These relationships can be envisioned to look like maps or topologies. This is a crucial step as if an AI model client was not able to find the right dataset that satisfies its requirements, the client might choose not to submit the model for training at this time which may reduce resource wastage from the get go.</t>
            </li>
            <li>
              <t>Timely reporting: Given the dynamic nature of data availability, characteristics, and accessibility, it is essential to have advertisement mechanisms that can promptly reflect any changes. Real-time or near-real-time updates ensure that the AI training process remains aligned with the most current data conditions, thereby maximizing both effectiveness and accuracy. Timely reporting helps prevent training on outdated or irrelevant data and supports optimal decision-making in model selection and training pipeline configuration.</t>
            </li>
          </ul>
          <t>Additionally, it should be highlighted that in AI training, discovering data alone is not enough. For instance, third-party resources like compute and storage are essential, and the providers of those resources must be able to advertise their capabilities so AI clients can locate and utilize them effectively. Just like with data, resource discovery requires descriptors, multi-domain accessibility, and timely updates to support seamless coordination between models, data, and infrastructure.
It should be highlighted that data and resource discovery is essential in both centralized and decentralized training, as both can be done on third party infrastructure.</t>
        </section>
        <section anchor="mobility-and-service-continuity-handling">
          <name>Mobility and Service Continuity Handling</name>
          <t>In some decentralized training applications, AI models are designed to traverse a predefined route, training on multiple datasets in a sequential or federated manner. This introduces the need to manage model mobility. However, the underlying data landscape is often dynamic—new data is continuously generated, existing data may be deleted, or datasets may be relocated to different nodes or domains.</t>
          <t>As a result, enabling reliable model mobility in such a fluid environment requires robust mobility management mechanisms. For instance, while a model is en-route to a specific data location for training, that dataset may be moved elsewhere. In such cases, the model must either be re-routed to the new location or redirected to an alternative dataset that satisfies similar training objectives.</t>
          <t>Additionally, since training occurs on remote compute infrastructure and can be time-intensive, unexpected resource shutdowns or failures may interrupt the process. These interruptions can lead to service discontinuity, which must be addressed through mechanisms such as checkpointing, fallback resource selection, or dynamic rerouting of model or data to maintain training progress and system reliability.</t>
          <t>Additionally, model mobility may involve training on datasets that are distributed across heterogeneous communication infrastructures. Some infrastructures, such as emerging 6G networks, offer built-in mobility support—for example, when data resides on mobile user equipment (UE), its location can be tracked using native features of the network. However, such mobility handling capabilities may not exist in other infrastructures, such as traditional wireline networks or legacy systems, making seamless model movement and data access more challenging in those environments.</t>
        </section>
        <section anchor="privacy-trust-and-data-ownership-and-utility">
          <name>Privacy, Trust, and Data Ownership and Utility</name>
          <t>Privacy and trust are mutual responsibilities—both data owners and model owners must be protected. Granting clients access to data for training and knowledge building should be a regulated process, with mechanisms to track data ownership and future use. Initial discussions on this topic have taken place in forums such as the AI-Control Working Group.</t>
          <t>Equally important is ensuring that model owners are protected from data poisoning. They must have confidence that the datasets they use are accurately described and not misrepresented. If data owners provide false metadata—intentionally or otherwise—model owners may unknowingly train on unsuitable or harmful datasets, leading to degraded model performance. To safeguard both parties, innovative verification and enforcement mechanisms are needed. Technologies like blockchain could offer potential solutions for establishing trust and accountability, but further research and exploration are necessary to develop practical frameworks.</t>
        </section>
        <section anchor="testing-and-performance-management">
          <name>Testing and Performance Management</name>
          <t>Another critical aspect of training is testing and performance evaluation, typically carried out using a separate subset of the data known as the testing dataset. This dataset is not used to update the model’s weights but to assess its performance on unseen samples. In centralized training, this process is straightforward because all data resides in a single, accessible location, making it easy to partition the dataset into training and testing subsets. However, in distributed training environments, where data is spread across multiple locations or devices, creating a representative and unbiased testing dataset without aggregating the data centrally becomes a major challenge. Developing effective, privacy-preserving methods for testing in such settings requires innovative solutions</t>
        </section>
        <section anchor="qos-guarantee">
          <name>QoS Guarantee</name>
          <t>Beyond ensuring traditional Quality of Service (QoS) for data transmission, a new dimension of QoS must be considered—the QoS of training itself. In AI training workflows, it is crucial to guarantee that key performance indicators (KPIs) related to training, such as accuracy convergence, training time, and resource utilization, are met consistently. This raises several important questions:
* How can these training KPIs be guaranteed in dynamic or distributed environments?</t>
          <ul spacing="normal">
            <li>
              <t>What mechanisms can be used to monitor and track training performance in real time?</t>
            </li>
            <li>
              <t>Should AI training be treated like best-effort traffic, where no guarantees are made and resources are allocated as available?</t>
            </li>
            <li>
              <t>Or should training tasks receive prioritized or differentiated service levels, similar to high-priority traffic in traditional networks?</t>
            </li>
          </ul>
          <t>Addressing these questions is essential to ensure predictable and reliable AI model development, especially as training workloads grow in complexity and scale. It may require introducing new QoS frameworks tailored specifically to the needs of AI training systems.</t>
        </section>
        <section anchor="charging-and-billing">
          <name>Charging and Billing</name>
          <t>The AI training process involves a diverse ecosystem of stakeholders, including data owners, model owners, and resource providers. Each of these parties plays one or more vital roles in enabling successful training workflows.</t>
          <t>For example, communication providers contribute not only by transporting data and models across the network but also they themselves may also serve as data providers. This is particularly evident in the emerging design of 6G networks, which integrate sensing capabilities with communication infrastructure. As a result, 6G operators are uniquely positioned to offer both connectivity and data, making them central players in the training pipeline.</t>
          <t>Despite their different roles, all parties contribute to enabling AI training as a service, a complex and resource-intensive process that is far from free. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should training services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Will there be tiered service offerings, such as gold, silver, and platinum, each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to facilitating collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="inference">
      <name>Inference</name>
      <t>Inference is critical because it represents the phase where the model begins to deliver practical value. Unlike training, which is typically a one-time or periodic, resource-intensive process, inference often needs to operate continuously and efficiently, sometimes in real-time. Although inference is a less resource-intensive process, it has strict requirements that govern its success. In what follows, we explore these requirements that shall enable a successful AI inference ecosystem.</t>
      <section anchor="requirement-breakdown">
        <name>Requirement Breakdown</name>
        <t>We envision an inference ecosystem composed of a large number of pre-trained AI models (or agents) distributed across the globe. These models are capable of performing a wide range of tasks, such as image classification, language translation, or speech recognition. Some models may specialize in the same task but vary in performance, accuracy, latency, or resource demands. This diverse pool of models is accessed by numerous inference clients (users or applications) who submit inputs, referred to as queries, and receive task-specific outputs.</t>
        <t>These queries can vary greatly in complexity, structure, and modality, with some requiring the cooperation of multiple models to fulfill a single request. The overarching goal of the ecosystem is to efficiently match incoming queries with the most suitable models, ensuring accurate, timely, and resource-aware responses. Achieving this requires intelligent orchestration, load balancing, and potentially dynamic model selection based on factors such as performance, availability, cost, and user-specific requirements. In what follows, we discuss the various aspects of this ecosystem and discuss the different requirements needed for its success.</t>
        <section anchor="model-deployment-and-mobility">
          <name>Model Deployment and Mobility</name>
          <t>The first step toward building a successful AI inference ecosystem is the optimal deployment of trained models, or agents. In this context, optimality refers to both the physical or network location of the model and the manner in which it is deployed. AI models vary significantly in size and resource requirements—ranging from lightweight models that are only a few kilobytes to large-scale models with billions of parameters. This wide range makes deployment decisions critical to achieving both efficient performance and effective resource utilization. Also, a unique factor to AI models/agents is the fact that they are software components that are not bounded to a certain hardware. They can be deleted, copied, moved, or split across multiple compute locations. All these unique aspects provide flexibility in design if the real-time status of the underlying network dynamics and resources is made accessible.</t>
          <ul spacing="normal">
            <li>
              <t>Choosing the right facility to host a model: whether it's a lightweight edge device, a local server, or a high-performance cloud data center, deployment will depend on the model's size, computational requirements, and expected query volume. For example, smaller models might be best suited for deployment on edge devices closer to users, enabling low-latency responses. In contrast, larger models may require centralized or specialized infrastructure with high compute and memory capacity.</t>
            </li>
            <li>
              <t>Load balancing: Once models are deployed, inference traffic begins to flow, with users or applications sending queries to the appropriate agents. If not managed properly, this traffic can lead to congestion, creating bottlenecks that degrade inference performance through increased latency or dropped requests. To avoid such scenarios, models should be deployed strategically to distribute the load, ensuring smooth operation. Traditional load balancing techniques can be employed to redirect traffic away from overburdened nodes and towards underutilized ones. However, more sophisticated strategies may involve replicating models and placing these replicas closer to regions with high query demand, thereby minimizing latency and easing network traffic engineering challenges.</t>
            </li>
            <li>
              <t>Mobility-aware deployment: the dynamic nature of inference traffic necessitates mobility-aware deployment. For instance, consider a large data center acting as a centralized inference hub, hosting numerous models and handling a significant volume of queries. Over time, this hub may experience traffic overload. In such cases, migrating certain models to alternative locations can help alleviate pressure. However, model migration is not without its challenges—particularly if a model is actively serving queries at the time of migration. In such situations, mobility handling mechanisms must be in place to ensure seamless service continuity. These mechanisms could involve session handovers, temporary state preservation, or model version synchronization, all designed to maintain uninterrupted service during the migration process.</t>
            </li>
          </ul>
          <t>In summary, optimal model deployment requires careful consideration of model size, resource needs, query distribution, and real-time adaptability. Achieving this lays the foundation for a responsive, scalable, and resilient AI inference ecosystem.</t>
        </section>
        <section anchor="model-discovery-and-description">
          <name>Model Discovery and Description</name>
          <t>Just as data descriptors and discovery mechanisms are essential during the training phase, AI model inference clients also require a robust discovery mechanism during the inference stage. In an ecosystem populated by a large and diverse pool of models—each with unique capabilities and specializations—clients are presented with significant flexibility and choice in selecting the most suitable models for their queries. However, to make informed decisions, clients must have access to information that enables them to distinguish between models based on criteria such as performance, specialization, availability, and resource requirements.</t>
          <t>This discovery process becomes even more complex when it needs to function across multiple network domains and heterogeneous communication infrastructures. For instance, a client connected via a wireline network might need to interact with a model deployed on a mobile 3GPP network. Such scenarios raise a critical question: How can model owners advertise their models in a way that ensures discoverability and interoperability across diverse domains?</t>
          <t>Addressing this challenge requires the development of standardized model advertisement and discovery protocols that can operate seamlessly across infrastructure boundaries. These protocols must accommodate differences in network technology, latency constraints, and security requirements while providing consistent and reliable access to model information. Ensuring cross-domain discoverability is crucial to unlocking the full potential of a globally distributed inference ecosystem.</t>
          <t>To enable such cross-domain model visibility and discovery, the following key requirements must be considered:</t>
          <ul spacing="normal">
            <li>
              <t>Model Descriptors: These are metadata objects used by model owners to reveal essential aspects about their datasets to AI inference clients. Effective data descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by inference clients. They should allow model owners to selectively disclose details about their model—such as skills, performance reviews, trust level, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information. To ensure interoperability, model descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>Model/agent Discovery Mechanisms: These refer to the processes by which AI inference clients locate and identify models/agents across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant model/agents regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility.</t>
            </li>
            <li>
              <t>Model/agent relationship maps: As queries may requiring the collaboration between multiple models/agents, relationships between models/agents with respect to different task might present useful tools as to help clients choose the appropriate subset of models/agents that would handle their queries.</t>
            </li>
            <li>
              <t>Timely Reporting: Similar to data, the status of a model can change over time—for example, due to shifts in workload or resource availability. It is important that such changes are reported promptly and accurately, allowing clients to make informed decisions based on the model’s current state. This is essential for ensuring efficient model selection and maintaining high-quality, reliable inference outcomes.</t>
            </li>
          </ul>
          <t>It is important to emphasize that model discovery differs fundamentally from data discovery. While data are passive objects that require external querying or manipulation, models are intelligent, autonomous agents capable of making decisions based on their own capabilities, status, and context. This distinction opens up new and more dynamic possibilities for how models are discovered and engaged in an inference ecosystem.</t>
          <t>In traditional data discovery, clients search for and retrieve relevant datasets based on metadata or predefined criteria. However, in the case of model discovery, the process can be much more interactive and flexible. One approach involves the client actively discovering models by querying a directory or registry using model descriptors. Based on these descriptors, the client selects one or more models to handle a specific inference task. However, given that models can reason and act independently, model discovery does not have to be limited to client-driven selection. An alternative approach is to reverse the flow of interaction. Instead of clients seeking out models, they can publish their tasks to a shared task pool, accessible to all available models. These tasks include descriptors that define the type of work to be done, expected outputs, and quality-of-service requirements. Models can then autonomously scan this pool, evaluate whether they are well-suited for specific tasks, and choose to express interest in executing them. This self-selection process allows models to play an active role in task matching, improving system scalability and efficiency.</t>
          <t>The final assignment of a task can be handled in different ways. Clients may retain full control and approve or reject interested models based on their preferences or priorities. Alternatively, the system may operate in a fully autonomous mode, where tasks are assigned automatically to the first or best-matching model, without requiring client intervention—depending on the client's chosen policy.</t>
          <t>This agent-driven paradigm reflects the shift toward more decentralized and intelligent AI ecosystems, where models are not merely passive computation endpoints but active participants in task negotiation and resource allocation. Such a system not only enhances scalability and flexibility but also allows for more efficient utilization of the available model pool, especially in heterogeneous and dynamic environments.</t>
        </section>
        <section anchor="query-and-inference-result-routing">
          <name>Query and Inference Result Routing</name>
          <t>A significant challenge in AI inference networks lies in efficiently routing client queries to the appropriate inference models and ensuring the corresponding results are reliably delivered back to the client. This becomes particularly complex in scenarios involving mobility and multi-domain environments, where both the client and the model may exist across different types of network infrastructures. The key challenges and considerations include:</t>
          <ul spacing="normal">
            <li>
              <t>Query Routing Across Heterogeneous Networks: When a client accesses the inference ecosystem through a mobile network such as 3GPP 6G, and the target model is hosted in a wireline or cloud-based infrastructure, routing the query across these distinct domains is non-trivial. Differences in network architecture, protocols, and service guarantees complicate the end-to-end flow.</t>
            </li>
            <li>
              <t>Mobility Management During Inference Execution: While mobile networks like 6G are designed to handle user mobility, inference tasks may take time to process—particularly when using large models or performing complex computations. During this time, the client may change physical location, switch devices, or even go offline. Ensuring that inference results can still reach the client under these dynamic conditions poses a significant challenge.</t>
            </li>
            <li>
              <t>Handling Client State Changes: If a client becomes idle or disconnects entirely during inference, the system must decide what to do with the completed result. Should it be queued, buffered, forwarded to another linked device, or simply discarded? A robust mechanism is needed to track client state, maintain context, and guarantee result delivery or at least graceful degradation.</t>
            </li>
            <li>
              <t>Support for Live and Streaming Inference: Some use cases, such as real-time audio transcription, involve live streaming of data from the client to the model and vice versa. These sessions require sustained, low-latency connections and are particularly sensitive to interruptions caused by mobility or handoffs between networks. Ensuring session continuity and maintaining streaming quality across network boundaries is a complex but critical aspect of real-world inference deployments.</t>
            </li>
            <li>
              <t>Cross-Domain Connectivity and Session Management: The involvement of multiple network operators and domains introduces questions around interoperability, session tracking, and handover coordination. There is a need for intelligent infrastructure capable of end-to-end session management, including maintaining metadata, context, and service quality as the session traverses’ different networks.</t>
            </li>
          </ul>
        </section>
        <section anchor="inference-chainingcollaborative-inference">
          <name>Inference Chaining/Collaborative Inference</name>
          <t>Another critical aspect of an AI inference ecosystem is the need for model collaboration to fulfill complex or multi-faceted tasks. Not all inference requests can be handled by a single model; in many cases, collaboration between multiple models is necessary. Effectively managing this task-based collaboration is essential to ensure accurate, efficient, and scalable inference services. Model collaboration can take several distinct forms:</t>
          <ul spacing="normal">
            <li>
              <t>Inference Chaining: In this model, the output of one model serves as the input to the next in a sequential pipeline. Each model performs a specific stage of the task, and the final result—produced by the last model in the chain—is returned to the client. This is common in multi-stage tasks such as image processing followed by object detection and then classification.</t>
            </li>
            <li>
              <t>Parallel Inference: Here, a complex task is decomposed into multiple subtasks, each of which is assigned to a specialized model. These models operate concurrently, and their outputs are aggregated to form a unified inference result. This approach is particularly useful when dealing with large data sets or when a task spans different domains of expertise.</t>
            </li>
            <li>
              <t>Hierarchical inference: A model is assigned as a task manager and is responsible for delegating tasks to service models</t>
            </li>
            <li>
              <t>Collaborative Inference: In this more dynamic and decentralized form, the task is assigned to a group of models that are capable of discovering one another, assessing their respective capabilities, and coordinating among themselves to devise a shared strategy for completing the task. This model requires more sophisticated communication, negotiation, and orchestration mechanisms.</t>
            </li>
          </ul>
          <t>Regardless of the collaboration format, the success of such multi-model interactions depends on the availability of a robust management infrastructure. This infrastructure must enable seamless coordination between models, even when:</t>
          <ul spacing="normal">
            <li>
              <t>The models are hosted by different providers,</t>
            </li>
            <li>
              <t>They are deployed across heterogeneous communication networks,</t>
            </li>
            <li>
              <t>They use varying protocols, or</t>
            </li>
            <li>
              <t>They have differing performance characteristics.</t>
            </li>
          </ul>
          <t>Such a management system must abstract away the underlying complexities and provide standardized interfaces, discovery mechanisms, communication protocols, and coordination frameworks that allow models to interact effectively. Without this, collaborative inference would be brittle, inefficient, or impossible to scale. In essence, the ability to orchestrate model collaboration across diverse environments is a cornerstone of a flexible, intelligent, and robust AI inference ecosystem.</t>
        </section>
        <section anchor="compute-and-resource-management">
          <name>Compute and Resource Management</name>
          <t>In many scenarios, the compute infrastructure used to host and run inference models is managed by third-party providers, not the model owners themselves. These compute providers are responsible for meeting the Quality of Service (QoS) levels agreed upon with the model owners—such as latency, uptime, throughput, and reliability.</t>
          <ul spacing="normal">
            <li>
              <t>Ensuring these service levels are consistently met raises the question of accountability. If performance degrades due to compute resource issues—such as overloaded hardware or network outages—who is responsible for the failed inference tasks?</t>
            </li>
            <li>
              <t>There must be clear, enforceable service-level agreements (SLAs) that define roles, responsibilities, and penalties for non-compliance.</t>
            </li>
            <li>
              <t>Mechanisms for performance monitoring, auditing, and dispute resolution need to be integrated into the ecosystem to make such arrangements viable and trustworthy.</t>
            </li>
          </ul>
        </section>
        <section anchor="privacy-preservation-and-security">
          <name>Privacy Preservation and Security</name>
          <t>While models are the intellectual property of their owners, they may operate on infrastructure owned by others. This raises significant concerns around privacy and intellectual property protection.</t>
          <ul spacing="normal">
            <li>
              <t>Sensitive model details such as architecture, weights, and optimization strategies must be protected from exposure or reverse engineering by untrusted compute hosts.</t>
            </li>
            <li>
              <t>Techniques such as secure computing, encrypted model execution, and remote attestation protocols may be necessary to ensure that models run securely without revealing proprietary details.</t>
            </li>
            <li>
              <t>Model owners must also be assured that inference inputs and outputs remain confidential, particularly in applications involving personal or sensitive data.</t>
            </li>
          </ul>
        </section>
        <section anchor="utility-handling-and-qos-requirements">
          <name>Utility Handling and QoS Requirements</name>
          <t>Utility handling refers to the regulation, protection, and fair governance of how models are used, accessed, and monitored throughout the ecosystem. This encompasses several critical questions:</t>
          <ul spacing="normal">
            <li>
              <t>How can we guarantee that a model deployed on remote infrastructure is not being tampered with, copied, or intentionally repurposed?</t>
            </li>
            <li>
              <t>How do we ensure that workload distribution is fair across available models, preventing monopolization by a few and giving equal visibility and opportunity to all participating models?</t>
            </li>
            <li>
              <t>What protections are in place to ensure that models are not being poisoned, exploited, or involved in illegal activities, either through malicious inputs or untrusted outputs?</t>
            </li>
            <li>
              <t>How do we ensure the integrity of inference results, so that outputs are delivered to clients without alteration, manipulation, or censorship?
Addressing these concerns may require digital rights management (DRM) for AI models, usage monitoring tools, and potentially blockchain-based logging or audit trails to ensure transparency and traceability.</t>
            </li>
          </ul>
          <t>On the other hand, the definition of Quality of Service (QoS), when it comes to inference tasks, is very broad and can take many forms. For instance, QoS could be to guarantee a certain accuracy of a response, or time of the response, or expertise level needed. We believe that the topic of QoS guarantee requires extensive studying and analysis.</t>
        </section>
        <section anchor="model-upgrade-streamlining">
          <name>Model Upgrade Streamlining</name>
          <t>AI models are not static; they undergo continuous upgrades, improvements, and fine-tuning to maintain accuracy, adapt to new data, or support evolving tasks.</t>
          <ul spacing="normal">
            <li>
              <t>The ecosystem must support seamless model versioning, including adding, removing, or modifying model agents without disrupting ongoing services.</t>
            </li>
            <li>
              <t>Updated model profiles must be instantly reflected in the discovery layer, ensuring clients always have access to the most current and accurate model descriptions.</t>
            </li>
            <li>
              <t>For large models, upgrade procedures must be efficient and bandwidth-conscious, potentially using incremental update techniques to avoid full redeployment.</t>
            </li>
            <li>
              <t>Moreover, strategies must be in place to handle hot-swapping of models, where an old model is gracefully decommissioned and replaced by a new one—without causing inference failures or data loss during the transition.</t>
            </li>
          </ul>
        </section>
        <section anchor="charging-and-billing-1">
          <name>Charging and Billing</name>
          <t>The AI inference process involves a diverse ecosystem of stakeholders, including model owners, compute providers, and communication providers. Each of these parties plays one or more vital roles in enabling successful inference workflows. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should inference services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Will there be tiered service offerings—such as gold, silver, and platinum—each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
            <li>
              <t>What about discovery framework providers? Would they be offering a free service like google search or would it be more structured?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to fostering collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="data-aware-inference-and-training-network-da-itn-general-framework">
      <name>Data Aware Inference and Training Network (DA-ITN): General Framework</name>
      <t>The DA-ITN is envisioned as a multi-domain, multi-technology network operating at the AI layer, designed to address the various layers of complexity inherent in modern AI ecosystems. It aims to support a wide range of requirements across AI training, inference, and agent-to-agent interaction, as previously outlined. To manage these complexities and cater for the requirements, we propose structuring the DA-ITN around four core components: a Control Plane (CP), a Data Plane (DP), an Operations and Management (OAM) Plane, and an Intelligence Layer. It is important to note that the DA-ITN is agnostic to the underlying communication infrastructure, allowing it to operate seamlessly over heterogeneous networks, whether mobile, wireline, or satellite-based. he DA-ITN integrates with these underlying infrastructures through any available means, embedding its control and intelligence capabilities to coordinate and manage AI-specific services in a flexible and scalable manner.</t>
      <section anchor="control-plane-and-intelligence-layer">
        <name>Control plane and Intelligence Layer</name>
        <t>The Control Plane and Intelligence Layer work together to enable an efficient, reliable, and timely information collection infrastructure. They continuously gather up-to-date information on data availability, model status, agent conditions, resource utilization, and reachability across all participating entities. The collected information comes in the form of dynamic descriptors for data, models, and resources, essential components for enabling intelligent, context-aware decision-making within the AI ecosystem as has previously been highlighted. Also, with the help of data, resource, and reachability topology engine (DRRT) housed within the intelligence layer, the gathered information and descriptors can be used to construct meaningful relationships across the ecosystem. These are captured in the form of dynamic topologies or map-like structures, which help optimize decision-making processes across training, inference, and agent-to-agent collaboration tasks. This design provides a continuous awareness that is very essential for the success, reliability, accuracy, and responsiveness of the AI functionalities and services enabled by the DA-ITN within the AI ecosystem.</t>
        <t>The DA-ITN control plane also lays a foundation for an advanced discovery infrastructure where the generated descriptors can be made easily accessible to all authorized participants to facilitate their required AI service For example, AI clients subscribed to training services can access up-to-date data descriptors and resource topologies, enabling them to select appropriate datasets and compute resources that align with their performance and accuracy goals. Similarly, inference clients or agents seeking collaboration can discover models based on capabilities, or submit task descriptors that enable models to respond intelligently and autonomously.</t>
        <t>Aside from descriptor collection, topology creation, and discovery, the DA-ITN control plane also supports a secure and trusted environment where clients, data providers, model providers, and resource providers can engage in AI processes without compromising integrity or accountability. It also plays a key role in managing charging, billing, and rights enforcement, ensuring that all contributors to the AI service chain are fairly compensated and protected.</t>
        <t>It is worth noting that the DA-ITN’s Control Plane is not constrained by specific protocol stacks. Instead, it provides a flexible connectivity and coordination infrastructure upon which various AI-related protocols—such as Agent-to-Agent (A2A), Model Control Protocol (MCP), or AI Coordination Protocol (ACP)—can operate. Regardless of the protocol used, implementations must meet the core DA-ITN requirements, including timely information exchange, flexible descriptor encapsulation, support for multi-model and multi-domain environments, and robust security and privacy protections. The DA-ITN is also designed to support both centralized and decentralized modes of operation, offering high adaptability across different deployment contexts.</t>
      </section>
      <section anchor="data-plane">
        <name>Data Plane</name>
        <t>On the other hand, the Data Plane of the DA-ITN provides support for mobility management and intelligent scheduling, enabling the dynamic creation of rendezvous points where data, queries, models, and compute infrastructure can be brought together with minimal latency and overhead. Thanks to its infrastructure-agnostic nature, the DA-ITN leverages existing communication networks—such as those offered by 6G or edge service providers—as tools to enable model mobility, data mobility, and agent-to-agent coordination.  This capability is essential for supporting scenarios where mobility or geographical dispersion of resources would otherwise lead to performance degradation or inefficiency.</t>
      </section>
      <section anchor="operation-and-management-plane-oam">
        <name>Operation and Management Plane (OAM)</name>
        <t>Finally, the Operations and Management (OAM) layer plays a critical role in supporting the day-to-day operational needs of the AI ecosystem. This layer is responsible for a wide range of essential functions, including monitoring, registration, configuration, fault management, and lifecycle maintenance of models, data, and services. It serves as the management backbone of the DA-ITN, ensuring transparency, accountability, and operational control throughout the system.</t>
        <t>Consider the scenario of an AI model training client deploying a model into the ecosystem for training. Through the capabilities of the OAM layer, the client can continuously monitor the training performance of their model in real time—tracking key performance indicators such as convergence speed, loss metrics, resource usage, and network traversal. The model’s location within the ecosystem can be dynamically tracked, allowing clients to know exactly where their model resides or which data centers or devices it is interacting with.</t>
        <t>Moreover, the OAM layer enables interactive control. Clients can use it to adjust training parameters on the fly, such as learning rates, data sampling strategies, or the choice of collaborative partners. They can even pause, resume, or terminate the training process at will, giving them full agency over the lifecycle of their models. This flexibility is crucial in adaptive AI systems where responsiveness and real-time decision-making are valued.</t>
        <t>In this way, the OAM layer effectively functions as the control dashboard or command-line terminal of the DA-ITN-enabled AI ecosystem. Whether through a graphical user interface (GUI), APIs, or automated orchestration scripts, the OAM provides the necessary tools for fine-grained management, status visualization, and policy enforcement.</t>
        <t>Beyond individual model control, the OAM layer also facilitates system-wide coordination and policy administration—ensuring compliance with service-level agreements (SLAs), enforcing data governance policies, and managing access rights across domains. It plays a foundational role in building trustworthy, maintainable, and operationally efficient AI services across diverse infrastructure providers and stakeholders.</t>
      </section>
      <section anchor="summary-of-the-da-itn-general-framework">
        <name>Summary of the DA-ITN General Framework</name>
        <t>Accordingly, the DA-ITN is well positioned and designed to provide a range of intelligent services that can be leveraged by both AI clients and service providers. It forms the foundation for a scalable, decentralized AI internet, driving the emergence of a vibrant and cooperative agent-based ecosystem. By enabling the formation of adaptive and intelligence-driven topologies and being agnostic to the infrastructure, the DA-ITN facilitates more effective decisions in AI training, inference, and agent-to-agent interactions—ultimately supporting a more responsive, resilient, and capable AI infrastructure that can scale with future demands.</t>
        <t>In the following sections, we provide more detailed insights into the specific DA-ITN components that support training and inference services.</t>
      </section>
    </section>
    <section anchor="da-itn-for-training">
      <name>DA-ITN for Training</name>
      <t>The training architecture of the DA-ITN consists of five layers: i) the terminal layer; ii) the network layer; iii) the data, resource, and reachability topology layer (DRRT); iv) the DA-ITN intelligence layer; and v) the OAM layer. The layers interact together using control and data planes (CP and DP respectively) as is discussed in the following.</t>
      <t>First, the network layer, which is at the heart of the DA-ITN training system, is responsible for providing connectivity services to the four other layers. It provides both control and data plane connectivity to enable various services. The network layer connects to the terminal and DRRT layers via CP and DP links, and connects to the intelligence layer via a CP link only. The network layer also enables the overarching OMA layer by enabling a multi-layer connectivity structure.</t>
      <t>Second, the terminal layer, the lowest layer in the architecture, contains the terminal components of the system. These include nodes that host the training data, facilities that provide computing resources where the model can be trained, and newly proposed components that we refer to as the model performance verification units (MPVUs), where the model testing phase takes place. It should be noted that facilities providing computing resources come in various forms including private property such as personal devices, in a distributed form such as in the case of mobile edge computing in 6G networks, on the cloud such as on the AWS cloud, or anywhere that is accessible by both the data and the model and holds sufficient compute for training. As for the MPVU, this unit is important when conducting distributed training as it takes the role of a trusted proxy node that holds a globally constructed testing dataset - the dataset is constructed via collecting sample datasets from each participating node - and provides safe and secure access to it. Last, the terminal layer also hosts the AI training clients.</t>
      <t>The terminal layer relies on the network layer to build an overarching knowledge-sharing network. To be exact, the network layer provides three main services to the terminal layer, namely: i) moving models and data between the identified rendezvous compute points where training can happen; ii) moving the models towards the MPVU units where performance evaluation can be conducted to keep track of the training progress; and iii) enabling AI training clients to submit their models, monitor the training progress, modify training requirements, and collect the trained models. Control and data traffic exist for each one of these services. For instance, moving a model toward a compute facility requires authorization for the utility of the resources; hence, authorization control data is required to be exchanged over the Terminal-NET CP links. The service also requires the physical transmission of the model to the computing facility which is handled over the Terminal-NET DP link. Similar situations can be extrapolated for the other provided services. It is worth noting that the network layer can be built on top of any access network technology including 3GPP cellular networks, WiFi, wireline, peer-to-peer, satellites, and non-terrestrial networks (NTN), or a combination of the above. These networks can be used to build dedicated CP and DP links strictly designed to enable the DA-ITN training system and its services.</t>
      <t>Third, the DRRT layer holds all the information required to make accurate decisions and sits between the intelligence layer and the terminal layer. It consists of a DRRT-manager (DRRT-M) unit which is the brain of this layer and interfaces with the other layers over CP links. The DRRT layer provides the intelligence layer with visibility and accessibility services to specific information about the underlying terminal layer's data, resource, and reachability status. To be exact, the DRRT layer holds information regarding the type, quality, amount, age, dynamics, and any other essential information about the data available for training. It also provides reachability information of the participating nodes to avoid unnecessary communication overhead and packet droppage.  Lastly, the DRRT also contains information about computing resources and MPVUs such as resource availability, location, trustworthiness, and nature of the testing datasets hosted at the different MPVF units.</t>
      <t>The DRRT relies on the network layer to collect the necessary information to build the Global-DRRT topology (G-DRRT). The G-DRRT is a none model specific topology, it is rather a large canvas that holds the high-level view of the data, resource, and reachability information. The DRRT-M unit in the DRRT layer communicates with the network layer over CP links to manage the collection process of the required information. For instance, the DRRT-M may instruct the 3GPP component of the network layer to convey connectivity information about the data nodes, or it might instruct it to wake up an ideal data provider device. It might also instruct satellites to share GPS locations of mobile data nodes. The collected data by the network layer are then shipped toward the G-DRRT component of the DRRT layer over DP links. The G-DRRT hosts intelligence that allows it to convert the collected information into useful global topology ready to provide services to the AI training clients.</t>
      <t>Fourth, The Intelligence Layer is responsible for hosting the decision-making logic required to fulfill the specific training requirements submitted by clients. It contains several key components that collaboratively determine how, where, and whether training should proceed. Among these is the Model Training Route Compute Engine (MTRCE), which identifies suitable rendezvous points between models and data. Another critical component is the Training Feasibility Assessment Module (T-FAM), which functions as an admission controller—evaluating whether a submitted model, given its requirements and constraints, can be effectively trained within the available ecosystem.</t>
      <t>Additional intelligent modules include the Training Algorithm Generator (TAG) and the Hyperparameter Optimizer (HPO). These components are responsible for selecting the appropriate training paradigm—such as reinforcement learning (RL), federated learning (FL), or supervised learning (SL)—as well as determining other configuration details like the number of training epochs, batch size, and optimization strategy. The Intelligence Layer also interfaces with both the Network Layer and the DRRT Layer to acquire the context needed for effective decision-making. From the Network Layer, it receives control data over CP links—this includes model structure, target accuracy, convergence time, monitoring instructions, and client-specified training preferences. It also receives feedback data that allows the TAG and HPO modules to refine their recommendations dynamically.</t>
      <t>Meanwhile, the Intelligence Layer connects to the DRRT Layer via both CP and DP links to access up-to-date visibility into training data, compute resources, and node reachability. This information is essential for components like MTRCE and T-FAM to make routing and admission decisions. To further enhance decision efficiency, the Intelligence Layer may also host a DRRT-Adaptability Unit (DRRT-A). This optional module works in coordination with MTRCE, T-FAM, and the DRRT Manager (DRRT-M) to generate model-specific DRR topologies—lightweight, targeted representations carved out from the global DRR topology. These customized topologies are optimized to reduce computational overhead and accelerate decision-making for individual training requests.</t>
      <t>Last, the OAM layer, which spans all the layers, is mainly intended as a management layer to configure the training components, the connectivity of the network layer, and enable feedback functions essential for progress monitoring and model localization and tracking. It is also intended to provide feedback to the clients about their submitted models every step of the way.</t>
    </section>
    <section anchor="da-itn-for-inference">
      <name>DA-ITN for Inference</name>
      <t>The Inference architecture of the DA-ITN provides automated AI inference services using a similar structure to the training architecture with a few differences.</t>
      <t>First, unlike training, where the moving components are models and training data, and the rendezvous points are computing facilities, in inference, models/agents and queries/tasks are the moving components that require networking, and the rendezvous points are model hosting facilities.</t>
      <t>Second, in inference, the clients are both the task/query owners as well as the model/agent owners. Query owners are the inference service users who send their queries into the system and collect the resulting inference. On the other hand, model owners are divided into two types. The first type consists of model hosts - the model used for inference does not have to be owned by them, but it is hosted on their computing facilities.  The second type consists of model providers - they develop models and deploy them either at their own facilities or at model hosts. Model owners are represented in the terminal layer as model deployment facility providers (MDFP) which are distributed across the global network.</t>
      <t>Third,  the network layer provides the following services to the terminal layer using its control and data planes: i) model mobility from model generators to model hosts; ii) query routing towards models deployed on MDFPs; iii) model mobility from one location to the other in case of load balancing situations; iv) model mobility towards re-training and calibration facilities which may be hosted on MVPF units; v) query response and inference result routing towards the query owners or any indicated destination around the globe; and vi) feedback and monitoring information to model and query owners.</t>
      <t>Fourth, the DRRT layer is replaced by a query, resource, and reachability topology (QRRT) layer. It provides the same type of services to the other layers; however, from the point of view of queries and models. That is, it provides information about both models and queries such as i) for models: model locations, model capabilities, current loading conditions, inference speed, inference accuracy, model reachability and accessibility (i.e., reachability and accessibility of the MDFP), and ii) for query: query patterns and dynamics (could be associated with a geographical location), query types, and reachability status of query owners for response communication purposes. The information collected by the QRRT is used to make appropriate decisions about model deployment and distribution strategies, query-to-model routing decisions, and response routing decisions. The QRRT has a management function that coordinates with the Network layer to collect the required information from the terminal layer to build the Global-QRRT (G-QRRT). It also optionally communicates with the QRRT-adaptation (QRRT-A) function in the inference intelligence layer to build query- or model-specific QRRTs.</t>
      <t>Last, the inference intelligence layer hosts different intelligent decision-making components including the Query Feasibility Assessment Module (Q-FAM), the Query Inference Route Compute Engine (QIRCE), and the Model Deployment Optimizer module (MDO). Just like with the training, these components make decisions based on the QRRT. For instance, the Q-FAM hosts intelligence that acts as an admission control unit that evaluates if a submitted query could be serviced given the current network inference capabilities. The QIRCE handles query routing towards the correct models while observing loading conditions. Furthermore, the MDO module acts as an admission controller for newly submitted models where it evaluates deployment feasibility based on the submitted model's architecture, compute requirements, and storage requirements. It matches these requirements to the currently available resources indicated in the QRRT and makes an admittance decision. It also handles deployment location optimization, aiming to minimize query response time and cost for inference.</t>
    </section>
    <section anchor="da-itn-facilitation-agentic-networks">
      <name>DA-ITN-Facilitation Agentic Networks</name>
      <t>While agent-to-agent interaction is commonly associated with task-oriented collaboration—often relying on inference chaining as discussed in the inference section—we propose that this only reflects one side of the coin. We believe there is a transformative alternative: collaborative agent training, where agents not only work together to complete tasks, but also contribute to each other's learning and evolution. This paradigm marks a significant shift from traditional models and positions the DA-ITN as an ideal enabler of a truly agentic future, where intelligent agents can grow, adapt, and improve continuously through structured cooperation.</t>
      <t>It is important to distinguish clearly between collaborative training and task-based collaboration. In task-based collaboration, agents exchange data or partial inferences related to the execution of a specific, external objective—such as processing a query or generating an output. Their internal models remain unchanged; they simply contribute to a shared computational goal. In contrast, collaborative training focuses on internal evolution: the goal is not to solve an external task, but to enhance the capabilities of the participating agents themselves.</t>
      <t>In a collaborative training setup, agents may exchange model parameters, training datasets, or knowledge representations. They may engage in distributed training paradigms such as federated learning, where learning happens locally and updates are shared globally, or continual learning, where agents adapt over time based on new experiences. They may also employ knowledge distillation or transfer learning, where more advanced "teacher agents" guide "student agents" through structured training programs.
One can even envision a highly dynamic and autonomous system where agents attend “agent schools”—virtual environments where they gather to learn, be tested, and graduate. In this imagined scenario, teacher agents would be responsible for training student agents, evaluating their performance, and possibly issuing certifications or verifiable credentials that guarantee the agent’s competencies and readiness for deployment. These credentials serve trust foundations in the broader agent ecosystem, ensuring that certified agents can be reliably selected and trusted by inference clients or other agents.</t>
      <t>To support such a vision, a wide range of new functional and technical requirements must be addressed. These include secure model sharing, certification and validation infrastructure, identity management, trust negotiation, resource discovery for training, and scheduling of learning sessions. Fortunately, many of these requirements align naturally with the capabilities and components of the DA-ITN architecture—including its support for mobility, discovery, descriptor sharing, trust enforcement, dynamic rendezvous, and topology management.</t>
    </section>
    <section anchor="security-considerations">
      <name>Security Considerations</name>
      <t>Security considerations are as outlined within the document under the privacy and security requirements</t>
    </section>
    <section anchor="iana-considerations">
      <name>IANA Considerations</name>
      <t>This document has no IANA actions.</t>
    </section>
    <section anchor="conclusions">
      <name>Conclusions</name>
      <t>As AI continues to evolve and integrate into every facet of modern life, it becomes increasingly clear that the supporting infrastructure must evolve with it. The training and inference processes—central to the success of AI—are no longer simple, isolated tasks; they are complex, distributed, and require intelligent coordination across data, compute, and communication domains.</t>
      <t>The DA-ITN architecture offers a forward-looking response to this complexity by providing a cohesive, scalable, and intelligent network ecosystem. With its dedicated control, data, and operations &amp; management planes, DA-ITN not only supports the technical requirements of training and inference but also addresses critical concerns such as mobility, privacy, trust, and agent collaboration.</t>
      <t>Ultimately, DA-ITN lays the foundation for a new generation of AI-native networks—capable of enabling persistent learning, dynamic agent interaction, and decentralized intelligence at scale. As we move toward an AI-driven future, such architectures will be essential for building reliable, trustworthy, and efficient AI ecosystems.</t>
    </section>
  </middle>
  <back>
    <section anchor="contributors" numbered="false" toc="include" removeInRFC="false">
      <name>Contributors</name>
      <contact fullname="Arashmid Akhavain">
        <organization>Huawei Canada</organization>
        <address>
          <email>arashmid.akhavain@huawei.com</email>
        </address>
      </contact>
      <contact fullname="Hesham Moussa">
        <organization>Huawei Canada</organization>
        <address>
          <email>hesham.moussa@huawei.com</email>
        </address>
      </contact>
      <contact fullname="Tong Wen">
        <organization>Huawei</organization>
        <address>
          <email>tongwen@huawei.com</email>
        </address>
      </contact>
    </section>
  </back>
  <!-- ##markdown-source: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-->

</rfc>
