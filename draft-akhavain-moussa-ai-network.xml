<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?>
<!-- generated by https://github.com/cabo/kramdown-rfc version 1.7.29 (Ruby 3.2.3) -->
<rfc xmlns:xi="http://www.w3.org/2001/XInclude" ipr="trust200902" docName="draft-akhavain-moussa-ai-network-01" category="info" consensus="true" submissionType="IETF" tocInclude="true" sortRefs="true" symRefs="true" version="3">
  <!-- xml2rfc v2v3 conversion 3.30.2 -->
  <front>
    <title abbrev="AI-Internet">AI Network for Training, Inference, and Agentic Interactions</title>
    <seriesInfo name="Internet-Draft" value="draft-akhavain-moussa-ai-network-01"/>
    <author fullname="Arashmid Akhavain">
      <organization>Huawei Canada</organization>
      <address>
        <email>arashmid.akhavain@huawei.com</email>
      </address>
    </author>
    <author fullname="Hesham Moussa">
      <organization>Huawei Canada</organization>
      <address>
        <email>hesham.moussa@huawei.com</email>
      </address>
    </author>
    <date year="2025" month="September" day="09"/>
    <area>Internet</area>
    <keyword>AI Network</keyword>
    <keyword>Agentic Networks</keyword>
    <keyword>AI inference</keyword>
    <keyword>AI training</keyword>
    <abstract>
      <?line 50?>

<t>Artificial Intelligence (AI) is rapidly reshaping industries and daily life, driven by advances in large language models (LLMs) such as ChatGPT, Claude, Grok, and DeepSeek. These models have demonstrated the transformative potential of AI across diverse applications, from productivity tools to complex decision-making systems. However, the effectiveness and reliability of AI hinge on two foundational processes: training and inference. Each presents unique challenges related to data management, computation, connectivity, privacy, trust, security, and governance.
In this draft, we introduce the Data and Agent Aware-Inference and Training Network (DA-ITN)—a unified, intelligent, multi-plane network architecture designed to address the full spectrum of AI system requirements. DA-ITN provides a scalable and adaptive infrastructure that connects AI clients, data providers, model providers, agent providers,  service facilitators, and computational resources to support end-to-end AI lifecycle operations. The architecture features dedicated control, data, and operations &amp; management (OAM) planes to orchestrate training, inference, and agentic services while ensuring reliability, transparency, and accountability. By outlining the key requirements of such an AI ecosystem and demonstrating how DA-ITN fulfills them, this document presents an architecture for the future of AI-native networking—an "AI internet"—optimized for AI learning, efficient inference, scalable deployment, and seamless agent-to-agent collaboration.</t>
    </abstract>
  </front>
  <middle>
    <?line 57?>

<section anchor="introduction">
      <name>Introduction</name>
      <t>AI has become a major focus in recent years, with its influence rapidly expanding from everyday tasks like scheduling to complex areas such as healthcare. This growth is largely driven by advances in large language models (LLMs) like ChatGPT, Claude, Grok, and DeepSeek, which are now widely used for tasks such as brainstorming, editing, coding, and data analysis. These real-world applications highlight AI’s transformative power to boost productivity and simplify life. It’s clear that AI is not a passing trend but a lasting and evolving force.</t>
      <t>However, it is crucial to recognize that the success of AI systems relies on two fundamental pillars: training and inference. Both of these pillars have a number of factors and moving parts that need to be carefully coordinated, designed, and managed to ensure accuracy, resilience, usability, continuous evolution, trustworthiness, and reliability. Moreover, once deployed, AI systems must be continuously monitored and governed to safeguard user safety and societal well-being.</t>
      <t>As such, aspects such as data management, computational resources, connectivity, security, privacy, trust, billing, and rigorous testing are all crucial when handling AI systems. Thus, it is important to clearly understand the requirements of the AI systems from both the training and inference prospective as both of these pillars constitute an entangled framework and cannot be tackled in isolation.</t>
      <t>In this document, we present a vision of an ecosystem, especially designed to satisfy the requirements of AI from training and inference points of view. We propose a unified, intelligent network architecture—the Data and Agent aware-Inference and Training Network (DA-ITN). This ecosystem is envisioned as a comprehensive, multi-plane network with dedicated control, data, and operations &amp; management (OAM) planes. It is designed to interconnect all relevant stakeholders, including clients, AI service providers, data providers, and third-party facilitators. Its core objective is to provide the infrastructure and coordination necessary to support an ecosystem for enabling AI of the future at scale.</t>
      <t>This document aims to introduce the DA-ITN vision and establish a compelling case for its central role in enabling a new generation of AI-native networks, i.e., AI internet. These networks will be optimized not only for learning and inference but also for seamless collaboration, interaction, and communication among AI agents.
To that end, we begin by outlining the specific requirements of AI from both the training and inference standpoints. We then introduce the core components of the DA-ITN and illustrate how they collectively meet these requirements. Finally, this network is positioned as an ecosystem for agent-to-agent collaborations, interactions, and communications.</t>
    </section>
    <section anchor="training-requirements">
      <name>Training Requirements</name>
      <t>AI model training is the foundational process through which an artificial intelligence system learns to perform tasks by analyzing data and adjusting its internal parameters—typically the weights in neural networks—to minimize prediction errors. At its core, this process involves feeding input data into a model, and applying optimization algorithms to iteratively refine the model’s performance. Among the most influential outcomes of this process are foundation models, such as ChatGPT and its peers, which are capable of performing a wide range of tasks across domains. Training these models now occurs at an unprecedented scale, requiring massive compute infrastructure, enormous amounts of training data, high-speed interconnects, and parallelized training frameworks (e.g., data, model, and pipeline parallelism).</t>
      <section anchor="centralized-versus-decentralized-training">
        <name>Centralized versus Decentralized Training</name>
        <t>It is clear from the above that no matter how advanced the model architecture may be, the success of any training process ultimately hinges on two fundamental components: the model and the data. While the model itself is often developed and hosted in a centralized location—typically within the secure infrastructure of the model owner or designer—data is inherently distributed. It originates from sensors, devices, logs, events, documents, and other diverse sources spread across different geographies and domains. To be exact, whether due to geographic dispersion, organizational silos, privacy constraints, or edge-device generation, data rarely exists in a single, clean repository.</t>
        <t>Today, model training can happen in one of two ways or a combination thereof: centralized or decentralized. In centralized training, thanks to the development of robust data collection techniques and high-throughput connectivity networks, it is now feasible to collect data and bring it to where the model training would occur. This traditional approach is often referred to as model-centralized training. On the other hand, a more recent paradigm known as model-follow-data has emerged, advocating for the reverse: rather than transporting large volumes of potentially sensitive data to a central location, the model is dispatched to where the data resides—enabling distributed or federated training.</t>
        <t>Accordingly, to facilitate the training process, rendezvous points scheduling between distributed data, compute and storage resources, and an AI model awaiting training needs to be arranged and managed, which is fundamental for successful model training. However, this scheduling process introduces a number of challenges spanning privacy, trust, utility, and computational and connectivity resources management. Moreover, as AI adoption accelerates, both centralized and decentralized approaches will drive increasing pressure on underlying connectivity infrastructure. Therefore, to ensure scalable, efficient, and cost-effective AI training, it is vital to implement intelligent mechanisms for managing data and model movement, selecting relevant subsets for training, and minimizing unnecessary transfers.</t>
        <t>In the sections that follow, we explore the architectural and operational requirements needed to support this vision and lay the foundation for a high-performance, AI-native training ecosystem.</t>
      </section>
      <section anchor="requirements-breakdown">
        <name>Requirements Breakdown</name>
        <t>Consider a number of AI model training clients awaiting training service. An AI model training client is a user with a raw or a pre-trained model who wishes to train or continue training their AI model using data that can be found in the data corpus. The data corpus (the global dataset), as has been previously established, consists of a group of datasets that are distributed across various geographical locations. AI clients require access to this data either in a centralized or distributed manner.</t>
        <section anchor="data-collectionmodel-dispatching">
          <name>Data Collection/Model Dispatching</name>
          <t>As previously discussed, data is inherently distributed. In centralized training paradigms, this data must be transferred from its sources to centralized locations where model training occurs. Consider a scenario involving multiple clients, each awaiting centralized training of AI models using distinct data sets of interest. Aggregating large volumes of data from geographically dispersed sources to centralized servers introduces several significant challenges:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: The sheer volume of data to be transmitted can place substantial strain on the underlying transport networks, resulting in increased latency and bandwidth consumption.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Despite originating from different sources, data sets may carry overlapping or identical knowledge content. Transmitting such redundant content leads to unnecessary duplication, wasting resources without providing additional training value.</t>
            </li>
            <li>
              <t>Timely Delivery: In certain applications, the freshness of data is critical. Delays in transmission can degrade the value of the information, as these applications are sensitive to the Age of Information (AoI)—the time elapsed since data was last updated at the destination.</t>
            </li>
            <li>
              <t>Multi-Modal Data Handling: Data often exists in various formats—such as text, images, audio, video, etc—each with distinct transmission requirements. Ensuring accurate and reliable delivery of these diverse data types necessitates differentiated Quality of Service (QoS) levels tailored to the characteristics and sensitivity of each modality.</t>
            </li>
            <li>
              <t>Heterogeneous Access Media: Data may reside across diverse communication infrastructures—for example, some data may be accessible only via 3GPP mobile networks, while other data may be confined to wireline networks. Coordinating data collection across these heterogeneous domains, while maintaining synchronization and consistency, presents a significant operational challenge.</t>
            </li>
          </ul>
          <t>Importantly, many of these challenges are alleviated in decentralized training frameworks, where data remains local to its source and is not transferred over the network. Instead, the model itself is distributed to the various data locations. However, this alternate paradigm introduces its own set of unique challenges.</t>
          <t>As previously noted, modern AI models are growing increasingly large in size. In decentralized training, it is often necessary to replicate the model and transmit it to multiple, geographically dispersed data sites. This results in a different but equally significant set of logistical and technical hurdles:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: While data transfer is avoided, dispatching large model files across the network to multiple destinations can still impose substantial load on communication infrastructure, particularly in bandwidth-constrained environments.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Data residing at different locations may share overlapping knowledge content. Sending models to multiple sites with redundant knowledge content leads to inefficient use of network resources. In some cases, even when knowledge content is only partially redundant, it may be more efficient—considering communication cost—to forego marginal training benefits in favor of reduced overhead.</t>
            </li>
            <li>
              <t>Timeliness and Data Freshness: In certain applications, the Age of Information (AoI) remains critical. Prioritizing model dispatch to data sources with soon-to-expire or time-sensitive information is essential to maximize the utility of training and to maintain up-to-date model performance.</t>
            </li>
          </ul>
        </section>
        <section anchor="data-and-resource-discovery">
          <name>Data and Resource Discovery</name>
          <t>Given the distributed nature of data, there must be a mechanism through which data owners can advertise information about their datasets to AI model training clients. This requires the ability to describe the characteristics of the data—such as its knowledge content, quality, size, and Age of Information (AoI)—in a way that allows AI clients to discover and evaluate whether the data aligns with their training objectives. Training objectives can be one or more of: target performance, convergence time, training cost, etc.</t>
          <t>Crucially, this discovery process may need to operate across multiple network domains and heterogeneous communication infrastructures. For example, an AI training client operating over a wireline connection may be interested in data residing on a 3GPP mobile network. This raises an important question: How can data owners effectively advertise their datasets in a way that is discoverable across diverse domains?
To enable such cross-domain data visibility and discovery, the following key requirements must be considered:</t>
          <ul spacing="normal">
            <li>
              <t>Data Descriptors: These are metadata objects used by data owners to reveal essential information about their datasets to AI clients. Effective data descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by training clients. They should allow data owners to selectively disclose details about their data—such as type, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information (privacy preservation). Data descriptors also need to be easily modified as data can be dynamic, and the change in data needs to be effectively reflected into the data descriptions. To ensure interoperability, data descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>Data Discovery Mechanisms: These refer to the processes by which AI training clients locate and identify datasets across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant datasets regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility. It should also be highlighted that, discovery mechanisms should be considerably up-to-date with the changes that would occur as the underlying data changes dynamically.</t>
            </li>
            <li>
              <t>Data Relationship Maps: Training often requires identifying groups of datasets that collectively meet specific requirements. Evaluating each dataset in isolation may be insufficient. Instead, a mechanism is needed to establish relationships among datasets, enabling AI training clients to assemble the appropriate combination of data for their tasks. These relationships can be envisioned to look like maps or topologies. This is a crucial step as if an AI model client was not able to find the right dataset that satisfies its requirements, the client might choose not to submit the model for training at this time which may reduce resource wastage from the get go.</t>
            </li>
            <li>
              <t>Timely reporting: Given the dynamic nature of data availability, characteristics, and accessibility, it is essential to have advertisement mechanisms that can promptly reflect any changes. Real-time or near-real-time updates ensure that the AI training process remains aligned with the most current data conditions, thereby maximizing both effectiveness and accuracy. Timely reporting helps prevent training on outdated or irrelevant data and supports optimal decision-making in model selection and training pipeline configuration.</t>
            </li>
          </ul>
          <t>Additionally, it should be highlighted that in AI training, discovering data alone is not enough. For instance, third-party resources like compute and storage are essential, and the providers of those resources must be able to advertise their capabilities so AI clients can locate and utilize them effectively. Just like with data, resource discovery requires descriptors, multi-domain accessibility, and timely updates to support seamless coordination between models, data, and infrastructure.
It should be highlighted that data and resource discovery is essential in both centralized and decentralized training, as both can be done on third party infrastructure.</t>
        </section>
        <section anchor="mobility-and-service-continuity-handling">
          <name>Mobility and Service Continuity Handling</name>
          <t>In some decentralized training applications, AI models are designed to traverse a predefined route, training on multiple datasets in a sequential or federated manner. This introduces the need to manage model mobility. However, the underlying data landscape is often dynamic—new data is continuously generated, existing data may be deleted, or datasets may be relocated to different nodes or domains.</t>
          <t>As a result, enabling reliable model mobility in such a fluid environment requires robust mobility management mechanisms. For instance, while a model is en-route to a specific data location for training, that dataset may be moved elsewhere. In such cases, the model must either be re-routed to the new location or redirected to an alternative dataset that satisfies similar training objectives.</t>
          <t>Additionally, since training occurs on remote compute infrastructure and can be time-intensive, unexpected resource shutdowns or failures may interrupt the process. These interruptions can lead to service discontinuity, which must be addressed through mechanisms such as checkpointing, fallback resource selection, or dynamic rerouting of model or data to maintain training progress and system reliability.</t>
          <t>Additionally, model mobility may involve training on datasets that are distributed across heterogeneous communication infrastructures. Some infrastructures, such as emerging 6G networks, offer built-in mobility support—for example, when data resides on mobile user equipment (UE), its location can be tracked using native features of the network. However, such mobility handling capabilities may not exist in other infrastructures, such as traditional wireline networks or legacy systems, making seamless model movement and data access more challenging in those environments.</t>
        </section>
        <section anchor="privacy-trust-and-data-ownership-and-utility">
          <name>Privacy, Trust, and Data Ownership and Utility</name>
          <t>Privacy and trust are mutual responsibilities—both data owners and model owners must be protected. Granting clients access to data for training and knowledge building should be a regulated process, with mechanisms to track data ownership and future use. Initial discussions on this topic have taken place in forums such as the AI-Control Working Group.</t>
          <t>Equally important is ensuring that model owners are protected from data poisoning. They must have confidence that the datasets they use are accurately described and not misrepresented. If data owners provide false metadata—intentionally or otherwise—model owners may unknowingly train on unsuitable or harmful datasets, leading to degraded model performance. To safeguard both parties, innovative verification and enforcement mechanisms are needed. Technologies like blockchain could offer potential solutions for establishing trust and accountability, but further research and exploration are necessary to develop practical frameworks.</t>
        </section>
        <section anchor="testing-and-performance-management">
          <name>Testing and Performance Management</name>
          <t>Another critical aspect of training is testing and performance evaluation, typically carried out using a separate subset of the data known as the testing dataset. This dataset is not used to update the model’s weights but to assess its performance on unseen samples. In centralized training, this process is straightforward because all data resides in a single, accessible location, making it easy to partition the dataset into training and testing subsets. However, in distributed training environments, where data is spread across multiple locations or devices, creating a representative and unbiased testing dataset without aggregating the data centrally becomes a major challenge. Developing effective, privacy-preserving methods for testing in such settings requires innovative solutions</t>
        </section>
        <section anchor="qos-guarantee">
          <name>QoS Guarantee</name>
          <t>Beyond ensuring traditional Quality of Service (QoS) for data transmission, a new dimension of QoS must be considered—the QoS of training itself. In AI training workflows, it is crucial to guarantee that key performance indicators (KPIs) related to training, such as accuracy convergence, training time, and resource utilization, are met consistently. This raises several important questions:
* How can these training KPIs be guaranteed in dynamic or distributed environments?</t>
          <ul spacing="normal">
            <li>
              <t>What mechanisms can be used to monitor and track training performance in real time?</t>
            </li>
            <li>
              <t>Should AI training be treated like best-effort traffic, where no guarantees are made and resources are allocated as available?</t>
            </li>
            <li>
              <t>Or should training tasks receive prioritized or differentiated service levels, similar to high-priority traffic in traditional networks?</t>
            </li>
          </ul>
          <t>Addressing these questions is essential to ensure predictable and reliable AI model development, especially as training workloads grow in complexity and scale. It may require introducing new QoS frameworks tailored specifically to the needs of AI training systems.</t>
        </section>
        <section anchor="charging-and-billing">
          <name>Charging and Billing</name>
          <t>The AI training process involves a diverse ecosystem of stakeholders, including data owners, model owners, and resource providers. Each of these parties plays one or more vital roles in enabling successful training workflows.</t>
          <t>For example, communication providers contribute not only by transporting data and models across the network but also they themselves may also serve as data providers. This is particularly evident in the emerging design of 6G networks, which integrate sensing capabilities with communication infrastructure. As a result, 6G operators are uniquely positioned to offer both connectivity and data, making them central players in the training pipeline.</t>
          <t>Despite their different roles, all parties contribute to enabling AI training as a service, a complex and resource-intensive process that is far from free. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should training services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Will there be tiered service offerings, such as gold, silver, and platinum, each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to facilitating collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="inference">
      <name>Inference</name>
      <t>Inference is critical because it represents the phase where the model begins to deliver practical value. Unlike training, which is typically a one-time or periodic, resource-intensive process, inference often needs to operate continuously and efficiently, sometimes in real-time. Although inference is a less resource-intensive process, it has strict requirements that govern its success. In what follows, we explore these requirements that shall enable a successful AI inference ecosystem.</t>
      <section anchor="requirement-breakdown">
        <name>Requirement Breakdown</name>
        <t>We envision an inference ecosystem composed of a large number of pre-trained AI models (or agents) distributed across the globe. These models are capable of performing a wide range of tasks, such as image classification, language translation, or speech recognition. Some models may specialize in the same task but vary in performance, accuracy, latency, or resource demands. This diverse pool of models is accessed by numerous inference clients (users or applications) who submit inputs, referred to as queries, and receive task-specific outputs.</t>
        <t>These queries can vary greatly in complexity, structure, and modality, with some requiring the cooperation of multiple models to fulfill a single request. The overarching goal of the ecosystem is to efficiently match incoming queries with the most suitable models, ensuring accurate, timely, and resource-aware responses. Achieving this requires intelligent orchestration, load balancing, and potentially dynamic model selection based on factors such as performance, availability, cost, and user-specific requirements. In what follows, we discuss the various aspects of this ecosystem and discuss the different requirements needed for its success.</t>
        <section anchor="model-deployment-and-mobility">
          <name>Model Deployment and Mobility</name>
          <t>The first step toward building a successful AI inference ecosystem is the optimal deployment of trained models, or agents. In this context, optimality refers to both the physical or network location of the model and the manner in which it is deployed. AI models vary significantly in size and resource requirements—ranging from lightweight models that are only a few kilobytes to large-scale models with billions of parameters. This wide range makes deployment decisions critical to achieving both efficient performance and effective resource utilization. Also, a unique factor to AI models/agents is the fact that they are software components that are not bounded to a certain hardware. They can be deleted, copied, moved, or split across multiple compute locations. All these unique aspects provide flexibility in design if the real-time status of the underlying network dynamics and resources is made accessible.</t>
          <ul spacing="normal">
            <li>
              <t>Choosing the right facility to host a model: whether it's a lightweight edge device, a local server, or a high-performance cloud data center, deployment will depend on the model's size, computational requirements, and expected query volume. For example, smaller models might be best suited for deployment on edge devices closer to users, enabling low-latency responses. In contrast, larger models may require centralized or specialized infrastructure with high compute and memory capacity.</t>
            </li>
            <li>
              <t>Load balancing: Once models are deployed, inference traffic begins to flow, with users or applications sending queries to the appropriate agents. If not managed properly, this traffic can lead to congestion, creating bottlenecks that degrade inference performance through increased latency or dropped requests. To avoid such scenarios, models should be deployed strategically to distribute the load, ensuring smooth operation. Traditional load balancing techniques can be employed to redirect traffic away from overburdened nodes and towards underutilized ones. However, more sophisticated strategies may involve replicating models and placing these replicas closer to regions with high query demand, thereby minimizing latency and easing network traffic engineering challenges.</t>
            </li>
            <li>
              <t>Mobility-aware deployment: the dynamic nature of inference traffic necessitates mobility-aware deployment. For instance, consider a large data center acting as a centralized inference hub, hosting numerous models and handling a significant volume of queries. Over time, this hub may experience traffic overload. In such cases, migrating certain models to alternative locations can help alleviate pressure. However, model migration is not without its challenges—particularly if a model is actively serving queries at the time of migration. In such situations, mobility handling mechanisms must be in place to ensure seamless service continuity. These mechanisms could involve session handovers, temporary state preservation, or model version synchronization, all designed to maintain uninterrupted service during the migration process.</t>
            </li>
          </ul>
          <t>In summary, optimal model deployment requires careful consideration of model size, resource needs, query distribution, and real-time adaptability. Achieving this lays the foundation for a responsive, scalable, and resilient AI inference ecosystem.</t>
        </section>
        <section anchor="model-discovery-and-description">
          <name>Model Discovery and Description</name>
          <t>Just as data descriptors and discovery mechanisms are essential during the training phase, AI model inference clients also require a robust discovery mechanism during the inference stage. In an ecosystem populated by a large and diverse pool of models—each with unique capabilities and specializations—clients are presented with significant flexibility and choice in selecting the most suitable models for their queries. However, to make informed decisions, clients must have access to information that enables them to distinguish between models based on criteria such as performance, specialization, availability, and resource requirements.</t>
          <t>This discovery process becomes even more complex when it needs to function across multiple network domains and heterogeneous communication infrastructures. For instance, a client connected via a wireline network might need to interact with a model deployed on a mobile 3GPP network. Such scenarios raise a critical question: How can model owners advertise their models in a way that ensures discoverability and interoperability across diverse domains?</t>
          <t>Addressing this challenge requires the development of standardized model advertisement and discovery protocols that can operate seamlessly across infrastructure boundaries. These protocols must accommodate differences in network technology, latency constraints, and security requirements while providing consistent and reliable access to model information. Ensuring cross-domain discoverability is crucial to unlocking the full potential of a globally distributed inference ecosystem.</t>
          <t>To enable such cross-domain model visibility and discovery, the following key requirements must be considered:</t>
          <ul spacing="normal">
            <li>
              <t>Model Descriptors: These are metadata objects used by model owners to reveal essential aspects about their datasets to AI inference clients. Effective data descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by inference clients. They should allow model owners to selectively disclose details about their model—such as skills, performance reviews, trust level, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information. To ensure interoperability, model descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>Model/agent Discovery Mechanisms: These refer to the processes by which AI inference clients locate and identify models/agents across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant model/agents regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility.</t>
            </li>
            <li>
              <t>Model/agent relationship maps: As queries may requiring the collaboration between multiple models/agents, relationships between models/agents with respect to different task might present useful tools as to help clients choose the appropriate subset of models/agents that would handle their queries.</t>
            </li>
            <li>
              <t>Timely Reporting: Similar to data, the status of a model can change over time—for example, due to shifts in workload or resource availability. It is important that such changes are reported promptly and accurately, allowing clients to make informed decisions based on the model’s current state. This is essential for ensuring efficient model selection and maintaining high-quality, reliable inference outcomes.</t>
            </li>
          </ul>
          <t>It is important to emphasize that model discovery differs fundamentally from data discovery. While data are passive objects that require external querying or manipulation, models are intelligent, autonomous agents capable of making decisions based on their own capabilities, status, and context. This distinction opens up new and more dynamic possibilities for how models are discovered and engaged in an inference ecosystem.</t>
          <t>In traditional data discovery, clients search for and retrieve relevant datasets based on metadata or predefined criteria. However, in the case of model discovery, the process can be much more interactive and flexible. One approach involves the client actively discovering models by querying a directory or registry using model descriptors. Based on these descriptors, the client selects one or more models to handle a specific inference task. However, given that models can reason and act independently, model discovery does not have to be limited to client-driven selection. An alternative approach is to reverse the flow of interaction. Instead of clients seeking out models, they can publish their tasks to a shared task pool, accessible to all available models. These tasks include descriptors that define the type of work to be done, expected outputs, and quality-of-service requirements. Models can then autonomously scan this pool, evaluate whether they are well-suited for specific tasks, and choose to express interest in executing them. This self-selection process allows models to play an active role in task matching, improving system scalability and efficiency.</t>
          <t>The final assignment of a task can be handled in different ways. Clients may retain full control and approve or reject interested models based on their preferences or priorities. Alternatively, the system may operate in a fully autonomous mode, where tasks are assigned automatically to the first or best-matching model, without requiring client intervention—depending on the client's chosen policy.</t>
          <t>This agent-driven paradigm reflects the shift toward more decentralized and intelligent AI ecosystems, where models are not merely passive computation endpoints but active participants in task negotiation and resource allocation. Such a system not only enhances scalability and flexibility but also allows for more efficient utilization of the available model pool, especially in heterogeneous and dynamic environments.</t>
        </section>
        <section anchor="query-and-inference-result-routing">
          <name>Query and Inference Result Routing</name>
          <t>A significant challenge in AI inference networks lies in efficiently routing client queries to the appropriate inference models and ensuring the corresponding results are reliably delivered back to the client. This becomes particularly complex in scenarios involving mobility and multi-domain environments, where both the client and the model may exist across different types of network infrastructures. The key challenges and considerations include:</t>
          <ul spacing="normal">
            <li>
              <t>Query Routing Across Heterogeneous Networks: When a client accesses the inference ecosystem through a mobile network such as 3GPP 6G, and the target model is hosted in a wireline or cloud-based infrastructure, routing the query across these distinct domains is non-trivial. Differences in network architecture, protocols, and service guarantees complicate the end-to-end flow.</t>
            </li>
            <li>
              <t>Mobility Management During Inference Execution: While mobile networks like 6G are designed to handle user mobility, inference tasks may take time to process—particularly when using large models or performing complex computations. During this time, the client may change physical location, switch devices, or even go offline. Ensuring that inference results can still reach the client under these dynamic conditions poses a significant challenge.</t>
            </li>
            <li>
              <t>Handling Client State Changes: If a client becomes idle or disconnects entirely during inference, the system must decide what to do with the completed result. Should it be queued, buffered, forwarded to another linked device, or simply discarded? A robust mechanism is needed to track client state, maintain context, and guarantee result delivery or at least graceful degradation.</t>
            </li>
            <li>
              <t>Support for Live and Streaming Inference: Some use cases, such as real-time audio transcription, involve live streaming of data from the client to the model and vice versa. These sessions require sustained, low-latency connections and are particularly sensitive to interruptions caused by mobility or handoffs between networks. Ensuring session continuity and maintaining streaming quality across network boundaries is a complex but critical aspect of real-world inference deployments.</t>
            </li>
            <li>
              <t>Cross-Domain Connectivity and Session Management: The involvement of multiple network operators and domains introduces questions around interoperability, session tracking, and handover coordination. There is a need for intelligent infrastructure capable of end-to-end session management, including maintaining metadata, context, and service quality as the session traverses’ different networks.</t>
            </li>
          </ul>
        </section>
        <section anchor="inference-chainingcollaborative-inference">
          <name>Inference Chaining/Collaborative Inference</name>
          <t>Another critical aspect of an AI inference ecosystem is the need for model collaboration to fulfill complex or multi-faceted tasks. Not all inference requests can be handled by a single model; in many cases, collaboration between multiple models is necessary. Effectively managing this task-based collaboration is essential to ensure accurate, efficient, and scalable inference services. Model collaboration can take several distinct forms:</t>
          <ul spacing="normal">
            <li>
              <t>Inference Chaining: In this model, the output of one model serves as the input to the next in a sequential pipeline. Each model performs a specific stage of the task, and the final result—produced by the last model in the chain—is returned to the client. This is common in multi-stage tasks such as image processing followed by object detection and then classification.</t>
            </li>
            <li>
              <t>Parallel Inference: Here, a complex task is decomposed into multiple subtasks, each of which is assigned to a specialized model. These models operate concurrently, and their outputs are aggregated to form a unified inference result. This approach is particularly useful when dealing with large data sets or when a task spans different domains of expertise.</t>
            </li>
            <li>
              <t>Hierarchical inference: A model is assigned as a task manager and is responsible for delegating tasks to service models</t>
            </li>
            <li>
              <t>Collaborative Inference: In this more dynamic and decentralized form, the task is assigned to a group of models that are capable of discovering one another, assessing their respective capabilities, and coordinating among themselves to devise a shared strategy for completing the task. This model requires more sophisticated communication, negotiation, and orchestration mechanisms.</t>
            </li>
          </ul>
          <t>Regardless of the collaboration format, the success of such multi-model interactions depends on the availability of a robust management infrastructure. This infrastructure must enable seamless coordination between models, even when:</t>
          <ul spacing="normal">
            <li>
              <t>The models are hosted by different providers,</t>
            </li>
            <li>
              <t>They are deployed across heterogeneous communication networks,</t>
            </li>
            <li>
              <t>They use varying protocols, or</t>
            </li>
            <li>
              <t>They have differing performance characteristics.</t>
            </li>
          </ul>
          <t>Such a management system must abstract away the underlying complexities and provide standardized interfaces, discovery mechanisms, communication protocols, and coordination frameworks that allow models to interact effectively. Without this, collaborative inference would be brittle, inefficient, or impossible to scale. In essence, the ability to orchestrate model collaboration across diverse environments is a cornerstone of a flexible, intelligent, and robust AI inference ecosystem.</t>
        </section>
        <section anchor="compute-and-resource-management">
          <name>Compute and Resource Management</name>
          <t>In many scenarios, the compute infrastructure used to host and run inference models is managed by third-party providers, not the model owners themselves. These compute providers are responsible for meeting the Quality of Service (QoS) levels agreed upon with the model owners—such as latency, uptime, throughput, and reliability.</t>
          <ul spacing="normal">
            <li>
              <t>Ensuring these service levels are consistently met raises the question of accountability. If performance degrades due to compute resource issues—such as overloaded hardware or network outages—who is responsible for the failed inference tasks?</t>
            </li>
            <li>
              <t>There must be clear, enforceable service-level agreements (SLAs) that define roles, responsibilities, and penalties for non-compliance.</t>
            </li>
            <li>
              <t>Mechanisms for performance monitoring, auditing, and dispute resolution need to be integrated into the ecosystem to make such arrangements viable and trustworthy.</t>
            </li>
          </ul>
        </section>
        <section anchor="privacy-preservation-and-security">
          <name>Privacy Preservation and Security</name>
          <t>While models are the intellectual property of their owners, they may operate on infrastructure owned by others. This raises significant concerns around privacy and intellectual property protection.</t>
          <ul spacing="normal">
            <li>
              <t>Sensitive model details such as architecture, weights, and optimization strategies must be protected from exposure or reverse engineering by untrusted compute hosts.</t>
            </li>
            <li>
              <t>Techniques such as secure computing, encrypted model execution, and remote attestation protocols may be necessary to ensure that models run securely without revealing proprietary details.</t>
            </li>
            <li>
              <t>Model owners must also be assured that inference inputs and outputs remain confidential, particularly in applications involving personal or sensitive data.</t>
            </li>
          </ul>
        </section>
        <section anchor="utility-handling-and-qos-requirements">
          <name>Utility Handling and QoS Requirements</name>
          <t>Utility handling refers to the regulation, protection, and fair governance of how models are used, accessed, and monitored throughout the ecosystem. This encompasses several critical questions:</t>
          <ul spacing="normal">
            <li>
              <t>How can we guarantee that a model deployed on remote infrastructure is not being tampered with, copied, or intentionally repurposed?</t>
            </li>
            <li>
              <t>How do we ensure that workload distribution is fair across available models, preventing monopolization by a few and giving equal visibility and opportunity to all participating models?</t>
            </li>
            <li>
              <t>What protections are in place to ensure that models are not being poisoned, exploited, or involved in illegal activities, either through malicious inputs or untrusted outputs?</t>
            </li>
            <li>
              <t>How do we ensure the integrity of inference results, so that outputs are delivered to clients without alteration, manipulation, or censorship?
Addressing these concerns may require digital rights management (DRM) for AI models, usage monitoring tools, and potentially blockchain-based logging or audit trails to ensure transparency and traceability.</t>
            </li>
          </ul>
          <t>On the other hand, the definition of Quality of Service (QoS), when it comes to inference tasks, is very broad and can take many forms. For instance, QoS could be to guarantee a certain accuracy of a response, or time of the response, or expertise level needed. We believe that the topic of QoS guarantee requires extensive studying and analysis.</t>
        </section>
        <section anchor="model-upgrade-streamlining">
          <name>Model Upgrade Streamlining</name>
          <t>AI models are not static; they undergo continuous upgrades, improvements, and fine-tuning to maintain accuracy, adapt to new data, or support evolving tasks.</t>
          <ul spacing="normal">
            <li>
              <t>The ecosystem must support seamless model versioning, including adding, removing, or modifying model agents without disrupting ongoing services.</t>
            </li>
            <li>
              <t>Updated model profiles must be instantly reflected in the discovery layer, ensuring clients always have access to the most current and accurate model descriptions.</t>
            </li>
            <li>
              <t>For large models, upgrade procedures must be efficient and bandwidth-conscious, potentially using incremental update techniques to avoid full redeployment.</t>
            </li>
            <li>
              <t>Moreover, strategies must be in place to handle hot-swapping of models, where an old model is gracefully decommissioned and replaced by a new one—without causing inference failures or data loss during the transition.</t>
            </li>
          </ul>
        </section>
        <section anchor="charging-and-billing-1">
          <name>Charging and Billing</name>
          <t>The AI inference process involves a diverse ecosystem of stakeholders, including model owners, compute providers, and communication providers. Each of these parties plays one or more vital roles in enabling successful inference workflows. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should inference services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Will there be tiered service offerings—such as gold, silver, and platinum—each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
            <li>
              <t>What about discovery framework providers? Would they be offering a free service like google search or would it be more structured?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to fostering collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="data-and-agent-aware-inference-and-training-network-da-itn-general-framework">
      <name>Data and Agent Aware Inference and Training Network (DA-ITN): General Framework</name>
      <t>The DA-ITN is envisioned as a multi-domain, multi-technology network operating at the AI layer, designed to address the various layers of complexity inherent in modern AI ecosystems. As mentioned earlier, the network aims to support a wide range of requirements, some of which are outlined above, across AI training, inference, and agent-to-agent interaction.</t>
      <t>The network consists of set of nodes and equipment connected via one or more traditional underlay networks as depicted below.</t>
      <figure anchor="fig1">
        <name>Figure 1: DA-ITN nodal view</name>
        <artwork align="center"><![CDATA[
+---------------------------------------------+
| DA-ITN nodal view                           |
|                                             |
|  +----------------+     +----------------+  |    DA-ITN node types
|  | DA-ITN Node (A)|<--->| DA-ITN Node (B)|  |      A- Data node
|  +----------------+  |  +----------------+  |      B- Compute node
|                      |                      |      C- Storage node
|                      |                      |      D- Model node
|  +----------------+  |  +----------------+  |      E- Evaluation node
|  | DA-ITN Node (E)|<--->| DA-ITN Node (G)|  |      F- Agent node
|  +----------------+  |  +----------------+  |      G- Multi-purpose node
|                      |                      |
|                      |                      |
|  +----------------+  |  +----------------+  |
|  | DA-ITN Node (F)|<--->|DA-ITN Node(C+D)|  |
|  +----------------+     +----------------+  |
|                                             |
+---------------------------------------------+
]]></artwork>
      </figure>
      <t>Nodes with DA-ITN along with its core functionality interact together to provide different training, inference, and agentic services. In this manner, DA-ITN can be divided into four interacting major building blocks as shown bellow.</t>
      <figure anchor="fig2">
        <name>Figure 2: DA-ITN high level architecture and building blocks</name>
        <artwork align="center"><![CDATA[
+--------------------+         +--------------------+ 
|   DA-ITN Service   |         |   DA-ITN Client    |
| Provider Community |         |     Community      |
+--------------------+         +--------------------+
    ↑     ↑                               ↑     ↑
    |     |                               |     |
    |     |                               |     |
    |     +-------------------------------+     |
    |                     |                     |
    |                     |                     |
    |                     ↓                     |
    |           +--------------------+          |
    |           |     DA-ITN Core    |          |
    |           |                    |          |
    |           +--------------------+          |
    |                     ↑                     |
    |                     |                     |
    |                     |                     |
    ↓                     ↓                     ↓
+---------------------------------------------------+
|                    DA-ITN Enablers                |
+---------------------------------------------------+
]]></artwork>
      </figure>
      <section anchor="da-itn-core">
        <name>DA-ITN Core</name>
        <t>This block contains DA-ITN main internal modules, functions, and services. Dedicated logical
planes in this block handle interactions between its different modules and functions. Interactions between different modules and functions in this block are not visible or accessible to entities in other blocks. DA-ITN core offers its services to external entities via clear and well defined interfaces and protocols. The following illustrates different modules and functions of DA-ITN core block.</t>
        <figure anchor="fig3">
          <name>Figure 3: DA-ITN core and its different modules and function</name>
          <artwork align="center"><![CDATA[
+-----------------------------------+
|            DA-ITN Core            |
|                                   |
|   +----------+ +--------------+   |
|   | X-RCE    | |Registration &|   |     X-RCE:  Training, model, query, etc.
|   |          | |Authentication|   |             route compute engine
|   +----------+ +--------------+   |     XOD:    Model, agent deployment  
|   +----------+ +--------------+   |             optimizer
|   | X-DO     | |Discovery &   |   |     S-FAM:  Different Service feasibility
|   |          | |Advertisement |   |             assessment module
|   +----------+ +--------------+   |     TAG:    Training algorithm generator
|   +----------+ +--------------+   |     PVM:    Performance verification
|   | S-FAM    | |Billing &     |   |             Module 
|   |          | |Accounting    |   |     DDRT:   Data dynamics and resource
|   +----------+ +--------------+   |             topology
|   +----------+ +--------------+   |
|   | TAG      | |Reputation &  |   |
|   |          | |Trust Mgmt.   |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   | PVM      | | Upgrade Mgmt.|   |
|   |          | |              |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   | Resource | |Mobility Mgmt.|   |
|   | Mgmt.    | |              |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   |   DDRT   | | Tools Mgmt.  |   |
|   |          | |     ???      |   |
|   +----------+ +--------------+   |
|            +---------+            |
|            |   OAM   |            |
|            +---------+            |
+-----------------------------------+
]]></artwork>
        </figure>
      </section>
      <section anchor="da-itn-service-provider-community">
        <name>DA-ITN Service Provider Community</name>
        <t>Providers for different services such as data, model, agent, and resource providers reside within the Service Provider Community block of the DA-ITN. Service providers join the network via a registration and authentication process offered by DA-ITN core. The service providers use DA-ITN to advertise their services, capabilities, etc. across the overall network. They can also register for notifications to get updates e.g. arrival of new models, training data, agents, etc. DA-ITN dispenses revenue to providers for the services rendered via its billing and accounting module.</t>
        <t>The following figure shows different modules of DA-ITN service provider community.</t>
        <figure anchor="fig4">
          <name>Figure 4: DA-ITN Service Provider Community</name>
          <artwork align="center"><![CDATA[
+-------------------------------+
|       DA-ITN Service          |
|     Provider Community        |
|                               |
|  +----------+ +----------+    |
|  | Data     | | Model    |    |
|  | providers| | providers|    |
|  +----------+ +----------+    |
|  +----------+ +----------+    |
|  | Agent    | | Resource |    |
|  | providers| | providers|    |
|  +----------+ +----------+    |
|  +--------------+             |
|  | Tools        |             |
|  | providers ???|             |
|  +--------------+             |
+-------------------------------+
]]></artwork>
        </figure>
        <t>The tool module within the provider block requires further investigation and analysis. Agentic protocols such as Model Context Protocol(MCP) provide access to MCP tools from the agent interaction point of view. Whether DA-ITN needs to support additional capabilities w.r.t agents or whether it needs to support distinct tools w.r.t training and inference is an open question for now. Will there be a need for unified tools' protocols that fits all utilities, or a protocol per utility?</t>
      </section>
      <section anchor="da-itn-client-community">
        <name>DA-ITN Client Community</name>
        <t>This block represents the client side of DA-ITN. The clients are network participants requiring training, inference, agent-to-agent interactions, and those who need access to resources such as storage, compute, etc. offered by resource providers in DA-ITN.</t>
        <t>DA-ITN enables clients to discover potential providers by tuning into DA-ITN discovery, and advertisement module, allowing them to select the best match based on their requirements. Alternatively, clients may delegate the matching process to DA-ITN, requesting DA-ITN to identify the most suitable provider based on their criteria. For example, a client using the model training service may opt to fully control the training process and make all decisions independently. Alternatively, the client can delegate the training responsibilities to the DA-ITN core. In the case of delegation, modules such as X-RCE, DDRT, PVM, S-FAM, and TAG can work collaboratively to train the model on the client’s behalf and deliver the finalized, trained model back to them.</t>
        <figure anchor="fig5">
          <name>Figure 5: DA-ITN Client Community</name>
          <artwork align="center"><![CDATA[
+-------------------------------+
|       DA-ITN Client           |
|         Community             |
|                               |
|  +----------+ +----------+    |
|  | Data     | | Model    |    |
|  | clients  | | clients  |    |
|  +----------+ +----------+    |
|  +----------+ +----------+    |
|  | Agent    | | Resource |    |
|  | clients  | | clients  |    |
|  +----------+ +----------+    |
|  +--------------+             |
|  | Tools        |             |
|  | Clients   ???|             |
|  +--------------+             |
+-------------------------------+
]]></artwork>
        </figure>
        <t>It must be noted that a node/entity in DA-ITN can act both as provider and/or a client. For example, a node providing data as its service, might need access to a resource provider service. Or a model provider enabling inference might employ the services of data providers for Retrieval-Augmented Generation (RAG).</t>
        <t>Similar to the provider community block in DA-ITN, the tools module withing the client community requires further study.</t>
      </section>
      <section anchor="da-itn-enablers">
        <name>DA-ITN Enablers</name>
        <t>This layer represents external and underlying services that DA-ITN itself employs to accomplish its different tasks. Various networking layers, access technologies, location, and sensing functions are examples of such services.</t>
        <figure anchor="fig6">
          <name>Figure 6: DA-ITN Enablers</name>
          <artwork align="center"><![CDATA[
+-------------------------------------------------------------------------+
|                             DA-ITN Enablers                             |
|                                                                         |
|  +---------------------------+  +-----------+  +-----------+            |
|  | Communications/Networking |  | Location  |  |  Sensing  |            |
|  |                           |  |           |  |           |            |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | | Mobile  |  | Internet | |  | | GPS   | |  | | IoT   | |            |
|  | | network |  +----------+ |  | +-------+ |  | +-------+ |            |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | | NTN     |  | WiFi     | |  | |Sensors| |  | | ISAC  | |  Others??? |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | +-----------------------+ |  | +-------+ |  | +-------+ |            |
|  | |        Others?        | |  | |Mobile | |  | |Others?| |            |
|  | +-----------------------+ |  | |network| |  | +-------+ |            |
|  |                           |  | +-------+ |  |           |            |
|  |                           |  | +-------+ |  |           |            |
|  |                           |  | |Others?| |  |           |            |
|  |                           |  | +-------+ |  |           |            |
|  +---------------------------+  +-----------+  +-----------+            |
+-------------------------------------------------------------------------+
]]></artwork>
        </figure>
      </section>
    </section>
    <section anchor="da-itn-high-level-architecture">
      <name>DA-ITN high level architecture</name>
      <t>To manage these complexities and cater for the requirements, we propose structuring the DA-ITN around four core components: a Control Plane (CP), a Data Plane (DP), an Operations and Management (OAM) Plane, and an Intelligence Layer. It is important to note that the DA-ITN is agnostic to the underlying communication infrastructure, allowing it to operate seamlessly over heterogeneous networks, whether mobile, wire-line, or satellite-based. he DA-ITN integrates with these underlying infrastructures through any available means, embedding its control and intelligence capabilities to coordinate and manage AI-specific services in a flexible and scalable manner.</t>
      <section anchor="control-plane-and-intelligence-layer">
        <name>Control plane and Intelligence Layer</name>
        <t>The Control Plane and Intelligence Layer work together to enable an efficient, reliable, and timely information collection infrastructure. They continuously gather up-to-date information on data availability, model status, agent conditions, resource utilization, and reachability across all participating entities. The collected information comes in the form of dynamic descriptors for data, models, and resources, essential components for enabling intelligent, context-aware decision-making within the AI ecosystem as has previously been highlighted. Also, with the help of data, resource, and reachability topology engine (DRRT) housed within the intelligence layer, the gathered information and descriptors can be used to construct meaningful relationships across the ecosystem. These are captured in the form of dynamic topologies or map-like structures, which help optimize decision-making processes across training, inference, and agent-to-agent collaboration tasks. This design provides a continuous awareness that is very essential for the success, reliability, accuracy, and responsiveness of the AI functionalities and services enabled by the DA-ITN within the AI ecosystem.</t>
        <t>The DA-ITN control plane also lays a foundation for an advanced discovery infrastructure where the generated descriptors can be made easily accessible to all authorized participants to facilitate their required AI service For example, AI clients subscribed to training services can access up-to-date data descriptors and resource topologies, enabling them to select appropriate datasets and compute resources that align with their performance and accuracy goals. Similarly, inference clients or agents seeking collaboration can discover models based on capabilities, or submit task descriptors that enable models to respond intelligently and autonomously.</t>
        <t>Aside from descriptor collection, topology creation, and discovery, the DA-ITN control plane also supports a secure and trusted environment where clients, data providers, model providers, and resource providers can engage in AI processes without compromising integrity or accountability. It also plays a key role in managing charging, billing, and rights enforcement, ensuring that all contributors to the AI service chain are fairly compensated and protected.</t>
        <t>It is worth noting that the DA-ITN’s Control Plane is not constrained by specific protocol stacks. Instead, it provides a flexible connectivity and coordination infrastructure upon which various AI-related protocols—such as Agent-to-Agent (A2A), Model Control Protocol (MCP), or AI Coordination Protocol (ACP)—can operate. Regardless of the protocol used, implementations must meet the core DA-ITN requirements, including timely information exchange, flexible descriptor encapsulation, support for multi-model and multi-domain environments, and robust security and privacy protections. The DA-ITN is also designed to support both centralized and decentralized modes of operation, offering high adaptability across different deployment contexts.</t>
        <t>It’s also important to clarify that the Intelligence Layer encompasses all previously mentioned DA-ITN core functions, along with any additional intelligence required to support the full range of DA-ITN services. The term “Intelligence Layer” is intentionally broad to allow flexibility in its design and contents. Nonetheless, its role is clearly defined: it serves as a functional layer that interfaces with other DA-ITN components through the control plane, data plane, and OAM plane to fulfill its responsibilities.</t>
      </section>
      <section anchor="data-plane">
        <name>Data Plane</name>
        <t>On the other hand, the Data Plane of the DA-ITN provides support for mobility management and intelligent scheduling, enabling the dynamic creation of rendezvous points where data, queries, models, agents, and compute infrastructure can be brought together with minimal latency and overhead. Thanks to its infrastructure-agnostic nature, the DA-ITN leverages existing communication networks—such as those offered by 6G or edge service providers—as tools to enable model mobility, data mobility, and agent-to-agent coordination. This capability is essential for supporting scenarios where mobility or geographical dispersion of resources would otherwise lead to performance degradation or inefficiency.</t>
        <t>The construction of the Data Plane may fall under the responsibility of the DA-ITN core or Intelligence Layer, which would orchestrate the necessary resources from the DA-ITN Enabler block to build the required structure. Alternatively, the Enabler block itself may possess sufficient intelligence to autonomously construct the Data Plane as needed.</t>
      </section>
      <section anchor="operation-and-management-plane-oam">
        <name>Operation and Management Plane (OAM)</name>
        <t>Finally, the Operations and Management (OAM) layer plays a critical role in supporting the day-to-day operational needs of the AI ecosystem. This layer is responsible for a wide range of essential functions, including monitoring, registration, configuration, fault management, and lifecycle maintenance of models, data, and services. It serves as the management backbone of the DA-ITN, ensuring transparency, accountability, and operational control throughout the system.</t>
        <t>Consider the scenario of an AI model training client deploying a model into the ecosystem for training. Through the capabilities of the OAM layer, the client can continuously monitor the training performance of their model in real time—tracking key performance indicators such as convergence speed, loss metrics, resource usage, and network traversal. The model’s location within the ecosystem can be dynamically tracked, allowing clients to know exactly where their model resides or which data centers or devices it is interacting with.</t>
        <t>Moreover, the OAM layer enables interactive control. Clients can use it to adjust training parameters on the fly, such as learning rates, data sampling strategies, or the choice of collaborative partners. They can even pause, resume, or terminate the training process at will, giving them full agency over the lifecycle of their models. This flexibility is crucial in adaptive AI systems where responsiveness and real-time decision-making are valued.</t>
        <t>In this way, the OAM layer effectively functions as the control dashboard or command-line terminal of the DA-ITN-enabled AI ecosystem. Whether through a graphical user interface (GUI), APIs, or automated orchestration scripts, the OAM provides the necessary tools for fine-grained management, status visualization, and policy enforcement.</t>
        <t>Beyond individual model control, the OAM layer also facilitates system-wide coordination and policy administration. OAM in coordination with a potential policy enforcement module man help ensuring compliance with service-level agreements (SLAs), enforcing data governance policies, and managing access rights across domains. It plays a foundational role in building trustworthy, maintainable, and operationally efficient AI services across diverse infrastructure providers and stakeholders.</t>
      </section>
      <section anchor="summary-of-the-da-itn-general-framework">
        <name>Summary of the DA-ITN General Framework</name>
        <t>Accordingly, the DA-ITN is well positioned and designed to provide a range of intelligent services that can be leveraged by both AI clients and service providers. It forms the foundation for a scalable, decentralized AI internet, driving the emergence of a vibrant and cooperative agent-based ecosystem. By enabling the formation of adaptive and intelligence-driven topologies and being agnostic to the infrastructure, the DA-ITN facilitates more effective decisions in AI training, inference, and agent-to-agent interactions—ultimately supporting a more responsive, resilient, and capable AI infrastructure that can scale with future demands.</t>
        <t>In the following sections, we provide more detailed insights into the specific DA-ITN components that support training and inference services.</t>
      </section>
    </section>
    <section anchor="da-itn-for-training">
      <name>DA-ITN for Training</name>
      <t>The training architecture of the DA-ITN consists of five layers: i) the terminal layer (DA-ITN provider and client communities); ii) the network layer (Enablers); iii) the data, resource, and reachability topology layer (DRRT); iv) the DA-ITN intelligence layer (DA-ITN core); and v) the OAM layer. The layers interact together using control and data planes (CP and DP respectively) as is discussed in the following.</t>
      <t>First, the network layer, which is at the heart of the DA-ITN training system, is responsible for providing connectivity services to the four other layers. It provides both control and data plane connectivity to enable various services. The network layer connects to the terminal and DRRT layers via CP and DP links, and connects to the intelligence layer via a CP link only. The network layer also enables the overarching OMA layer by enabling a multi-layer connectivity structure.</t>
      <t>Second, the terminal layer from the point of view of training, is the lowest layer in the architecture, and it contains the terminal components of the system. These include nodes that host the training data, facilities that provide computing resources where the model can be trained, and newly proposed components that we refer to as the model performance verification modules (MPVMs), where the model testing phase takes place. It should be noted that facilities providing computing resources come in various forms including private property such as personal devices, in a distributed form such as in the case of mobile edge computing in 6G networks, on the cloud such as on the AWS cloud, or anywhere that is accessible by both the data and the model and holds sufficient compute for training. As for the MPVU, this unit is important when conducting distributed training as it takes the role of a trusted proxy node that holds a globally constructed testing dataset - the dataset is constructed via collecting sample datasets from each participating node - and provides safe and secure access to it. Last, the terminal layer also hosts the AI training clients.</t>
      <t>The terminal layer relies on the network layer to build an overarching knowledge-sharing network. To be exact, the network layer provides three main services to the terminal layer, namely: i) moving models and data between the identified rendezvous compute points where training can happen; ii) moving the models towards the MPVU units where performance evaluation can be conducted to keep track of the training progress; and iii) enabling AI training clients to submit their models, monitor the training progress, modify training requirements, and collect the trained models. Control and data traffic exist for each one of these services. For instance, moving a model toward a compute facility requires authorization for the utility of the resources; hence, authorization control data is required to be exchanged over the Terminal-NET CP links. The service also requires the physical transmission of the model to the computing facility which is handled over the Terminal-NET DP link. Similar situations can be extrapolated for the other provided services. It is worth noting that the network layer can be built on top of any access network technology including 3GPP cellular networks, WiFi, wireline, peer-to-peer, satellites, and non-terrestrial networks (NTN), or a combination of the above. These networks can be used to build dedicated CP and DP links strictly designed to enable the DA-ITN training system and its services.</t>
      <t>Third, the DRRT layer holds all the information required to make accurate decisions and sits between the intelligence layer and the terminal layer. It consists of a DRRT-manager (DRRT-M) unit which is the brain of this layer and interfaces with the other layers over CP links. The DRRT layer provides the intelligence layer with visibility and accessibility services to specific information about the underlying terminal layer's data, resource, and reachability status. To be exact, the DRRT layer holds information regarding the type, quality, amount, age, dynamics, and any other essential information about the data available for training. It also provides reachability information of the participating nodes to avoid unnecessary communication overhead and packet droppage.  Lastly, the DRRT also contains information about computing resources and MPVUs such as resource availability, location, trustworthiness, and nature of the testing datasets hosted at the different MPVF units.</t>
      <t>The DRRT relies on the network layer to collect the necessary information to build the Global-DRRT topology (G-DRRT). The G-DRRT is a none model specific topology, it is rather a large canvas that holds the high-level view of the data, resource, and reachability information. The DRRT-M unit in the DRRT layer communicates with the network layer over CP links to manage the collection process of the required information. For instance, the DRRT-M may instruct the 3GPP component of the network layer to convey connectivity information about the data nodes, or it might instruct it to wake up an ideal data provider device. It might also instruct satellites to share GPS locations of mobile data nodes. The collected data by the network layer are then shipped toward the G-DRRT component of the DRRT layer over DP links. The G-DRRT hosts intelligence that allows it to convert the collected information into useful global topology ready to provide services to the AI training clients.</t>
      <t>Fourth, The Intelligence Layer is responsible for hosting the decision-making logic required to fulfill the specific training requirements submitted by clients. It contains several key components that collaboratively determine how, where, and whether training should proceed. Among these is the Model Training Route Compute Engine (MTRCE), which identifies suitable rendezvous points between models and data. Another critical component is the Training Feasibility Assessment Module (T-FAM), which functions as an admission controller—evaluating whether a submitted model, given its requirements and constraints, can be effectively trained within the available ecosystem.</t>
      <t>Additional intelligent modules include the Training Algorithm Generator (TAG) and the Hyperparameter Optimizer (HPO). These components are responsible for selecting the appropriate training paradigm—such as reinforcement learning (RL), federated learning (FL), or supervised learning (SL)—as well as determining other configuration details like the number of training epochs, batch size, and optimization strategy. The Intelligence Layer also interfaces with both the Network Layer and the DRRT Layer to acquire the context needed for effective decision-making. From the Network Layer, it receives control data over CP links—this includes model structure, target accuracy, convergence time, monitoring instructions, and client-specified training preferences. It also receives feedback data that allows the TAG and HPO modules to refine their recommendations dynamically.</t>
      <t>Meanwhile, the Intelligence Layer connects to the DRRT Layer via both CP and DP links to access up-to-date visibility into training data, compute resources, and node reachability. This information is essential for components like MTRCE and T-FAM to make routing and admission decisions. To further enhance decision efficiency, the Intelligence Layer may also host a DRRT-Adaptability Unit (DRRT-A). This optional module works in coordination with MTRCE, T-FAM, and the DRRT Manager (DRRT-M) to generate model-specific DRR topologies—lightweight, targeted representations carved out from the global DRR topology. These customized topologies are optimized to reduce computational overhead and accelerate decision-making for individual training requests.</t>
      <t>Last, the OAM layer, which spans all the layers, is mainly intended as a management layer to configure the training components, the connectivity of the network layer, and enable feedback functions essential for progress monitoring and model localization and tracking. It is also intended to provide feedback to the clients about their submitted models every step of the way.</t>
    </section>
    <section anchor="da-itn-for-inference">
      <name>DA-ITN for Inference</name>
      <t>The Inference architecture of the DA-ITN provides automated AI inference services using a similar structure to the training architecture with a few differences.</t>
      <t>First, unlike training, where the moving components are models and training data, and the rendezvous points are computing facilities, in inference, models/agents and queries/tasks are the moving components that require networking, and the rendezvous points are model hosting facilities.</t>
      <t>Second, in inference, the clients are both the task/query owners as well as the model/agent owners. Query owners are the inference service users who send their queries into the system and collect the resulting inference. On the other hand, model owners are divided into two types. The first type consists of model hosts - the model used for inference does not have to be owned by them, but it is hosted on their computing facilities.  The second type consists of model/agent providers - they develop models/agents and deploy them either at their own facilities or at model hosts. Model owners are represented in the terminal layer as model deployment facility providers (MDFP) which are distributed across the global network.</t>
      <t>Third,  the network layer provides the following services to the terminal layer using its control and data planes: i) model mobility from model generators to model hosts; ii) query routing towards models deployed on MDFPs; iii) model mobility from one location to the other in case of load balancing situations; iv) model mobility towards re-training and calibration facilities which may be hosted on MVPF units; v) query response and inference result routing towards the query owners or any indicated destination around the globe; and vi) feedback and monitoring information to model and query owners.</t>
      <t>Fourth, the DRRT layer is replaced by a query, resource, and reachability topology (QRRT) layer. It provides the same type of services to the other layers; however, from the point of view of queries and models. That is, it provides information about both models and queries such as i) for models: model locations, model capabilities, current loading conditions, inference speed, inference accuracy, model reachability and accessibility (i.e., reachability and accessibility of the MDFP), and ii) for query: query patterns and dynamics (could be associated with a geographical location), query types, and reachability status of query owners for response communication purposes. The information collected by the QRRT is used to make appropriate decisions about model deployment and distribution strategies, query-to-model routing decisions, and response routing decisions. The QRRT has a management function that coordinates with the Network layer to collect the required information from the terminal layer to build the Global-QRRT (G-QRRT). It also optionally communicates with the QRRT-adaptation (QRRT-A) function in the inference intelligence layer to build query- or model-specific QRRTs.</t>
      <t>Last, the inference intelligence layer hosts different intelligent decision-making components including the Query Feasibility Assessment Module (Q-FAM), the Query Inference Route Compute Engine (QIRCE), and the Model Deployment Optimizer module (MDO). Just like with the training, these components make decisions based on the QRRT. For instance, the Q-FAM hosts intelligence that acts as an admission control unit that evaluates if a submitted query could be serviced given the current network inference capabilities. The QIRCE handles query routing towards the correct models while observing loading conditions. Furthermore, the MDO module acts as an admission controller for newly submitted models where it evaluates deployment feasibility based on the submitted model's architecture, compute requirements, and storage requirements. It matches these requirements to the currently available resources indicated in the QRRT and makes an admittance decision. It also handles deployment location optimization, aiming to minimize query response time and cost for inference.</t>
    </section>
    <section anchor="da-itn-facilitation-agentic-networks">
      <name>DA-ITN-Facilitation Agentic Networks</name>
      <t>While agent-to-agent interaction is commonly associated with task-oriented collaboration—often relying on inference chaining as discussed in the inference section—we propose that this only reflects one side of the coin. We believe there is a transformative alternative: collaborative agent training, where agents not only work together to complete tasks, but also contribute to each other's learning and evolution. This paradigm marks a significant shift from traditional models and positions the DA-ITN as an ideal enabler of a truly agentic future, where intelligent agents can grow, adapt, and improve continuously through structured cooperation.</t>
      <t>It is important to distinguish clearly between collaborative training and task-based collaboration. In task-based collaboration, agents exchange data or partial inferences related to the execution of a specific, external objective—such as processing a query or generating an output. Their internal models remain unchanged; they simply contribute to a shared computational goal. In contrast, collaborative training focuses on internal evolution: the goal is not to solve an external task, but to enhance the capabilities of the participating agents themselves.</t>
      <t>In a collaborative training setup, agents may exchange model parameters, training datasets, or knowledge representations. They may engage in distributed training paradigms such as federated learning, where learning happens locally and updates are shared globally, or continual learning, where agents adapt over time based on new experiences. They may also employ knowledge distillation or transfer learning, where more advanced "teacher agents" guide "student agents" through structured training programs.
One can even envision a highly dynamic and autonomous system where agents attend “agent schools”—virtual environments where they gather to learn, be tested, and graduate. In this imagined scenario, teacher agents would be responsible for training student agents, evaluating their performance, and possibly issuing certifications or verifiable credentials that guarantee the agent’s competencies and readiness for deployment. These credentials serve trust foundations in the broader agent ecosystem, ensuring that certified agents can be reliably selected and trusted by inference clients or other agents.</t>
      <t>To support such a vision, a wide range of new functional and technical requirements must be addressed. These include secure model sharing, certification and validation infrastructure, identity management, trust negotiation, resource discovery for training, and scheduling of learning sessions. Fortunately, many of these requirements align naturally with the capabilities and components of the DA-ITN architecture—including its support for mobility, discovery, descriptor sharing, trust enforcement, dynamic rendezvous, and topology management.</t>
    </section>
    <section anchor="security-considerations">
      <name>Security Considerations</name>
      <t>Security considerations are as outlined within the document under the privacy and security requirements</t>
    </section>
    <section anchor="iana-considerations">
      <name>IANA Considerations</name>
      <t>This document has no IANA actions.</t>
    </section>
    <section anchor="conclusions">
      <name>Conclusions</name>
      <t>As AI continues to evolve and integrate into every facet of modern life, it becomes increasingly clear that the supporting infrastructure must evolve with it. The training and inference processes—central to the success of AI—are no longer simple, isolated tasks; they are complex, distributed, and require intelligent coordination across data, compute, and communication domains.</t>
      <t>The DA-ITN architecture offers a forward-looking response to this complexity by providing a cohesive, scalable, and intelligent network ecosystem. With its dedicated control, data, and operations &amp; management planes, DA-ITN not only supports the technical requirements of training and inference but also addresses critical concerns such as mobility, privacy, trust, and agent collaboration.</t>
      <t>Ultimately, DA-ITN lays the foundation for a new generation of AI-native networks—capable of enabling persistent learning, dynamic agent interaction, and decentralized intelligence at scale. As we move toward an AI-driven future, such architectures will be essential for building reliable, trustworthy, and efficient AI ecosystems.</t>
    </section>
  </middle>
  <back>
    <section anchor="contributors" numbered="false" toc="include" removeInRFC="false">
      <name>Contributors</name>
      <contact fullname="Arashmid Akhavain">
        <organization>Huawei Canada</organization>
        <address>
          <email>arashmid.akhavain@huawei.com</email>
        </address>
      </contact>
      <contact fullname="Hesham Moussa">
        <organization>Huawei Canada</organization>
        <address>
          <email>hesham.moussa@huawei.com</email>
        </address>
      </contact>
      <contact fullname="Tong Wen">
        <organization>Huawei</organization>
        <address>
          <email>tongwen@huawei.com</email>
        </address>
      </contact>
    </section>
  </back>
  <!-- ##markdown-source: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-->

</rfc>
