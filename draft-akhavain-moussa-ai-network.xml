<?xml version='1.0' encoding='utf-8'?>
<!DOCTYPE rfc [
  <!ENTITY nbsp    "&#160;">
  <!ENTITY zwsp   "&#8203;">
  <!ENTITY nbhy   "&#8209;">
  <!ENTITY wj     "&#8288;">
]>
<?xml-stylesheet type="text/xsl" href="rfc2629.xslt" ?>
<!-- generated by https://github.com/cabo/kramdown-rfc version 1.7.29 (Ruby 3.2.3) -->
<rfc xmlns:xi="http://www.w3.org/2001/XInclude" ipr="trust200902" docName="draft-akhavain-moussa-ai-network-01" category="info" consensus="true" submissionType="IETF" tocInclude="true" sortRefs="true" symRefs="true" version="3">
  <!-- xml2rfc v2v3 conversion 3.30.2 -->
  <front>
    <title abbrev="AI-Internet">AI Network for Training, Inference, and Agentic Interactions</title>
    <seriesInfo name="Internet-Draft" value="draft-akhavain-moussa-ai-network-01"/>
    <author fullname="Arashmid Akhavain">
      <organization>Huawei Canada</organization>
      <address>
        <email>arashmid.akhavain@huawei.com</email>
      </address>
    </author>
    <author fullname="Hesham Moussa">
      <organization>Huawei Canada</organization>
      <address>
        <email>hesham.moussa@huawei.com</email>
      </address>
    </author>
    <date year="2025" month="September" day="09"/>
    <area>Internet</area>
    <workgroup>Internet Area Working Group</workgroup>
    <keyword>AI Network</keyword>
    <keyword>Agentic Networks</keyword>
    <keyword>AI inference</keyword>
    <keyword>AI training</keyword>
    <abstract>
      <?line 50?>

<t>Artificial Intelligence (AI) is rapidly reshaping industries and daily life, driven by advances in large language models (LLMs) such as ChatGPT, Claude, Grok, and DeepSeek. These models have demonstrated the transformative potential of AI across diverse applications, from productivity tools to complex decision-making systems. However, the effectiveness and reliability of AI hinge on two foundational processes: training and inference. Each presents unique challenges related to data management, computation, connectivity, privacy, trust, security, and governance.
This document introduces the Data Aware-Inference and Training Network (DA-ITN)—a unified, intelligent, multi-plane network architecture designed to address the full spectrum of AI system requirements. DA-ITN provides a scalable and adaptive infrastructure that connects AI clients, data providers, service facilitators, and computational resources to support end-to-end AI lifecycle operations. The architecture features dedicated control, data, and operations &amp; management (OAM) planes to orchestrate training and inference while ensuring reliability, transparency, and accountability.
By outlining the key requirements of AI systems and demonstrating how DA-ITN fulfills them, this document presents a vision for the future of AI-native networking—an "AI internet"—optimized for continuous learning, scalable deployment, and seamless agent-to-agent collaboration.</t>
    </abstract>
  </front>
  <middle>
    <?line 58?>

<section anchor="introduction">
      <name>Introduction</name>
      <t>AI has become a major focus in recent years, with its influence rapidly expanding from everyday tasks like scheduling to complex areas such as healthcare. This growth is largely driven by advances in large language models (LLMs) like ChatGPT, Claude, Grok, and DeepSeek, which are now widely used for tasks such as brainstorming, editing, coding, and data analysis. These real-world applications highlight AI’s transformative power to boost productivity and simplify life. It’s clear that AI is not a passing trend but a lasting and evolving force.</t>
      <t>However, it is crucial to recognize that the success of AI systems relies on two fundamental pillars: training and inference. Both of these pillars have a number of factors and moving parts that need to be carefully coordinated, designed, and managed to ensure accuracy, resilience, usability, continuous evolution, trustworthiness, and reliability. Moreover, once deployed, AI systems must be continuously monitored and governed to safeguard user safety and societal well-being.</t>
      <t>As such, aspects such as data management, computational resources, connectivity, security, privacy, trust, billing, and rigorous testing are all crucial when handling AI systems. Thus, it is important to clearly understand the requirements of the AI systems from both the training and inference prospective as both of these pillars constitute an entangled framework and cannot be tackled in isolation.</t>
      <t>In this document, we present a vision of an ecosystem, especially designed to satisfy the requirements of AI from training and inference points of view. We propose a unified, intelligent network architecture—the Data and Agent aware-Inference and Training Network (DA-ITN). This ecosystem is envisioned as a comprehensive, multi-plane network with dedicated control, data, and operations &amp; management (OAM) planes. It is designed to interconnect all relevant stakeholders, including clients, AI service providers, data providers, and third-party facilitators. Its core objective is to provide the infrastructure and coordination necessary to support an ecosystem for enabling AI of the future at scale.</t>
      <t>This document aims to introduce the DA-ITN vision and establish a compelling case for its central role in enabling a new generation of AI-native networks, i.e., AI internet. These networks will be optimized not only for learning and inference but also for seamless collaboration, interaction, and communication among AI agents.
To that end, we begin by outlining the specific requirements of AI from both the training and inference standpoints. We then introduce the core components of the DA-ITN and illustrate how they collectively meet these requirements. Finally, this network is positioned as an ecosystem for agent-to-agent collaborations, interactions, and communications.</t>
    </section>
    <section anchor="training-requirements">
      <name>Training Requirements</name>
      <t>AI model training is the foundational process through which an artificial intelligence system learns to perform tasks by analyzing data and adjusting its internal parameters—typically the weights in neural networks—to minimize prediction errors. At its core, this process involves feeding input data into a model, and applying optimization algorithms to iteratively refine the model’s performance. Among the most influential outcomes of this process are foundation models, such as ChatGPT and its peers, which are capable of performing a wide range of tasks across domains. Training these models now occurs at an unprecedented scale, requiring massive compute infrastructure, enormous amounts of training data, high-speed interconnects, and parallelized training frameworks (e.g., data, model, and pipeline parallelism).</t>
      <section anchor="centralized-versus-decentralized-training">
        <name>Centralized versus Decentralized Training</name>
        <t>It is clear from the above that no matter how advanced the model architecture may be, the success of any training process ultimately hinges on two fundamental components: the model and the data. While the model itself is often developed and hosted in a centralized location—typically within the secure infrastructure of the model owner or designer—data is inherently distributed. It originates from sensors, devices, logs, events, documents, and other diverse sources spread across different geographies and domains. To be exact, whether due to geographic dispersion, organizational silos, privacy constraints, or edge-device generation, data rarely exists in a single, clean repository.</t>
        <t>Today, model training can happen in one of two ways or a combination thereof: centralized or decentralized. In centralized training, thanks to the development of robust data collection techniques and high-throughput connectivity networks, it is now feasible to collect data and bring it to where the model training would occur. This traditional approach is often referred to as model-centralized training. On the other hand, a more recent paradigm known as model-follow-data has emerged, advocating for the reverse: rather than transporting large volumes of potentially sensitive data to a central location, the model is dispatched to where the data resides—enabling distributed or federated training.</t>
        <t>Accordingly, to facilitate the training process, rendezvous points scheduling between distributed data, compute and storage resources, and an AI model awaiting training needs to be arranged and managed, which is fundamental for successful model training. However, this scheduling process introduces a number of challenges spanning privacy, trust, utility, and computational and connectivity resources management. Moreover, as AI adoption accelerates, both centralized and decentralized approaches will drive increasing pressure on underlying connectivity infrastructure. Therefore, to ensure scalable, efficient, and cost-effective AI training, it is vital to implement intelligent mechanisms for managing data and model movement, selecting relevant subsets for training, and minimizing unnecessary transfers.</t>
        <t>In the sections that follow, we explore the architectural and operational requirements needed to support this vision and lay the foundation for a high-performance, AI-native training ecosystem.</t>
      </section>
      <section anchor="requirements-breakdown">
        <name>Requirements Breakdown</name>
        <t>Consider a number of AI model training clients awaiting training service. An AI model training client is a user with a raw or a pre-trained model who wishes to train or continue training their AI model using data that can be found in the data corpus. The data corpus (the global dataset), as has been previously established, consists of a group of datasets that are distributed across various geographical locations. AI clients require access to this data either in a centralized or distributed manner.</t>
        <section anchor="data-collectionmodel-dispatching">
          <name>Data Collection/Model Dispatching</name>
          <t>As previously discussed, data is inherently distributed. In centralized training paradigms, this data must be transferred from its sources to centralized locations where model training occurs. Consider a scenario involving multiple clients, each awaiting centralized training of AI models using distinct data sets of interest. Aggregating large volumes of data from geographically dispersed sources to centralized servers introduces several significant challenges:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: The sheer volume of data to be transmitted can place substantial strain on the underlying transport networks, resulting in increased latency and bandwidth consumption.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Despite originating from different sources, data sets may carry overlapping or identical knowledge content. Transmitting such redundant content leads to unnecessary duplication, wasting resources without providing additional training value.</t>
            </li>
            <li>
              <t>Timely Delivery: In certain applications, the freshness of data is critical. Delays in transmission can degrade the value of the information, as these applications are sensitive to the Age of Information (AoI)—the time elapsed since data was last updated at the destination.</t>
            </li>
            <li>
              <t>Multi-Modal Data Handling: Data often exists in various formats—such as text, images, audio, video, etc—each with distinct transmission requirements. Ensuring accurate and reliable delivery of these diverse data types necessitates differentiated Quality of Service (QoS) levels tailored to the characteristics and sensitivity of each modality.</t>
            </li>
            <li>
              <t>Heterogeneous Access Media: Data may reside across diverse communication infrastructures—for example, some data may be accessible only via 3GPP mobile networks, while other data may be confined to wireline networks. Coordinating data collection across these heterogeneous domains, while maintaining synchronization and consistency, presents a significant operational challenge.</t>
            </li>
          </ul>
          <t>Importantly, many of these challenges are alleviated in decentralized training frameworks, where data remains local to its source and is not transferred over the network. Instead, the model itself is distributed to the various data locations. However, this alternate paradigm introduces its own set of unique challenges.</t>
          <t>As previously noted, modern AI models are growing increasingly large in size. In decentralized training, it is often necessary to replicate the model and transmit it to multiple, geographically dispersed data sites. This results in a different but equally significant set of logistical and technical hurdles:</t>
          <ul spacing="normal">
            <li>
              <t>Communication Overhead: While data transfer is avoided, dispatching large model files across the network to multiple destinations can still impose substantial load on communication infrastructure, particularly in bandwidth-constrained environments.</t>
            </li>
            <li>
              <t>Redundant Knowledge Transfer: Data residing at different locations may share overlapping knowledge content. Sending models to multiple sites with redundant knowledge content leads to inefficient use of network resources. In some cases, even when knowledge content is only partially redundant, it may be more efficient—considering communication cost—to forego marginal training benefits in favor of reduced overhead.</t>
            </li>
            <li>
              <t>Timeliness and Data Freshness: In certain applications, the Age of Information (AoI) remains critical. Prioritizing model dispatch to data sources with soon-to-expire or time-sensitive information is essential to maximize the utility of training and to maintain up-to-date model performance.</t>
            </li>
          </ul>
        </section>
        <section anchor="data-and-resource-discovery">
          <name>Data and Resource Discovery</name>
          <t>Given the distributed nature of data, there must be a mechanism through which data owners can advertise information about their datasets to AI model training clients. This requires the ability to describe the characteristics of the data—such as its knowledge content, quality, size, and Age of Information (AoI)—in a way that allows AI clients to discover and evaluate whether the data aligns with their training objectives. Training objectives can be one or more of: target performance, convergence time, training cost, etc.</t>
          <t>Crucially, this discovery process may need to operate across multiple network domains and heterogeneous communication infrastructures. For example, an AI training client operating over a wireline connection may be interested in data residing on a 3GPP mobile network. This raises an important question: How can data owners effectively advertise their datasets in a way that is discoverable across diverse domains?
To enable such cross-domain data visibility and discovery, the following key requirements must be considered:</t>
          <ul spacing="normal">
            <li>
              <t>Data Descriptors: These are metadata objects used by data owners to reveal essential information about their datasets to AI clients. Effective data descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by training clients. They should allow data owners to selectively disclose details about their data—such as type, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information (privacy preservation). Data descriptors also need to be easily modified as data can be dynamic, and the change in data needs to be effectively reflected into the data descriptions. To ensure interoperability, data descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>Data Discovery Mechanisms: These refer to the processes by which AI training clients locate and identify datasets across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant datasets regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility. It should also be highlighted that, discovery mechanisms should be considerably up-to-date with the changes that would occur as the underlying data changes dynamically.</t>
            </li>
            <li>
              <t>Data Relationship Maps: Training often requires identifying groups of datasets that collectively meet specific requirements. Evaluating each dataset in isolation may be insufficient. Instead, a mechanism is needed to establish relationships among datasets, enabling AI training clients to assemble the appropriate combination of data for their tasks. These relationships can be envisioned to look like maps or topologies. This is a crucial step as if an AI model client was not able to find the right dataset that satisfies its requirements, the client might choose not to submit the model for training at this time which may reduce resource wastage from the get go.</t>
            </li>
            <li>
              <t>Timely reporting: Given the dynamic nature of data availability, characteristics, and accessibility, it is essential to have advertisement mechanisms that can promptly reflect any changes. Real-time or near-real-time updates ensure that the AI training process remains aligned with the most current data conditions, thereby maximizing both effectiveness and accuracy. Timely reporting helps prevent training on outdated or irrelevant data and supports optimal decision-making in model selection and training pipeline configuration.</t>
            </li>
          </ul>
          <t>Additionally, it should be highlighted that in AI training, discovering data alone is not enough. For instance, third-party resources like compute and storage are essential, and the providers of those resources must be able to advertise their capabilities so AI clients can locate and utilize them effectively. Just like with data, resource discovery requires descriptors, multi-domain accessibility, and timely updates to support seamless coordination between models, data, and infrastructure.
It should be highlighted that data and resource discovery is essential in both centralized and decentralized training, as both can be done on third party infrastructure.</t>
        </section>
        <section anchor="mobility-and-service-continuity-handling">
          <name>Mobility and Service Continuity Handling</name>
          <t>In some decentralized training applications, AI models are designed to traverse a predefined route, training on multiple datasets in a sequential or federated manner. This introduces the need to manage model mobility. However, the underlying data landscape is often dynamic—new data is continuously generated, existing data may be deleted, or datasets may be relocated to different nodes or domains.</t>
          <t>As a result, enabling reliable model mobility in such a fluid environment requires robust mobility management mechanisms. For instance, while a model is en-route to a specific data location for training, that dataset may be moved elsewhere. In such cases, the model must either be re-routed to the new location or redirected to an alternative dataset that satisfies similar training objectives.</t>
          <t>Additionally, since training occurs on remote compute infrastructure and can be time-intensive, unexpected resource shutdowns or failures may interrupt the process. These interruptions can lead to service discontinuity, which must be addressed through mechanisms such as checkpointing, fallback resource selection, or dynamic rerouting of model or data to maintain training progress and system reliability.</t>
          <t>Additionally, model mobility may involve training on datasets that are distributed across heterogeneous communication infrastructures. Some infrastructures, such as emerging 6G networks, offer built-in mobility support—for example, when data resides on mobile user equipment (UE), its location can be tracked using native features of the network. However, such mobility handling capabilities may not exist in other infrastructures, such as traditional wireline networks or legacy systems, making seamless model movement and data access more challenging in those environments.</t>
        </section>
        <section anchor="privacy-trust-and-data-ownership-and-utility">
          <name>Privacy, Trust, and Data Ownership and Utility</name>
          <t>Privacy and trust are mutual responsibilities—both data owners and model owners must be protected. Granting clients access to data for training and knowledge building should be a regulated process, with mechanisms to track data ownership and future use. Initial discussions on this topic have taken place in forums such as the AI-Control Working Group.</t>
          <t>Equally important is ensuring that model owners are protected from data poisoning. They must have confidence that the datasets they use are accurately described and not misrepresented. If data owners provide false metadata—intentionally or otherwise—model owners may unknowingly train on unsuitable or harmful datasets, leading to degraded model performance. To safeguard both parties, innovative verification and enforcement mechanisms are needed. Technologies like blockchain could offer potential solutions for establishing trust and accountability, but further research and exploration are necessary to develop practical frameworks.</t>
        </section>
        <section anchor="testing-and-performance-management">
          <name>Testing and Performance Management</name>
          <t>Another critical aspect of training is testing and performance evaluation, typically carried out using a separate subset of the data known as the testing dataset. This dataset is not used to update the model’s weights but to assess its performance on unseen samples. In centralized training, this process is straightforward because all data resides in a single, accessible location, making it easy to partition the dataset into training and testing subsets. However, in distributed training environments, where data is spread across multiple locations or devices, creating a representative and unbiased testing dataset without aggregating the data centrally becomes a major challenge. Developing effective, privacy-preserving methods for testing in such settings requires innovative solutions</t>
        </section>
        <section anchor="qos-guarantee">
          <name>QoS Guarantee</name>
          <t>Beyond ensuring traditional Quality of Service (QoS) for data transmission, a new dimension of QoS must be considered—the QoS of training itself. In AI training workflows, it is crucial to guarantee that key performance indicators (KPIs) related to training, such as accuracy convergence, training time, and resource utilization, are met consistently. This raises several important questions:
* How can these training KPIs be guaranteed in dynamic or distributed environments?</t>
          <ul spacing="normal">
            <li>
              <t>What mechanisms can be used to monitor and track training performance in real time?</t>
            </li>
            <li>
              <t>Should AI training be treated like best-effort traffic, where no guarantees are made and resources are allocated as available?</t>
            </li>
            <li>
              <t>Or should training tasks receive prioritized or differentiated service levels, similar to high-priority traffic in traditional networks?</t>
            </li>
          </ul>
          <t>Addressing these questions is essential to ensure predictable and reliable AI model development, especially as training workloads grow in complexity and scale. It may require introducing new QoS frameworks tailored specifically to the needs of AI training systems.</t>
        </section>
        <section anchor="charging-and-billing">
          <name>Charging and Billing</name>
          <t>The AI training process involves a diverse ecosystem of stakeholders, including data owners, model owners, and resource providers. Each of these parties plays one or more vital roles in enabling successful training workflows.</t>
          <t>For example, communication providers contribute not only by transporting data and models across the network but also they themselves may also serve as data providers. This is particularly evident in the emerging design of 6G networks, which integrate sensing capabilities with communication infrastructure. As a result, 6G operators are uniquely positioned to offer both connectivity and data, making them central players in the training pipeline.</t>
          <t>Despite their different roles, all parties contribute to enabling AI training as a service, a complex and resource-intensive process that is far from free. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should training services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Will there be tiered service offerings, such as gold, silver, and platinum, each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to facilitating collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="inference">
      <name>Inference</name>
      <t>Inference is critical because it represents the phase where the model begins to deliver practical value. Unlike training, which is typically a one-time or periodic, resource-intensive process, inference often needs to operate continuously and efficiently, sometimes in real-time. Although inference is a less resource-intensive process, it has strict requirements that govern its success. In what follows, we explore these requirements that shall enable a successful AI inference ecosystem.</t>
      <section anchor="requirement-breakdown">
        <name>Requirement Breakdown</name>
        <t>We envision an inference ecosystem composed of a large number of pre-trained AI models (or agents) distributed across the globe. These models are capable of performing a wide range of tasks, such as image classification, language translation, or speech recognition. Some models may specialize in the same task but vary in performance, accuracy, latency, or resource demands. This diverse pool of models is accessed by numerous inference clients (users or applications) who submit inputs, referred to as queries, and receive task-specific outputs.</t>
        <t>These queries can vary greatly in complexity, structure, and modality, with some requiring the cooperation of multiple models to fulfill a single request. The overarching goal of the ecosystem is to efficiently match incoming queries with the most suitable models, ensuring accurate, timely, and resource-aware responses. Achieving this requires intelligent orchestration, load balancing, and potentially dynamic model selection based on factors such as performance, availability, cost, and user-specific requirements. In what follows, we discuss the various aspects of this ecosystem and discuss the different requirements needed for its success.</t>
        <section anchor="model-deployment-and-mobility">
          <name>Model Deployment and Mobility</name>
          <t>The first step toward building a successful AI inference ecosystem is the optimal deployment of trained models, or agents. In this context, optimality refers to both the physical or network location of the model and the manner in which it is deployed. AI models vary significantly in size and resource requirements—ranging from lightweight models that are only a few kilobytes to large-scale models with billions of parameters. This wide range makes deployment decisions critical to achieving both efficient performance and effective resource utilization. Also, a unique factor to AI models/agents is the fact that they are software components that are not bounded to a certain hardware. They can be deleted, copied, moved, or split across multiple compute locations. All these unique aspects provide flexibility in design if the real-time status of the underlying network dynamics and resources is made accessible.</t>
          <ul spacing="normal">
            <li>
              <t>Choosing the right facility to host a model: whether it's a lightweight edge device, a local server, or a high-performance cloud data center, deployment will depend on the model's size, computational requirements, and expected query volume. For example, smaller models might be best suited for deployment on edge devices closer to users, enabling low-latency responses. In contrast, larger models may require centralized or specialized infrastructure with high compute and memory capacity.</t>
            </li>
            <li>
              <t>Load balancing: Once models are deployed, inference traffic begins to flow, with users or applications sending queries to the appropriate agents. If not managed properly, this traffic can lead to congestion, creating bottlenecks that degrade inference performance through increased latency or dropped requests. To avoid such scenarios, models should be deployed strategically to distribute the load, ensuring smooth operation. Traditional load balancing techniques can be employed to redirect traffic away from overburdened nodes and towards underutilized ones. However, more sophisticated strategies may involve replicating models and placing these replicas closer to regions with high query demand, thereby minimizing latency and easing network traffic engineering challenges.</t>
            </li>
            <li>
              <t>Mobility-aware deployment: the dynamic nature of inference traffic necessitates mobility-aware deployment. For instance, consider a large data center acting as a centralized inference hub, hosting numerous models and handling a significant volume of queries. Over time, this hub may experience traffic overload. In such cases, migrating certain models to alternative locations can help alleviate pressure. However, model migration is not without its challenges—particularly if a model is actively serving queries at the time of migration. In such situations, mobility handling mechanisms must be in place to ensure seamless service continuity. These mechanisms could involve session handovers, temporary state preservation, or model version synchronization, all designed to maintain uninterrupted service during the migration process.</t>
            </li>
          </ul>
          <t>In summary, optimal model deployment requires careful consideration of model size, resource needs, query distribution, and real-time adaptability. Achieving this lays the foundation for a responsive, scalable, and resilient AI inference ecosystem.</t>
        </section>
        <section anchor="model-discovery-and-description">
          <name>Model Discovery and Description</name>
          <t>Just as data descriptors and discovery mechanisms are essential during the training phase, AI model inference clients also require a robust discovery mechanism during the inference stage. In an ecosystem populated by a large and diverse pool of models—each with unique capabilities and specializations—clients are presented with significant flexibility and choice in selecting the most suitable models for their queries. However, to make informed decisions, clients must have access to information that enables them to distinguish between models based on criteria such as performance, specialization, availability, and resource requirements.</t>
          <t>This discovery process becomes even more complex when it needs to function across multiple network domains and heterogeneous communication infrastructures. For instance, a client connected via a wireline network might need to interact with a model deployed on a mobile 3GPP network. Such scenarios raise a critical question: How can model owners advertise their models in a way that ensures discoverability and interoperability across diverse domains?</t>
          <t>Addressing this challenge requires the development of standardized model advertisement and discovery protocols that can operate seamlessly across infrastructure boundaries. These protocols must accommodate differences in network technology, latency constraints, and security requirements while providing consistent and reliable access to model information. Ensuring cross-domain discoverability is crucial to unlocking the full potential of a globally distributed inference ecosystem.</t>
          <t>To enable such cross-domain model visibility and discovery, the following key requirements must be considered:</t>
          <ul spacing="normal">
            <li>
              <t>Model Descriptors: These are metadata objects used by model owners to reveal essential aspects about their datasets to AI inference clients. Effective data descriptors must be self-contained, privacy-preserving, and informative enough to support decision-making by inference clients. They should allow model owners to selectively disclose details about their model—such as skills, performance reviews, trust level, relevance, quality metrics, freshness, and perhaps cost of utility—while concealing sensitive or proprietary information. To ensure interoperability, model descriptors can either follow a standardized format or adopt a flexible but well-defined structure that enables consistent interpretation across different systems and domains.</t>
            </li>
            <li>
              <t>Model/agent Discovery Mechanisms: These refer to the processes by which AI inference clients locate and identify models/agents across potentially vast and heterogeneous environments. An effective discovery mechanism should support global-scale searchability and cross-domain operability, allowing clients to find relevant model/agents regardless of where they reside or which communication infrastructure they are accessible through. Discovery protocols may be standardized within specific domains (e.g., mobile networks, IoT platforms) or designed to function interoperable across multiple domains, enabling seamless integration and visibility.</t>
            </li>
            <li>
              <t>Model/agent relationship maps: As queries may requiring the collaboration between multiple models/agents, relationships between models/agents with respect to different task might present useful tools as to help clients choose the appropriate subset of models/agents that would handle their queries.</t>
            </li>
            <li>
              <t>Timely Reporting: Similar to data, the status of a model can change over time—for example, due to shifts in workload or resource availability. It is important that such changes are reported promptly and accurately, allowing clients to make informed decisions based on the model’s current state. This is essential for ensuring efficient model selection and maintaining high-quality, reliable inference outcomes.</t>
            </li>
          </ul>
          <t>It is important to emphasize that model discovery differs fundamentally from data discovery. While data are passive objects that require external querying or manipulation, models are intelligent, autonomous agents capable of making decisions based on their own capabilities, status, and context. This distinction opens up new and more dynamic possibilities for how models are discovered and engaged in an inference ecosystem.</t>
          <t>In traditional data discovery, clients search for and retrieve relevant datasets based on metadata or predefined criteria. However, in the case of model discovery, the process can be much more interactive and flexible. One approach involves the client actively discovering models by querying a directory or registry using model descriptors. Based on these descriptors, the client selects one or more models to handle a specific inference task. However, given that models can reason and act independently, model discovery does not have to be limited to client-driven selection. An alternative approach is to reverse the flow of interaction. Instead of clients seeking out models, they can publish their tasks to a shared task pool, accessible to all available models. These tasks include descriptors that define the type of work to be done, expected outputs, and quality-of-service requirements. Models can then autonomously scan this pool, evaluate whether they are well-suited for specific tasks, and choose to express interest in executing them. This self-selection process allows models to play an active role in task matching, improving system scalability and efficiency.</t>
          <t>The final assignment of a task can be handled in different ways. Clients may retain full control and approve or reject interested models based on their preferences or priorities. Alternatively, the system may operate in a fully autonomous mode, where tasks are assigned automatically to the first or best-matching model, without requiring client intervention—depending on the client's chosen policy.</t>
          <t>This agent-driven paradigm reflects the shift toward more decentralized and intelligent AI ecosystems, where models are not merely passive computation endpoints but active participants in task negotiation and resource allocation. Such a system not only enhances scalability and flexibility but also allows for more efficient utilization of the available model pool, especially in heterogeneous and dynamic environments.</t>
        </section>
        <section anchor="query-and-inference-result-routing">
          <name>Query and Inference Result Routing</name>
          <t>A significant challenge in AI inference networks lies in efficiently routing client queries to the appropriate inference models and ensuring the corresponding results are reliably delivered back to the client. This becomes particularly complex in scenarios involving mobility and multi-domain environments, where both the client and the model may exist across different types of network infrastructures. The key challenges and considerations include:</t>
          <ul spacing="normal">
            <li>
              <t>Query Routing Across Heterogeneous Networks: When a client accesses the inference ecosystem through a mobile network such as 3GPP 6G, and the target model is hosted in a wireline or cloud-based infrastructure, routing the query across these distinct domains is non-trivial. Differences in network architecture, protocols, and service guarantees complicate the end-to-end flow.</t>
            </li>
            <li>
              <t>Mobility Management During Inference Execution: While mobile networks like 6G are designed to handle user mobility, inference tasks may take time to process—particularly when using large models or performing complex computations. During this time, the client may change physical location, switch devices, or even go offline. Ensuring that inference results can still reach the client under these dynamic conditions poses a significant challenge.</t>
            </li>
            <li>
              <t>Handling Client State Changes: If a client becomes idle or disconnects entirely during inference, the system must decide what to do with the completed result. Should it be queued, buffered, forwarded to another linked device, or simply discarded? A robust mechanism is needed to track client state, maintain context, and guarantee result delivery or at least graceful degradation.</t>
            </li>
            <li>
              <t>Support for Live and Streaming Inference: Some use cases, such as real-time audio transcription, involve live streaming of data from the client to the model and vice versa. These sessions require sustained, low-latency connections and are particularly sensitive to interruptions caused by mobility or handoffs between networks. Ensuring session continuity and maintaining streaming quality across network boundaries is a complex but critical aspect of real-world inference deployments.</t>
            </li>
            <li>
              <t>Cross-Domain Connectivity and Session Management: The involvement of multiple network operators and domains introduces questions around interoperability, session tracking, and handover coordination. There is a need for intelligent infrastructure capable of end-to-end session management, including maintaining metadata, context, and service quality as the session traverses’ different networks.</t>
            </li>
          </ul>
        </section>
        <section anchor="inference-chainingcollaborative-inference">
          <name>Inference Chaining/Collaborative Inference</name>
          <t>Another critical aspect of an AI inference ecosystem is the need for model collaboration to fulfill complex or multi-faceted tasks. Not all inference requests can be handled by a single model; in many cases, collaboration between multiple models is necessary. Effectively managing this task-based collaboration is essential to ensure accurate, efficient, and scalable inference services. Model collaboration can take several distinct forms:</t>
          <ul spacing="normal">
            <li>
              <t>Inference Chaining: In this model, the output of one model serves as the input to the next in a sequential pipeline. Each model performs a specific stage of the task, and the final result—produced by the last model in the chain—is returned to the client. This is common in multi-stage tasks such as image processing followed by object detection and then classification.</t>
            </li>
            <li>
              <t>Parallel Inference: Here, a complex task is decomposed into multiple subtasks, each of which is assigned to a specialized model. These models operate concurrently, and their outputs are aggregated to form a unified inference result. This approach is particularly useful when dealing with large data sets or when a task spans different domains of expertise.</t>
            </li>
            <li>
              <t>Hierarchical inference: A model is assigned as a task manager and is responsible for delegating tasks to service models</t>
            </li>
            <li>
              <t>Collaborative Inference: In this more dynamic and decentralized form, the task is assigned to a group of models that are capable of discovering one another, assessing their respective capabilities, and coordinating among themselves to devise a shared strategy for completing the task. This model requires more sophisticated communication, negotiation, and orchestration mechanisms.</t>
            </li>
          </ul>
          <t>Regardless of the collaboration format, the success of such multi-model interactions depends on the availability of a robust management infrastructure. This infrastructure must enable seamless coordination between models, even when:</t>
          <ul spacing="normal">
            <li>
              <t>The models are hosted by different providers,</t>
            </li>
            <li>
              <t>They are deployed across heterogeneous communication networks,</t>
            </li>
            <li>
              <t>They use varying protocols, or</t>
            </li>
            <li>
              <t>They have differing performance characteristics.</t>
            </li>
          </ul>
          <t>Such a management system must abstract away the underlying complexities and provide standardized interfaces, discovery mechanisms, communication protocols, and coordination frameworks that allow models to interact effectively. Without this, collaborative inference would be brittle, inefficient, or impossible to scale. In essence, the ability to orchestrate model collaboration across diverse environments is a cornerstone of a flexible, intelligent, and robust AI inference ecosystem.</t>
        </section>
        <section anchor="compute-and-resource-management">
          <name>Compute and Resource Management</name>
          <t>In many scenarios, the compute infrastructure used to host and run inference models is managed by third-party providers, not the model owners themselves. These compute providers are responsible for meeting the Quality of Service (QoS) levels agreed upon with the model owners—such as latency, uptime, throughput, and reliability.</t>
          <ul spacing="normal">
            <li>
              <t>Ensuring these service levels are consistently met raises the question of accountability. If performance degrades due to compute resource issues—such as overloaded hardware or network outages—who is responsible for the failed inference tasks?</t>
            </li>
            <li>
              <t>There must be clear, enforceable service-level agreements (SLAs) that define roles, responsibilities, and penalties for non-compliance.</t>
            </li>
            <li>
              <t>Mechanisms for performance monitoring, auditing, and dispute resolution need to be integrated into the ecosystem to make such arrangements viable and trustworthy.</t>
            </li>
          </ul>
        </section>
        <section anchor="privacy-preservation-and-security">
          <name>Privacy Preservation and Security</name>
          <t>While models are the intellectual property of their owners, they may operate on infrastructure owned by others. This raises significant concerns around privacy and intellectual property protection.</t>
          <ul spacing="normal">
            <li>
              <t>Sensitive model details such as architecture, weights, and optimization strategies must be protected from exposure or reverse engineering by untrusted compute hosts.</t>
            </li>
            <li>
              <t>Techniques such as secure computing, encrypted model execution, and remote attestation protocols may be necessary to ensure that models run securely without revealing proprietary details.</t>
            </li>
            <li>
              <t>Model owners must also be assured that inference inputs and outputs remain confidential, particularly in applications involving personal or sensitive data.</t>
            </li>
          </ul>
        </section>
        <section anchor="utility-handling-and-qos-requirements">
          <name>Utility Handling and QoS Requirements</name>
          <t>Utility handling refers to the regulation, protection, and fair governance of how models are used, accessed, and monitored throughout the ecosystem. This encompasses several critical questions:</t>
          <ul spacing="normal">
            <li>
              <t>How can we guarantee that a model deployed on remote infrastructure is not being tampered with, copied, or intentionally repurposed?</t>
            </li>
            <li>
              <t>How do we ensure that workload distribution is fair across available models, preventing monopolization by a few and giving equal visibility and opportunity to all participating models?</t>
            </li>
            <li>
              <t>What protections are in place to ensure that models are not being poisoned, exploited, or involved in illegal activities, either through malicious inputs or untrusted outputs?</t>
            </li>
            <li>
              <t>How do we ensure the integrity of inference results, so that outputs are delivered to clients without alteration, manipulation, or censorship?
Addressing these concerns may require digital rights management (DRM) for AI models, usage monitoring tools, and potentially blockchain-based logging or audit trails to ensure transparency and traceability.</t>
            </li>
          </ul>
          <t>On the other hand, the definition of Quality of Service (QoS), when it comes to inference tasks, is very broad and can take many forms. For instance, QoS could be to guarantee a certain accuracy of a response, or time of the response, or expertise level needed. We believe that the topic of QoS guarantee requires extensive studying and analysis.</t>
        </section>
        <section anchor="model-upgrade-streamlining">
          <name>Model Upgrade Streamlining</name>
          <t>AI models are not static; they undergo continuous upgrades, improvements, and fine-tuning to maintain accuracy, adapt to new data, or support evolving tasks.</t>
          <ul spacing="normal">
            <li>
              <t>The ecosystem must support seamless model versioning, including adding, removing, or modifying model agents without disrupting ongoing services.</t>
            </li>
            <li>
              <t>Updated model profiles must be instantly reflected in the discovery layer, ensuring clients always have access to the most current and accurate model descriptions.</t>
            </li>
            <li>
              <t>For large models, upgrade procedures must be efficient and bandwidth-conscious, potentially using incremental update techniques to avoid full redeployment.</t>
            </li>
            <li>
              <t>Moreover, strategies must be in place to handle hot-swapping of models, where an old model is gracefully decommissioned and replaced by a new one—without causing inference failures or data loss during the transition.</t>
            </li>
          </ul>
        </section>
        <section anchor="charging-and-billing-1">
          <name>Charging and Billing</name>
          <t>The AI inference process involves a diverse ecosystem of stakeholders, including model owners, compute providers, and communication providers. Each of these parties plays one or more vital roles in enabling successful inference workflows. Therefore, it is essential to establish a robust charging and billing framework that ensures each participant is fairly compensated based on their contribution.</t>
          <t>Several open questions arise in this context:</t>
          <ul spacing="normal">
            <li>
              <t>Should inference services follow a prepaid model, or adopt a pay-per-use structure?</t>
            </li>
            <li>
              <t>Will there be tiered service offerings—such as gold, silver, and platinum—each providing different levels of performance guarantees or priority access?</t>
            </li>
            <li>
              <t>How should these tiers be defined and enforced in terms of service quality, resource allocation, and response time?</t>
            </li>
            <li>
              <t>What about discovery framework providers? Would they be offering a free service like google search or would it be more structured?</t>
            </li>
          </ul>
          <t>Developing fair, transparent, and scalable billing mechanisms is critical to fostering collaboration across stakeholders and sustaining the economic viability of distributed AI training ecosystems. These challenges call for further research into incentive structures, dynamic pricing models, and smart contract-based enforcement, especially in scenarios involving cross-organizational or cross-network cooperation.</t>
        </section>
      </section>
    </section>
    <section anchor="data-and-agent-aware-inference-and-training-network-da-itn-general-framework">
      <name>Data and Agent Aware Inference and Training Network (DA-ITN): General Framework</name>
      <t>The DA-ITN is envisioned as a multi-domain, multi-technology network operating at the AI layer, designed to address the various layers of complexity inherent in modern AI ecosystems. As mentioned earlier, the network aims to support a wide range of requirements, some of which are outlined above, across AI training, inference, and agent-to-agent interaction.</t>
      <t>The network consists of set of nodes and equipment connected via one or more traditional underlay networks as depicted below.</t>
      <figure anchor="fig1">
        <name>Figure 1: DA-ITN nodal view</name>
        <artwork align="center"><![CDATA[
+---------------------------------------------+
| DA-ITN nodal view                           |
|                                             |
|  +----------------+     +----------------+  |    DA-ITN node types
|  | DA-ITN Node (A)|<--->| DA-ITN Node (B)|  |      A- Data node
|  +----------------+  |  +----------------+  |      B- Compute node
|                      |                      |      C- Storage node
|                      |                      |      D- Model node
|  +----------------+  |  +----------------+  |      E- Evaluation node
|  | DA-ITN Node (E)|<--->| DA-ITN Node (G)|  |      F- Agent node
|  +----------------+  |  +----------------+  |      G- Multi-purpose node
|                      |                      |
|                      |                      |
|  +----------------+  |  +----------------+  |
|  | DA-ITN Node (F)|<--->|DA-ITN Node(C+D)|  |
|  +----------------+     +----------------+  |
|                                             |
+---------------------------------------------+
]]></artwork>
      </figure>
      <t>Nodes with DA-ITN along with its core functionality interact together to provide different training, inference, and agentic services. In this manner, DA-ITN can be divided into four interacting major building blocks as shown bellow.</t>
      <figure anchor="fig2">
        <name>Figure 2: DA-ITN high level architecture and building blocks</name>
        <artwork align="center"><![CDATA[
+--------------------+         +--------------------+ 
|   DA-ITN Service   |         |   DA-ITN Client    |
| Provider Community |         |     Community      |
+--------------------+         +--------------------+
    ↑     ↑                               ↑     ↑
    |     |                               |     |
    |     |                               |     |
    |     +-------------------------------+     |
    |                     |                     |
    |                     |                     |
    |                     ↓                     |
    |           +--------------------+          |
    |           |     DA-ITN Core    |          |
    |           |                    |          |
    |           +--------------------+          |
    |                     ↑                     |
    |                     |                     |
    |                     |                     |
    ↓                     ↓                     ↓
+---------------------------------------------------+
|                    DA-ITN Enablers                |
+---------------------------------------------------+
]]></artwork>
      </figure>
      <section anchor="da-itn-core">
        <name>DA-ITN Core</name>
        <t>This block contains DA-ITN main internal modules, functions, and services. Dedicated logical
planes in this block handle interactions between its different modules and functions. Interactions between different modules and functions in this block are not visible or accessible to entities in other blocks. DA-ITN core offers its services to external entities via clear and well defined interfaces and protocols. The following illustrates different modules and functions of DA-ITN core block.</t>
        <figure anchor="fig3">
          <name>Figure 3: DA-ITN core and its different modules and function</name>
          <artwork align="center"><![CDATA[
+-----------------------------------+
|            DA-ITN Core            |
|                                   |
|   +----------+ +--------------+   |
|   | X-RCE    | |Registration &|   |     X-RCE:  Training, model, query, etc.
|   |          | |Authentication|   |             route compute engine
|   +----------+ +--------------+   |     XOD:    Model, agent deployment  
|   +----------+ +--------------+   |             optimizer
|   | X-DO     | |Discovery &   |   |     S-FAM:  Different Service feasibility
|   |          | |Advertisement |   |             assessment module
|   +----------+ +--------------+   |     TAG:    Training algorithm generator
|   +----------+ +--------------+   |     PVM:    Performance verification
|   | S-FAM    | |Billing &     |   |             Module 
|   |          | |Accounting    |   |     DDRT:   Data dynamics and resource
|   +----------+ +--------------+   |             topology
|   +----------+ +--------------+   |
|   | TAG      | |Reputation &  |   |
|   |          | |Trust Mgmt.   |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   | PVM      | | Upgrade Mgmt.|   |
|   |          | |              |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   | Resource | |Mobility Mgmt.|   |
|   | Mgmt.    | |              |   |
|   +----------+ +--------------+   |
|   +----------+ +--------------+   |
|   |   DDRT   | | Tools Mgmt.  |   |
|   |          | |     ???      |   |
|   +----------+ +--------------+   |
|            +---------+            |
|            |   OAM   |            |
|            +---------+            |
+-----------------------------------+
]]></artwork>
        </figure>
      </section>
      <section anchor="da-itn-service-provider-community">
        <name>DA-ITN Service Provider Community</name>
        <t>Providers for different services such as data, model, agent, and resource providers reside within the Service Provider Community block of the DA-ITN. Service providers join the network via a registration and authentication process offered by DA-ITN core. The service providers use DA-ITN to advertise their services, capabilities, etc. across the overall network. They can also register for notifications to get updates e.g. arrival of new models, training data, agents, etc. DA-ITN dispenses revenue to providers for the services rendered via its billing and accounting module.</t>
        <t>The following figure shows different modules of DA-ITN service provider community.</t>
        <figure anchor="fig4">
          <name>Figure 4: DA-ITN Service Provider Community</name>
          <artwork align="center"><![CDATA[
+-------------------------------+
|       DA-ITN Service          |
|     Provider Community        |
|                               |
|  +----------+ +----------+    |
|  | Data     | | Model    |    |
|  | providers| | providers|    |
|  +----------+ +----------+    |
|  +----------+ +----------+    |
|  | Agent    | | Resource |    |
|  | providers| | providers|    |
|  +----------+ +----------+    |
|  +--------------+             |
|  | Tools        |             |
|  | providers ???|             |
|  +--------------+             |
+-------------------------------+
]]></artwork>
        </figure>
        <t>The tool module within the provider block requires further investigation and analysis. Agentic protocols such as Model Context Protocol(MCP) provide access to MCP tools from the agent interaction point of view. Whether DA-ITN needs to support additional capabilities w.r.t agents or whether it needs to support distinct tools w.r.t training and inference is an open question for now. Will there be a need for unified tools' protocols that fits all utilities, or a protocol per utility?</t>
      </section>
      <section anchor="da-itn-client-community">
        <name>DA-ITN Client Community</name>
        <t>This block represents the client side of DA-ITN. The clients are network participants requiring training, inference, agent-to-agent interactions, and those who need access to resources such as storage, compute, etc. offered by resource providers in DA-ITN.</t>
        <t>DA-ITN enables clients to discover potential providers by tuning into DA-ITN discovery, and advertisement module, allowing them to select the best match based on their requirements. Alternatively, clients may delegate the matching process to DA-ITN, requesting DA-ITN to identify the most suitable provider based on their criteria. For example, a client using the model training service may opt to fully control the training process and make all decisions independently. Alternatively, the client can delegate the training responsibilities to the DA-ITN core. In the case of delegation, modules such as X-RCE, DDRT, PVM, S-FAM, and TAG can work collaboratively to train the model on the client’s behalf and deliver the finalized, trained model back to them.</t>
        <figure anchor="fig5">
          <name>Figure 5: DA-ITN Client Community</name>
          <artwork align="center"><![CDATA[
+-------------------------------+
|       DA-ITN Client           |
|         Community             |
|                               |
|  +----------+ +----------+    |
|  | Data     | | Model    |    |
|  | clients  | | clients  |    |
|  +----------+ +----------+    |
|  +----------+ +----------+    |
|  | Agent    | | Resource |    |
|  | clients  | | clients  |    |
|  +----------+ +----------+    |
|  +--------------+             |
|  | Tools        |             |
|  | Clients   ???|             |
|  +--------------+             |
+-------------------------------+
]]></artwork>
        </figure>
        <t>It must be noted that a node/entity in DA-ITN can act both as provider and/or a client. For example, a node providing data as its service, might need access to a resource provider service. Or a model provider enabling inference might employ the services of data providers for Retrieval-Augmented Generation (RAG).</t>
        <t>Similar to the provider community block in DA-ITN, the tools module withing the client community requires further study.</t>
      </section>
      <section anchor="da-itn-enablers">
        <name>DA-ITN Enablers</name>
        <t>This layer represents external and underlying services that DA-ITN itself employs to accomplish its different tasks. Various networking layers, access technologies, location, and sensing functions are examples of such services.</t>
        <figure anchor="fig6">
          <name>Figure 6: DA-ITN Enablers</name>
          <artwork align="center"><![CDATA[
+-------------------------------------------------------------------------+
|                             DA-ITN Enablers                             |
|                                                                         |
|  +---------------------------+  +-----------+  +-----------+            |
|  | Communications/Networking |  | Location  |  |  Sensing  |            |
|  |                           |  |           |  |           |            |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | | Mobile  |  | Internet | |  | | GPS   | |  | | IoT   | |            |
|  | | network |  +----------+ |  | +-------+ |  | +-------+ |            |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | | NTN     |  | WiFi     | |  | |Sensors| |  | | ISAC  | |  Others??? |
|  | +---------+  +----------+ |  | +-------+ |  | +-------+ |            |
|  | +-----------------------+ |  | +-------+ |  | +-------+ |            |
|  | |        Others?        | |  | |Mobile | |  | |Others?| |            |
|  | +-----------------------+ |  | |network| |  | +-------+ |            |
|  |                           |  | +-------+ |  |           |            |
|  |                           |  | +-------+ |  |           |            |
|  |                           |  | |Others?| |  |           |            |
|  |                           |  | +-------+ |  |           |            |
|  +---------------------------+  +-----------+  +-----------+            |
+-------------------------------------------------------------------------+
]]></artwork>
        </figure>
      </section>
    </section>
    <section anchor="da-itn-high-level-architecture">
      <name>DA-ITN high level architecture</name>
      <t>To manage these complexities and cater for the requirements, we propose structuring the DA-ITN around four core components: a Control Plane (CP), a Data Plane (DP), an Operations and Management (OAM) Plane, and an Intelligence Layer. It is important to note that the DA-ITN is agnostic to the underlying communication infrastructure, allowing it to operate seamlessly over heterogeneous networks, whether mobile, wire-line, or satellite-based. he DA-ITN integrates with these underlying infrastructures through any available means, embedding its control and intelligence capabilities to coordinate and manage AI-specific services in a flexible and scalable manner.</t>
      <section anchor="control-plane-and-intelligence-layer">
        <name>Control plane and Intelligence Layer</name>
        <t>The Control Plane and Intelligence Layer work together to enable an efficient, reliable, and timely information collection infrastructure. They continuously gather up-to-date information on data availability, model status, agent conditions, resource utilization, and reachability across all participating entities. The collected information comes in the form of dynamic descriptors for data, models, and resources, essential components for enabling intelligent, context-aware decision-making within the AI ecosystem as has previously been highlighted. Also, with the help of data, resource, and reachability topology engine (DRRT) housed within the intelligence layer, the gathered information and descriptors can be used to construct meaningful relationships across the ecosystem. These are captured in the form of dynamic topologies or map-like structures, which help optimize decision-making processes across training, inference, and agent-to-agent collaboration tasks. This design provides a continuous awareness that is very essential for the success, reliability, accuracy, and responsiveness of the AI functionalities and services enabled by the DA-ITN within the AI ecosystem.</t>
        <t>The DA-ITN control plane also lays a foundation for an advanced discovery infrastructure where the generated descriptors can be made easily accessible to all authorized participants to facilitate their required AI service For example, AI clients subscribed to training services can access up-to-date data descriptors and resource topologies, enabling them to select appropriate datasets and compute resources that align with their performance and accuracy goals. Similarly, inference clients or agents seeking collaboration can discover models based on capabilities, or submit task descriptors that enable models to respond intelligently and autonomously.</t>
        <t>Aside from descriptor collection, topology creation, and discovery, the DA-ITN control plane also supports a secure and trusted environment where clients, data providers, model providers, and resource providers can engage in AI processes without compromising integrity or accountability. It also plays a key role in managing charging, billing, and rights enforcement, ensuring that all contributors to the AI service chain are fairly compensated and protected.</t>
        <t>It is worth noting that the DA-ITN’s Control Plane is not constrained by specific protocol stacks. Instead, it provides a flexible connectivity and coordination infrastructure upon which various AI-related protocols—such as Agent-to-Agent (A2A), Model Control Protocol (MCP), or AI Coordination Protocol (ACP)—can operate. Regardless of the protocol used, implementations must meet the core DA-ITN requirements, including timely information exchange, flexible descriptor encapsulation, support for multi-model and multi-domain environments, and robust security and privacy protections. The DA-ITN is also designed to support both centralized and decentralized modes of operation, offering high adaptability across different deployment contexts.</t>
        <t>It’s also important to clarify that the Intelligence Layer encompasses all previously mentioned DA-ITN core functions, along with any additional intelligence required to support the full range of DA-ITN services. The term “Intelligence Layer” is intentionally broad to allow flexibility in its design and contents. Nonetheless, its role is clearly defined: it serves as a functional layer that interfaces with other DA-ITN components through the control plane, data plane, and OAM plane to fulfill its responsibilities.</t>
      </section>
      <section anchor="data-plane">
        <name>Data Plane</name>
        <t>On the other hand, the Data Plane of the DA-ITN provides support for mobility management and intelligent scheduling, enabling the dynamic creation of rendezvous points where data, queries, models, agents, and compute infrastructure can be brought together with minimal latency and overhead. Thanks to its infrastructure-agnostic nature, the DA-ITN leverages existing communication networks—such as those offered by 6G or edge service providers—as tools to enable model mobility, data mobility, and agent-to-agent coordination. This capability is essential for supporting scenarios where mobility or geographical dispersion of resources would otherwise lead to performance degradation or inefficiency.</t>
        <t>The construction of the Data Plane may fall under the responsibility of the DA-ITN core or Intelligence Layer, which would orchestrate the necessary resources from the DA-ITN Enabler block to build the required structure. Alternatively, the Enabler block itself may possess sufficient intelligence to autonomously construct the Data Plane as needed.</t>
      </section>
      <section anchor="operation-and-management-plane-oam">
        <name>Operation and Management Plane (OAM)</name>
        <t>Finally, the Operations and Management (OAM) layer plays a critical role in supporting the day-to-day operational needs of the AI ecosystem. This layer is responsible for a wide range of essential functions, including monitoring, registration, configuration, fault management, and lifecycle maintenance of models, data, and services. It serves as the management backbone of the DA-ITN, ensuring transparency, accountability, and operational control throughout the system.</t>
        <t>Consider the scenario of an AI model training client deploying a model into the ecosystem for training. Through the capabilities of the OAM layer, the client can continuously monitor the training performance of their model in real time—tracking key performance indicators such as convergence speed, loss metrics, resource usage, and network traversal. The model’s location within the ecosystem can be dynamically tracked, allowing clients to know exactly where their model resides or which data centers or devices it is interacting with.</t>
        <t>Moreover, the OAM layer enables interactive control. Clients can use it to adjust training parameters on the fly, such as learning rates, data sampling strategies, or the choice of collaborative partners. They can even pause, resume, or terminate the training process at will, giving them full agency over the lifecycle of their models. This flexibility is crucial in adaptive AI systems where responsiveness and real-time decision-making are valued.</t>
        <t>In this way, the OAM layer effectively functions as the control dashboard or command-line terminal of the DA-ITN-enabled AI ecosystem. Whether through a graphical user interface (GUI), APIs, or automated orchestration scripts, the OAM provides the necessary tools for fine-grained management, status visualization, and policy enforcement.</t>
        <t>Beyond individual model control, the OAM layer also facilitates system-wide coordination and policy administration. OAM in coordination with a potential policy enforcement module man help ensuring compliance with service-level agreements (SLAs), enforcing data governance policies, and managing access rights across domains. It plays a foundational role in building trustworthy, maintainable, and operationally efficient AI services across diverse infrastructure providers and stakeholders.</t>
      </section>
      <section anchor="summary-of-the-da-itn-general-framework">
        <name>Summary of the DA-ITN General Framework</name>
        <t>Accordingly, the DA-ITN is well positioned and designed to provide a range of intelligent services that can be leveraged by both AI clients and service providers. It forms the foundation for a scalable, decentralized AI internet, driving the emergence of a vibrant and cooperative agent-based ecosystem. By enabling the formation of adaptive and intelligence-driven topologies and being agnostic to the infrastructure, the DA-ITN facilitates more effective decisions in AI training, inference, and agent-to-agent interactions—ultimately supporting a more responsive, resilient, and capable AI infrastructure that can scale with future demands.</t>
        <t>In the following sections, we provide more detailed insights into the specific DA-ITN components that support training and inference services.</t>
      </section>
    </section>
    <section anchor="da-itn-for-training">
      <name>DA-ITN for Training</name>
      <t>The training architecture of the DA-ITN consists of five layers: i) the terminal layer (DA-ITN provider and client communities); ii) the network layer (Enablers); iii) the data, resource, and reachability topology layer (DRRT); iv) the DA-ITN intelligence layer (DA-ITN core); and v) the OAM layer. The layers interact together using control and data planes (CP and DP respectively) as is discussed in the following.</t>
      <t>First, the network layer, which is at the heart of the DA-ITN training system, is responsible for providing connectivity services to the four other layers. It provides both control and data plane connectivity to enable various services. The network layer connects to the terminal and DRRT layers via CP and DP links, and connects to the intelligence layer via a CP link only. The network layer also enables the overarching OMA layer by enabling a multi-layer connectivity structure.</t>
      <t>Second, the terminal layer from the point of view of training, is the lowest layer in the architecture, and it contains the terminal components of the system. These include nodes that host the training data, facilities that provide computing resources where the model can be trained, and newly proposed components that we refer to as the model performance verification modules (MPVMs), where the model testing phase takes place. It should be noted that facilities providing computing resources come in various forms including private property such as personal devices, in a distributed form such as in the case of mobile edge computing in 6G networks, on the cloud such as on the AWS cloud, or anywhere that is accessible by both the data and the model and holds sufficient compute for training. As for the MPVU, this unit is important when conducting distributed training as it takes the role of a trusted proxy node that holds a globally constructed testing dataset - the dataset is constructed via collecting sample datasets from each participating node - and provides safe and secure access to it. Last, the terminal layer also hosts the AI training clients.</t>
      <t>The terminal layer relies on the network layer to build an overarching knowledge-sharing network. To be exact, the network layer provides three main services to the terminal layer, namely: i) moving models and data between the identified rendezvous compute points where training can happen; ii) moving the models towards the MPVU units where performance evaluation can be conducted to keep track of the training progress; and iii) enabling AI training clients to submit their models, monitor the training progress, modify training requirements, and collect the trained models. Control and data traffic exist for each one of these services. For instance, moving a model toward a compute facility requires authorization for the utility of the resources; hence, authorization control data is required to be exchanged over the Terminal-NET CP links. The service also requires the physical transmission of the model to the computing facility which is handled over the Terminal-NET DP link. Similar situations can be extrapolated for the other provided services. It is worth noting that the network layer can be built on top of any access network technology including 3GPP cellular networks, WiFi, wireline, peer-to-peer, satellites, and non-terrestrial networks (NTN), or a combination of the above. These networks can be used to build dedicated CP and DP links strictly designed to enable the DA-ITN training system and its services.</t>
      <t>Third, the DRRT layer holds all the information required to make accurate decisions and sits between the intelligence layer and the terminal layer. It consists of a DRRT-manager (DRRT-M) unit which is the brain of this layer and interfaces with the other layers over CP links. The DRRT layer provides the intelligence layer with visibility and accessibility services to specific information about the underlying terminal layer's data, resource, and reachability status. To be exact, the DRRT layer holds information regarding the type, quality, amount, age, dynamics, and any other essential information about the data available for training. It also provides reachability information of the participating nodes to avoid unnecessary communication overhead and packet droppage.  Lastly, the DRRT also contains information about computing resources and MPVUs such as resource availability, location, trustworthiness, and nature of the testing datasets hosted at the different MPVF units.</t>
      <t>The DRRT relies on the network layer to collect the necessary information to build the Global-DRRT topology (G-DRRT). The G-DRRT is a none model specific topology, it is rather a large canvas that holds the high-level view of the data, resource, and reachability information. The DRRT-M unit in the DRRT layer communicates with the network layer over CP links to manage the collection process of the required information. For instance, the DRRT-M may instruct the 3GPP component of the network layer to convey connectivity information about the data nodes, or it might instruct it to wake up an ideal data provider device. It might also instruct satellites to share GPS locations of mobile data nodes. The collected data by the network layer are then shipped toward the G-DRRT component of the DRRT layer over DP links. The G-DRRT hosts intelligence that allows it to convert the collected information into useful global topology ready to provide services to the AI training clients.</t>
      <t>Fourth, The Intelligence Layer is responsible for hosting the decision-making logic required to fulfill the specific training requirements submitted by clients. It contains several key components that collaboratively determine how, where, and whether training should proceed. Among these is the Model Training Route Compute Engine (MTRCE), which identifies suitable rendezvous points between models and data. Another critical component is the Training Feasibility Assessment Module (T-FAM), which functions as an admission controller—evaluating whether a submitted model, given its requirements and constraints, can be effectively trained within the available ecosystem.</t>
      <t>Additional intelligent modules include the Training Algorithm Generator (TAG) and the Hyperparameter Optimizer (HPO). These components are responsible for selecting the appropriate training paradigm—such as reinforcement learning (RL), federated learning (FL), or supervised learning (SL)—as well as determining other configuration details like the number of training epochs, batch size, and optimization strategy. The Intelligence Layer also interfaces with both the Network Layer and the DRRT Layer to acquire the context needed for effective decision-making. From the Network Layer, it receives control data over CP links—this includes model structure, target accuracy, convergence time, monitoring instructions, and client-specified training preferences. It also receives feedback data that allows the TAG and HPO modules to refine their recommendations dynamically.</t>
      <t>Meanwhile, the Intelligence Layer connects to the DRRT Layer via both CP and DP links to access up-to-date visibility into training data, compute resources, and node reachability. This information is essential for components like MTRCE and T-FAM to make routing and admission decisions. To further enhance decision efficiency, the Intelligence Layer may also host a DRRT-Adaptability Unit (DRRT-A). This optional module works in coordination with MTRCE, T-FAM, and the DRRT Manager (DRRT-M) to generate model-specific DRR topologies—lightweight, targeted representations carved out from the global DRR topology. These customized topologies are optimized to reduce computational overhead and accelerate decision-making for individual training requests.</t>
      <t>Last, the OAM layer, which spans all the layers, is mainly intended as a management layer to configure the training components, the connectivity of the network layer, and enable feedback functions essential for progress monitoring and model localization and tracking. It is also intended to provide feedback to the clients about their submitted models every step of the way.</t>
    </section>
    <section anchor="da-itn-for-inference">
      <name>DA-ITN for Inference</name>
      <t>The Inference architecture of the DA-ITN provides automated AI inference services using a similar structure to the training architecture with a few differences.</t>
      <t>First, unlike training, where the moving components are models and training data, and the rendezvous points are computing facilities, in inference, models/agents and queries/tasks are the moving components that require networking, and the rendezvous points are model hosting facilities.</t>
      <t>Second, in inference, the clients are both the task/query owners as well as the model/agent owners. Query owners are the inference service users who send their queries into the system and collect the resulting inference. On the other hand, model owners are divided into two types. The first type consists of model hosts - the model used for inference does not have to be owned by them, but it is hosted on their computing facilities.  The second type consists of model/agent providers - they develop models/agents and deploy them either at their own facilities or at model hosts. Model owners are represented in the terminal layer as model deployment facility providers (MDFP) which are distributed across the global network.</t>
      <t>Third,  the network layer provides the following services to the terminal layer using its control and data planes: i) model mobility from model generators to model hosts; ii) query routing towards models deployed on MDFPs; iii) model mobility from one location to the other in case of load balancing situations; iv) model mobility towards re-training and calibration facilities which may be hosted on MVPF units; v) query response and inference result routing towards the query owners or any indicated destination around the globe; and vi) feedback and monitoring information to model and query owners.</t>
      <t>Fourth, the DRRT layer is replaced by a query, resource, and reachability topology (QRRT) layer. It provides the same type of services to the other layers; however, from the point of view of queries and models. That is, it provides information about both models and queries such as i) for models: model locations, model capabilities, current loading conditions, inference speed, inference accuracy, model reachability and accessibility (i.e., reachability and accessibility of the MDFP), and ii) for query: query patterns and dynamics (could be associated with a geographical location), query types, and reachability status of query owners for response communication purposes. The information collected by the QRRT is used to make appropriate decisions about model deployment and distribution strategies, query-to-model routing decisions, and response routing decisions. The QRRT has a management function that coordinates with the Network layer to collect the required information from the terminal layer to build the Global-QRRT (G-QRRT). It also optionally communicates with the QRRT-adaptation (QRRT-A) function in the inference intelligence layer to build query- or model-specific QRRTs.</t>
      <t>Last, the inference intelligence layer hosts different intelligent decision-making components including the Query Feasibility Assessment Module (Q-FAM), the Query Inference Route Compute Engine (QIRCE), and the Model Deployment Optimizer module (MDO). Just like with the training, these components make decisions based on the QRRT. For instance, the Q-FAM hosts intelligence that acts as an admission control unit that evaluates if a submitted query could be serviced given the current network inference capabilities. The QIRCE handles query routing towards the correct models while observing loading conditions. Furthermore, the MDO module acts as an admission controller for newly submitted models where it evaluates deployment feasibility based on the submitted model's architecture, compute requirements, and storage requirements. It matches these requirements to the currently available resources indicated in the QRRT and makes an admittance decision. It also handles deployment location optimization, aiming to minimize query response time and cost for inference.</t>
    </section>
    <section anchor="da-itn-facilitation-agentic-networks">
      <name>DA-ITN-Facilitation Agentic Networks</name>
      <t>While agent-to-agent interaction is commonly associated with task-oriented collaboration—often relying on inference chaining as discussed in the inference section—we propose that this only reflects one side of the coin. We believe there is a transformative alternative: collaborative agent training, where agents not only work together to complete tasks, but also contribute to each other's learning and evolution. This paradigm marks a significant shift from traditional models and positions the DA-ITN as an ideal enabler of a truly agentic future, where intelligent agents can grow, adapt, and improve continuously through structured cooperation.</t>
      <t>It is important to distinguish clearly between collaborative training and task-based collaboration. In task-based collaboration, agents exchange data or partial inferences related to the execution of a specific, external objective—such as processing a query or generating an output. Their internal models remain unchanged; they simply contribute to a shared computational goal. In contrast, collaborative training focuses on internal evolution: the goal is not to solve an external task, but to enhance the capabilities of the participating agents themselves.</t>
      <t>In a collaborative training setup, agents may exchange model parameters, training datasets, or knowledge representations. They may engage in distributed training paradigms such as federated learning, where learning happens locally and updates are shared globally, or continual learning, where agents adapt over time based on new experiences. They may also employ knowledge distillation or transfer learning, where more advanced "teacher agents" guide "student agents" through structured training programs.
One can even envision a highly dynamic and autonomous system where agents attend “agent schools”—virtual environments where they gather to learn, be tested, and graduate. In this imagined scenario, teacher agents would be responsible for training student agents, evaluating their performance, and possibly issuing certifications or verifiable credentials that guarantee the agent’s competencies and readiness for deployment. These credentials serve trust foundations in the broader agent ecosystem, ensuring that certified agents can be reliably selected and trusted by inference clients or other agents.</t>
      <t>To support such a vision, a wide range of new functional and technical requirements must be addressed. These include secure model sharing, certification and validation infrastructure, identity management, trust negotiation, resource discovery for training, and scheduling of learning sessions. Fortunately, many of these requirements align naturally with the capabilities and components of the DA-ITN architecture—including its support for mobility, discovery, descriptor sharing, trust enforcement, dynamic rendezvous, and topology management.</t>
    </section>
    <section anchor="security-considerations">
      <name>Security Considerations</name>
      <t>Security considerations are as outlined within the document under the privacy and security requirements</t>
    </section>
    <section anchor="iana-considerations">
      <name>IANA Considerations</name>
      <t>This document has no IANA actions.</t>
    </section>
    <section anchor="conclusions">
      <name>Conclusions</name>
      <t>As AI continues to evolve and integrate into every facet of modern life, it becomes increasingly clear that the supporting infrastructure must evolve with it. The training and inference processes—central to the success of AI—are no longer simple, isolated tasks; they are complex, distributed, and require intelligent coordination across data, compute, and communication domains.</t>
      <t>The DA-ITN architecture offers a forward-looking response to this complexity by providing a cohesive, scalable, and intelligent network ecosystem. With its dedicated control, data, and operations &amp; management planes, DA-ITN not only supports the technical requirements of training and inference but also addresses critical concerns such as mobility, privacy, trust, and agent collaboration.</t>
      <t>Ultimately, DA-ITN lays the foundation for a new generation of AI-native networks—capable of enabling persistent learning, dynamic agent interaction, and decentralized intelligence at scale. As we move toward an AI-driven future, such architectures will be essential for building reliable, trustworthy, and efficient AI ecosystems.</t>
    </section>
  </middle>
  <back>
    <section anchor="contributors" numbered="false" toc="include" removeInRFC="false">
      <name>Contributors</name>
      <contact fullname="Arashmid Akhavain">
        <organization>Huawei Canada</organization>
        <address>
          <email>arashmid.akhavain@huawei.com</email>
        </address>
      </contact>
      <contact fullname="Hesham Moussa">
        <organization>Huawei Canada</organization>
        <address>
          <email>hesham.moussa@huawei.com</email>
        </address>
      </contact>
      <contact fullname="Tong Wen">
        <organization>Huawei</organization>
        <address>
          <email>tongwen@huawei.com</email>
        </address>
      </contact>
    </section>
  </back>
  <!-- ##markdown-source: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-->

</rfc>
